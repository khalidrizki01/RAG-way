{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e00868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selesai memuat dataset...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "dataset = load_from_disk(\"../generated_data/TUNING_final_summary\")\n",
    "train_dataset = dataset['train']\n",
    "dev_dataset = dataset['dev']\n",
    "test_dataset = dataset['test']\n",
    "print('selesai memuat dataset...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b66b935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'answer', 'generated_results'],\n",
       "        num_rows: 4542\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'answer', 'generated_results'],\n",
       "        num_rows: 1143\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'answer', 'generated_results'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c0187f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def split_each_passages(example):\n",
    "    # Pisahkan teks menjadi pasangan Judul-Teks berdasarkan '\\n\\n'\n",
    "    passages = example[\"passages\"].split('\\n\\n')\n",
    "    \n",
    "    # List untuk menyimpan hasil dari prepare_context_chunks untuk setiap bagian\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Proses setiap pasangan Judul-Teks\n",
    "    for passage in passages:\n",
    "        # Panggil fungsi prepare_context_chunks pada setiap bagian\n",
    "        all_chunks.extend(prepare_context_chunks(passage))\n",
    "    \n",
    "    # Kembalikan hasil ke dalam format baru\n",
    "    return {\"context_chunks\": all_chunks}\n",
    "\n",
    "def split_between_title_and_text(text: str) -> Tuple[str, str]:\n",
    "    \"\"\"Memisahkan bagian Judul dan Teks dari input lengkap.\"\"\"\n",
    "    title, content = text.split(\"Teks:\", 1)\n",
    "    return title.strip(), content.strip()\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Membagi teks panjang menjadi kalimat-kalimat.\"\"\"\n",
    "    text = re.sub(r'(\\.\\s?)\\[\\d+\\]', r'\\1', text)\n",
    "\n",
    "    matches = list(re.finditer(r'\\.(?=\\s+[A-Z])', text))\n",
    "    sentences = []\n",
    "    start = 0\n",
    "\n",
    "    for match in matches:\n",
    "        end = match.end()\n",
    "        candidate = text[start:end].strip()\n",
    "\n",
    "        before_dot = text[start:match.start()].strip()\n",
    "        if re.search(r'\\b([A-Z][a-z]?|[A-Z](?:\\.[A-Z])+)$', before_dot):\n",
    "            continue\n",
    "\n",
    "        sentences.append(text[start:end].strip())\n",
    "        start = end\n",
    "\n",
    "    if start < len(text):\n",
    "        sentences.append(text[start:].strip())\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_rolling_segments(title: str, sentences: List[str], window_size: int = 3) -> List[str]:\n",
    "    \"\"\"Membuat rolling window dari kalimat-kalimat dan menyisipkan judul di awal setiap segmen.\"\"\"\n",
    "    segments = []\n",
    "    if len(sentences) < window_size:\n",
    "        segments.append(f\"{title}\\n{' '.join(sentences)}\")\n",
    "    else:\n",
    "        for i in range(len(sentences) - window_size + 1):\n",
    "            group = sentences[i:i + window_size]\n",
    "            combined = title + \"\\n\" + \" \".join(group)\n",
    "            segments.append(combined)\n",
    "    return segments\n",
    "\n",
    "def prepare_context_chunks(text: str) -> List[str]:\n",
    "    \"\"\"Fungsi utama: memproses teks penuh menjadi rolling segments.\"\"\"\n",
    "    judul, teks = split_between_title_and_text(text)\n",
    "    kalimat_list = split_sentences(teks)\n",
    "    segments = create_rolling_segments(judul, kalimat_list, window_size=3)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "# Terapkan fungsi ke kolom \"passages\" dan simpan hasilnya di kolom baru\n",
    "train_dataset = train_dataset.map(split_each_passages)\n",
    "dev_dataset = dev_dataset.map(split_each_passages)\n",
    "test_dataset = test_dataset.map(split_each_passages)\n",
    "print('selesai membuat chunks...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "153d5f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 4542/4542 [18:17<00:00,  4.14it/s]\n",
      "Processing: 100%|██████████| 1143/1143 [03:54<00:00,  4.87it/s]\n",
      "Processing: 100%|██████████| 565/565 [01:52<00:00,  5.00it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model dan tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "model = AutoModel.from_pretrained('intfloat/multilingual-e5-small')\n",
    "model.eval()\n",
    "\n",
    "# Fungsi pooling dari model card\n",
    "def average_pool(last_hidden_states, attention_mask):\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "# Fungsi pencocokan top-3\n",
    "def get_top3_chunks(query, chunks):\n",
    "    if not chunks:\n",
    "        return [], [], []\n",
    "\n",
    "    input_texts = [\"query: \" + query] + [\"passage: \" + chunk for chunk in chunks]\n",
    "    batch = tokenizer(input_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    query_embedding = embeddings[0]\n",
    "    chunk_embeddings = embeddings[1:]\n",
    "    scores = (query_embedding @ chunk_embeddings.T) * 100\n",
    "\n",
    "    topk = torch.topk(scores, k=min(3, len(chunks)))  # Antisipasi jika chunk < 3\n",
    "    top_chunks = [chunks[i] for i in topk.indices.tolist()]\n",
    "    top_scores = topk.values.tolist()\n",
    "    top_indices = topk.indices.tolist()\n",
    "\n",
    "    return top_chunks, top_scores, top_indices\n",
    "\n",
    "# Terapkan ke dataset\n",
    "def process_dataset(dataset):\n",
    "    top_chunks_all = []\n",
    "    top_scores_all = []\n",
    "    top_indices_all = []\n",
    "\n",
    "    for example in tqdm(dataset, desc=\"Processing\"):\n",
    "        query = example['query']\n",
    "        chunks = example['context_chunks']\n",
    "\n",
    "        top_chunks, top_scores, top_indices = get_top3_chunks(query, chunks)\n",
    "\n",
    "        top_chunks_all.append(top_chunks)\n",
    "        top_scores_all.append(top_scores)\n",
    "        top_indices_all.append(top_indices)\n",
    "\n",
    "    # Tambahkan kolom baru\n",
    "    dataset = dataset.add_column(\"top_chunks\", top_chunks_all)\n",
    "    dataset = dataset.add_column(\"top_chunk_scores\", top_scores_all)\n",
    "    dataset = dataset.add_column(\"top_chunk_indices\", top_indices_all)\n",
    "    return dataset\n",
    "\n",
    "# Proses semua split\n",
    "train_dataset = process_dataset(train_dataset)\n",
    "dev_dataset = process_dataset(dev_dataset)\n",
    "test_dataset = process_dataset(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "439f8976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4542/4542 [00:00<00:00, 71901.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1143/1143 [00:00<00:00, 65826.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 565/565 [00:00<00:00, 38521.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"dev\": dev_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "dataset.save_to_disk(\"../generated_data/top_reranked_chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191d795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_text = '''Judul: Ernest Douwes Dekker \n",
    "# Teks: Ernest Douwes Dekker wafat dini hari tanggal 28 Agustus 1950 (tertulis di batu nisannya; 29 Agustus 1950 versi van der Veur, 2006) dan dimakamkan di TMP Cikutra, Bandung.'''\n",
    "\n",
    "# # full_text = '''Judul: Kabupaten Probolinggo\n",
    "# # Teks: Bintang berwarna kuning melambangkan ketuhanan yang maha esa. Sungai sebagai tanda \"sungai banger\". Yang menjadi asal nama kabupaten ini. Angin berwarna merah dan putih sebagai ciri khas Kabupaten Probolinggo yang terkenal dengan sebutan \"Angin Gending\". Angin ini biasanya datang pada musim kemarau dari arah tenggara Gunung sebagai tanda Gunung Bromo yang terletak di Pegunungan Tengger. Dataran tanah berwarna hijau, merupakan pertanda daerah daratan Kab.Probolinggo yang cukup subur. Gelombang air laut, yang menunjukkan letak Kabupaten Probolinggo di tepi Selat Madura Daun anggur sebanyak 4 buah berwarna hijau muda, dengan 17 buah anggur. Menunjukkan hasil buah-buahan khas Probolinggo. Daun mangga sebanyak lima, dan buah mangga sebanyak 8 buah.Menunjukkan hasil buah-buahan khas Probolinggo. Susunan buah anggur-buah mangga-daun anggur-daun mangga melambangkan tanggal kemerdekaan Indonesia 17-8-45 (17 Agustus 1945) Pita dasar dengan warna putih bertuliskan Prasadja Ngesti Wibawa yang beartikan \"Bersahaja menciptakan kemuliaan\" Warna kuning berartikan keagungan, keluhuran, dan kemuliaan. Warna biru berartikan kesetiaan. Warna hijau berartikan kesuburan, kemakmuran.'''\n",
    "# hasil = prepare_context_chunks(full_text)\n",
    "\n",
    "# # Tampilkan hasil\n",
    "# for i, segmen in enumerate(hasil, 1):\n",
    "#     print(f\"[Segment {i}]\\n{segmen}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b154bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4542/4542 [00:01<00:00, 3425.62 examples/s]\n",
      "Map: 100%|██████████| 1143/1143 [00:00<00:00, 3494.14 examples/s]\n",
      "Map: 100%|██████████| 565/565 [00:00<00:00, 3395.50 examples/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58fb09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc189c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
