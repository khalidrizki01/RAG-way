{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "tydiqa_gold = load_dataset(\"khalidalt/tydiqa-goldp\", 'indonesian', trust_remote_code=True)\n",
    "mr_tydi = load_from_disk(\"./generated_data/mr_tydi_filled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'language', 'document_title', 'passage_text', 'question_text', 'answers'],\n",
       "        num_rows: 5702\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'language', 'document_title', 'passage_text', 'question_text', 'answers'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tydiqa_gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tydiqa_gold = tydiqa_gold.remove_columns([\"language\", \"document_title\", \"passage_text\"])\n",
    "tydiqa_gold = tydiqa_gold.rename_column(\"id\", \"tydiqa_id\")\n",
    "tydiqa_gold = tydiqa_gold.rename_column(\"question_text\", \"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "def clean_answers(example):\n",
    "    example[\"answers\"] = example[\"answers\"][\"text\"]  # Ambil hanya bagian text\n",
    "    return example\n",
    "\n",
    "# Terapkan fungsi untuk membersihkan answers di setiap split\n",
    "tydiqa_gold = DatasetDict({\n",
    "    split: dataset.map(clean_answers)\n",
    "    for split, dataset in tydiqa_gold.items()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7fbd0cfec14af194ac309dd7fbee71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebf55ed51aa4791bdb8086d80cff061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "\n",
    "# Fungsi untuk membersihkan teks: hapus newline & whitespace berlebih\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Fungsi untuk membersihkan dan memilih jawaban terpendek\n",
    "def clean_tydiqa(example):\n",
    "    # Bersihkan query\n",
    "    example[\"query\"] = clean_text(example[\"query\"])\n",
    "    \n",
    "    # Bersihkan answers dan pilih jawaban terpendek jika ada lebih dari satu\n",
    "    cleaned_answers = [clean_text(ans) for ans in example[\"answers\"]]\n",
    "    example[\"answers\"] = min(cleaned_answers, key=len) if cleaned_answers else \"\"  # Pilih jawaban terpendek\n",
    "\n",
    "    return example\n",
    "\n",
    "# Fungsi untuk membersihkan query di MR-TyDi\n",
    "def clean_mr_tydi(example):\n",
    "    example[\"query\"] = clean_text(example[\"query\"])\n",
    "    return example\n",
    "\n",
    "# Terapkan pembersihan pada dataset\n",
    "tydiqa_gold_cleaned = DatasetDict({\n",
    "    split: dataset.map(clean_tydiqa)\n",
    "    for split, dataset in tydiqa_gold.items()\n",
    "})\n",
    "\n",
    "mr_tydi_cleaned = DatasetDict({\n",
    "    split: dataset.map(clean_mr_tydi)\n",
    "    for split, dataset in mr_tydi.items()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a39a90fc9b24c0b818d256a6b0392ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nicolas Loufrani\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03fe9fd494149a0b01ddd4e22fd35c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tydiqa_id': '8601389648636013237-1',\n",
       " 'query': 'siapakah ketua Perum LKBN pertama?',\n",
       " 'answers': 'Mr. Soemanang'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_answer_is_cleaned = tydiqa_gold_cleaned['train'].filter(lambda x: x['tydiqa_id']==\"-2253919563477221294-3\")\n",
    "print(check_if_answer_is_cleaned[0]['answers'])\n",
    "\n",
    "check_if_answer_more_than_1 = tydiqa_gold_cleaned['validation'].filter(lambda x: x['tydiqa_id']==\"8601389648636013237-1\")\n",
    "check_if_answer_more_than_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Ditemukan 42 kata unik yang menjadi kata pertama dalam query ketika answers memiliki lebih dari 10 kata.\n",
      "üìã Kata unik tersebut: ['Aapakah', 'Ada', 'Apa', 'Apaka', 'Apakah', 'Bagaimana', 'Bagaimanakah', 'Berapa', 'Berapakah', 'Bidang', 'Dari', 'Darimanakah', 'Daun', 'Di', 'Dimana', 'Dimanakah', 'Dimankah', 'Kapan', 'Kenapa', 'Menceritakan', 'Mengapa', 'Musik', 'Siapa', 'Siapakah', 'Tentang', 'Untuk', 'Zat', 'aoakah', 'apa', 'apaah', 'apakah', 'berapa', 'berapakah', 'berdasarkan', 'dari', 'di', 'dimanakah', 'kapankah', 'mengapa', 'negara', 'siapakah', 'siapakh']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Gabungkan split train dan validation\n",
    "tydiqa_gold_combined = concatenate_datasets([tydiqa_gold_cleaned[\"train\"], tydiqa_gold_cleaned[\"validation\"]])\n",
    "\n",
    "# Fungsi untuk menghitung jumlah kata dalam jawaban\n",
    "def count_words(text):\n",
    "    return len(re.findall(r'\\w+', text))\n",
    "\n",
    "# Filter hanya row dengan answers lebih dari 10 kata\n",
    "long_answer_rows = [row for row in tydiqa_gold_combined if count_words(row[\"answers\"]) > 10]\n",
    "\n",
    "# Ambil kata pertama dari query\n",
    "first_words = [row[\"query\"].split()[0] for row in long_answer_rows]\n",
    "\n",
    "# Hitung jumlah kemunculan kata pertama dalam query\n",
    "first_word_counts = Counter(first_words)\n",
    "\n",
    "# Konversi ke daftar kata unik\n",
    "unique_first_words = sorted(set(first_words))\n",
    "\n",
    "# Cetak hasil\n",
    "print(f\"üîç Ditemukan {len(unique_first_words)} kata unik yang menjadi kata pertama dalam query ketika answers memiliki lebih dari 10 kata.\")\n",
    "print(\"üìã Kata unik tersebut:\", unique_first_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering untuk answers yang jumlah katanya kurang dari 15 (opsional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tydiqa_id', 'query', 'answers'],\n",
       "        num_rows: 5702\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tydiqa_id', 'query', 'answers'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tydiqa_gold_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset berhasil digabungkan berdasarkan `query` dengan struktur mengikuti `mr_tydi_cleaned`.\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets\n",
    "\n",
    "# Gabungkan split train & validation pada tydiqa_gold_cleaned\n",
    "tydiqa_gold_combined = concatenate_datasets([tydiqa_gold_cleaned[\"train\"], tydiqa_gold_cleaned[\"validation\"]])\n",
    "\n",
    "# Buat struktur baru mengikuti split dari mr_tydi_cleaned\n",
    "joined_datasets = {}\n",
    "\n",
    "for split, mr_tydi_split in mr_tydi_cleaned.items():\n",
    "    # Buat dictionary {query: row} dari tydiqa_gold_cleaned untuk lookup cepat\n",
    "    tydiqa_gold_dict = {row[\"query\"]: row for row in tydiqa_gold_combined}\n",
    "    \n",
    "    # Buat daftar baru dengan menggabungkan informasi dari mr_tydi_cleaned dan tydiqa_gold_cleaned\n",
    "    new_split_data = []\n",
    "    \n",
    "    for row in mr_tydi_split:\n",
    "        query = row[\"query\"]\n",
    "        tydiqa_data = tydiqa_gold_dict.get(query, None)  # Ambil data dari tydiqa_gold jika ada\n",
    "        \n",
    "        # Gabungkan data (jika tidak ada di tydiqa_gold, biarkan bagian tersebut kosong)\n",
    "        merged_row = {\n",
    "            **row,  # Data dari mr_tydi_cleaned\n",
    "            \"tydiqa_id\": tydiqa_data[\"tydiqa_id\"] if tydiqa_data else None,\n",
    "            \"answers\": tydiqa_data[\"answers\"] if tydiqa_data else None\n",
    "        }\n",
    "        \n",
    "        new_split_data.append(merged_row)\n",
    "\n",
    "    # Konversi kembali ke Dataset\n",
    "    joined_datasets[split] = mr_tydi_split.from_list(new_split_data)\n",
    "\n",
    "# Simpan hasil sebagai DatasetDict\n",
    "final_dataset = DatasetDict(joined_datasets)\n",
    "\n",
    "# # Simpan hasil ke disk\n",
    "# final_dataset.save_to_disk(\"generated_data/mr_tydi_tydiqa_joined\")\n",
    "\n",
    "print(\"‚úÖ Dataset berhasil digabungkan berdasarkan `query` dengan struktur mengikuti `mr_tydi_cleaned`.\")\n",
    "# print(\"‚úÖ Dataset telah disimpan di 'generated_data/mr_tydi_tydiqa_joined'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346c03fe26194d868d497470d98c2cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee34eba771d48148113e36ddafc8628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0407fd9d46c41e3a72ce20e8b05e37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/829 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Semua row dengan 'answers = None' telah dihapus dari dataset baru `finished_dataset`.\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Fungsi untuk menghapus rows dengan answers = None\n",
    "def remove_none_answers(dataset):\n",
    "    return dataset.filter(lambda row: row[\"answers\"] is not None)\n",
    "\n",
    "# Buat dataset baru tanpa row yang memiliki answers = None\n",
    "finished_dataset = DatasetDict({\n",
    "    \"train\": remove_none_answers(final_dataset[\"train\"]),\n",
    "    \"dev\": remove_none_answers(final_dataset[\"dev\"]),\n",
    "    \"test\": remove_none_answers(final_dataset[\"test\"])\n",
    "})\n",
    "\n",
    "print(\"‚úÖ Semua row dengan 'answers = None' telah dihapus dari dataset baru `finished_dataset`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Split 'train': 0 row(s) memiliki 'answers' = None\n",
      "üîç Split 'dev': 0 row(s) memiliki 'answers' = None\n",
      "üîç Split 'test': 0 row(s) memiliki 'answers' = None\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk mengecek apakah ada None dalam kolom 'answers'\n",
    "def check_none_answers(dataset, split_name):\n",
    "    none_count = sum(1 for row in dataset if row[\"answers\"] is None)\n",
    "    print(f\"üîç Split '{split_name}': {none_count} row(s) memiliki 'answers' = None\")\n",
    "\n",
    "# Periksa setiap split\n",
    "check_none_answers(finished_dataset[\"train\"], \"train\")\n",
    "check_none_answers(finished_dataset[\"dev\"], \"dev\")\n",
    "check_none_answers(finished_dataset[\"test\"], \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad097fed306d44969609697109a203c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b692f1cf2ee4ecb9f313a8fcd480d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268609c542fc4c5caee36ff5b239e4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c470f8077c464917b0bffdfa2fc989c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c610182de34cef8ecbf1f18fa9ce49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4746e9d700a04fd1ab71d8af63d59418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# # Load dataset yang sudah dibersihkan\n",
    "# finished_dataset = load_from_disk(\"generated_data/mr_tydi_tydiqa_cleaned\")\n",
    "\n",
    "# Load tokenizer & model untuk Multilingual-E5-Small\n",
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embedding_model = AutoModel.from_pretrained(model_name).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896b03a502804ca5aa5b15ca75972907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc689749e344fce95e9c16bc3b6ee97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98e5f6a7a134954b0a080d6356969c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c05e8f479bc4d11aff2048f0d221b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Negative passages pada split train telah dipangkas menjadi top 4 berdasarkan similarity!\n",
      "‚úÖ Dataset telah disimpan di 'generated_data/mr_tydi_tydiqa_final'.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "# Fungsi average pooling\n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "# Fungsi untuk memilih top 4 negative_passages berdasarkan similarity\n",
    "def select_top4_negative_passages(example):\n",
    "    query_text = example[\"query\"]\n",
    "    negative_passages = example[\"negative_passages\"]\n",
    "\n",
    "    # Jika sudah <= 4, tidak perlu pemrosesan\n",
    "    if len(negative_passages) <= 4:\n",
    "        return example\n",
    "\n",
    "    # Ambil teks dari negative_passages\n",
    "    neg_texts = [neg[\"text\"] for neg in negative_passages]\n",
    "\n",
    "    # Tokenisasi dan embedding query serta negative_passages\n",
    "    batch_dict = embedding_tokenizer([query_text] + neg_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    batch_dict = {k: v.to(\"cuda:0\") for k, v in batch_dict.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**batch_dict)\n",
    "\n",
    "    # Hitung embedding dan normalisasi\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)  # Normalisasi untuk cosine similarity\n",
    "\n",
    "    # Hitung similarity scores (query vs negative_passages)\n",
    "    query_embedding = embeddings[0].unsqueeze(0)  # Query ada di indeks pertama\n",
    "    neg_embeddings = embeddings[1:]  # Negative passages setelah query\n",
    "    scores = (query_embedding @ neg_embeddings.T).squeeze(0)  # Cosine similarity\n",
    "\n",
    "    # Ambil indeks top 4 dengan similarity tertinggi\n",
    "    top_indices = torch.argsort(scores, descending=True)[:4]\n",
    "\n",
    "    # Simpan hanya 4 negative_passages terbaik\n",
    "    example[\"negative_passages\"] = [negative_passages[i] for i in top_indices]\n",
    "\n",
    "    # Bersihkan cache GPU setelah query diproses\n",
    "    del batch_dict, outputs, embeddings, scores\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return example\n",
    "\n",
    "# Terapkan fungsi ke split train\n",
    "finished_dataset[\"train\"] = finished_dataset[\"train\"].map(select_top4_negative_passages)\n",
    "\n",
    "# Simpan hasilnya ke disk\n",
    "finished_dataset.save_to_disk(\"generated_data/mr_tydi_tydiqa_final\")\n",
    "\n",
    "print(\"‚úÖ Negative passages pada split train telah dipangkas menjadi top 4 berdasarkan similarity!\")\n",
    "print(\"‚úÖ Dataset telah disimpan di 'generated_data/mr_tydi_tydiqa_final'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_12-4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
