{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c31939b",
   "metadata": {},
   "source": [
    "# Corpus khusus untuk xRAG - pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7206105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata \n",
    "\n",
    "# === helpers ===\n",
    "ZW_RE = re.compile(r\"[\\u200b-\\u200d\\u2060\\ufeff\\u00ad\\u200e\\u200f]\")\n",
    "NBSP_RE = re.compile(r\"[\\u00A0]\")\n",
    "MULTI_SPACE_RE = re.compile(r\"\\s{2,}\")\n",
    "\n",
    "# wiki image markup (case-insensitive)\n",
    "WIKI_IMG_TOKEN_RE = re.compile(\n",
    "    r\"(?i)(?:\\|\\s*(?:jmpl|thumb|kiri|kanan|ka)\\b|\\b(?:jmpl|thumb|kiri|kanan|ka)\\b\\s*\\|?)\"\n",
    ")\n",
    "WIKI_IMG_SIZE_RE = re.compile(\n",
    "    r\"(?i)(?:\\|\\s*\\d{2,4}(?:x\\d{2,4})?px\\b|\\b\\d{2,4}(?:x\\d{2,4})?px\\b\\s*\\|?)\"\n",
    ")\n",
    "# sisa pipa/whitespace berlebih\n",
    "PIPE_SPACES_RE = re.compile(r\"(?:\\s*\\|\\s*)+\")\n",
    "\n",
    "_UNITS = r\"(km2|m2|km|m|cm|mm|kg|g|mg|t|l|ml|hz|khz|mhz|ghz|v|kv|w|kw|mw|a|ma|pa|kpa|mpa|bar|%)\"\n",
    "\n",
    "def normalize(q: str, lowercase: bool = True) -> str:\n",
    "    if not q:\n",
    "        return \"\"\n",
    "    # -------- hapus sitasi [angka] --------\n",
    "    q = re.sub(r\"\\[\\d+\\]\", \"\", q)\n",
    "\n",
    "    # -------- unicode & basic cleanup --------\n",
    "    q = unicodedata.normalize(\"NFKC\", q)\n",
    "    q = ZW_RE.sub(\"\", q)\n",
    "    q = NBSP_RE.sub(\" \", q)\n",
    "\n",
    "    # -------- bersihkan artefak gambar wiki --------\n",
    "    for _ in range(2):\n",
    "        q = WIKI_IMG_TOKEN_RE.sub(\" \", q)\n",
    "        q = WIKI_IMG_SIZE_RE.sub(\" \", q)\n",
    "    q = PIPE_SPACES_RE.sub(\" \", q)\n",
    "\n",
    "    # -------- bersihkan artefak HTML/parsoid --------\n",
    "    # buang atribut parsoid & payload \"dsr\"\n",
    "    q = re.sub(r\"data-parsoid\\s*=\\s*'[^']*'\", \" \", q, flags=re.IGNORECASE)\n",
    "    q = re.sub(r'\"dsr\"\\s*:\\s*\\[[^\\]]*\\]', \" \", q, flags=re.IGNORECASE)\n",
    "\n",
    "    # hapus tag HTML normal (misalnya <b>...</b>)\n",
    "    q = re.sub(r\"</?\\s*[a-zA-Z][^>]*>\", \" \", q)\n",
    "\n",
    "    # sambungkan huruf yang terpisah oleh tag patah /b>, /i>, /u>, /s>\n",
    "    q = re.sub(r\"(?<=\\w)/[bius]>(?=\\w)\", \"\", q, flags=re.IGNORECASE)\n",
    "\n",
    "    # hilangkan sisa /b> /i> /u> /s> sendirian\n",
    "    q = re.sub(r\"/[bius]>\", \" \", q, flags=re.IGNORECASE)\n",
    "\n",
    "    # -------- lower & normalisasi tanda baca --------\n",
    "\n",
    "    q = q.strip()\n",
    "    q = (q.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\")\n",
    "           .replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "           .replace(\"…\", \"...\")\n",
    "           .replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "           .replace(\"؟\", \"?\").replace(\"．\", \".\").replace(\"؟\", \"?\"))\n",
    "    \n",
    "    if lowercase:\n",
    "        q = q.lower()\n",
    "\n",
    "    # --- spasi + tanda baca (ramah tanda kurung) ---\n",
    "    q = re.sub(r\"\\s+\\?\", \"?\", q)                            # hapus spasi sebelum '?'\n",
    "    q = re.sub(r\"\\s+([,.:;!?/\\)\\-])\", r\"\\1\", q)             # hapus spasi sebelum tanda baca (kecuali '(')\n",
    "    q = re.sub(r\"([/(])\\s+\", r\"\\1\", q)                      # hapus spasi setelah '/' dan '('\n",
    "    q = re.sub(r\"(?<=\\w)\\(\", r\" (\", q)                      # pastikan ada spasi sebelum '(' bila didahului huruf/angka\n",
    "    q = re.sub(r\"\\)(?=\\w)\", r\") \", q)                       # pastikan ada spasi setelah ')' bila diikuti huruf/angka\n",
    "\n",
    "    # rapikan spasi\n",
    "    q = MULTI_SPACE_RE.sub(\" \", q).strip()\n",
    "\n",
    "    # hapus -kah pada kata pertama (opsional)\n",
    "    parts = q.split()\n",
    "    if parts:\n",
    "        parts[0] = re.sub(r\"(-?kah)$\", \"\", parts[0])\n",
    "    q = \" \".join(parts)\n",
    "\n",
    "    # normalisasi ringan kosakata/angka\n",
    "    q = re.sub(r\"\\bdi\\s+mana\\b\", \"dimana\", q)\n",
    "    q = re.sub(r\"\\byg\\b\", \"yang\", q)\n",
    "    q = re.sub(r\"(?<=\\d)\\.(?=\\d{3}\\b)\", \"\", q)   # 1.234.567 → 1234567\n",
    "    q = re.sub(r\"(\\d),(\\d)\", r\"\\1.\\2\", q)        # 3,5 → 3.5\n",
    "    q = q.replace(\"km²\", \"km2\").replace(\"m²\", \"m2\")\n",
    "    # --- sisipkan spasi antara angka dan satuan ---\n",
    "\n",
    "    # 12km -> 12 km, 5m2 -> 5 m2, 100% -> 100 %\n",
    "    q = re.sub(rf\"(?i)(\\d)\\s*({_UNITS})\\b\", r\"\\1 \\2\", q)\n",
    "\n",
    "    # suhu: 50°C / 50 °C -> 50 °C  (pastikan ada spasi sebelum '°' dan antara ° dengan huruf)\n",
    "    q = re.sub(r\"(?i)(\\d)\\s*°\\s*([cf])\\b\", r\"\\1 °\\2\", q)\n",
    "\n",
    "    q = re.sub(r\"\\s*&\\s*\", \" dan \", q)\n",
    "    q = re.sub(r\"\\?{2,}\", \"?\", q)\n",
    "\n",
    "    # final cleanup\n",
    "    q = MULTI_SPACE_RE.sub(\" \", q).strip()\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331ef9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['docid', 'title', 'text'],\n",
       "        num_rows: 1469399\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "# Load corpus & tokenizer\n",
    "corpus = load_dataset(\"castorini/mr-tydi-corpus\", \"indonesian\", trust_remote_code=True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f81d1",
   "metadata": {},
   "source": [
    "nengok kasus dimana terdapat tag html atau yang mirip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b60f79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil disimpan ke mr_tydi_matches.xlsx, total baris: 97\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Muat dataset (ganti sesuai punyamu)\n",
    "ds = load_dataset(\"castorini/mr-tydi-corpus\", \"indonesian\", split=\"train\")\n",
    "\n",
    "# Pola rawan\n",
    "PATTERNS = {\n",
    "    \"slash_bius\": re.compile(r\"/[bius]>\"),            # /b> /i> /u> /s>\n",
    "    \"broken_join\": re.compile(r\"\\w/[bius]>\\w\"),       # d/b>e\n",
    "    \"data_parsoid\": re.compile(r\"data-parsoid\\s*=\", re.I),\n",
    "    \"dsr\": re.compile(r'\"dsr\"\\s*:\\s*\\[', re.I),\n",
    "    \"html_tag\": re.compile(r\"</?\\s*[a-zA-Z][^>]*>\"),  # <...> tag\n",
    "}\n",
    "\n",
    "# Simpan hasil match\n",
    "records = []\n",
    "for ex in ds.select(range(100000)):\n",
    "    text = ex[\"text\"]\n",
    "    matches = {name: bool(pat.search(text)) for name, pat in PATTERNS.items()}\n",
    "    if any(matches.values()):\n",
    "        record = {\n",
    "            \"docid\": ex[\"docid\"],\n",
    "            \"title\": ex[\"title\"],\n",
    "            \"text_snippet\": text,  # simpan potongan teks saja biar file tidak terlalu berat\n",
    "        }\n",
    "        record.update(matches)\n",
    "        records.append(record)\n",
    "\n",
    "# Buat DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Simpan ke Excel\n",
    "output_file = \"mr_tydi_matches.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Hasil disimpan ke {output_file}, total baris: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8462dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing title & text: 100%|██████████| 1469399/1469399 [07:13<00:00, 3386.90 examples/s]\n",
      "Chunking into <=180 tokens (word-aware) & merging last <50:   0%|          | 0/1469399 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking into <=180 tokens (word-aware) & merging last <50: 100%|██████████| 1469399/1469399 [10:28<00:00, 2336.38 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['docid', 'title', 'text'],\n",
      "    num_rows: 1588236\n",
      "})\n",
      "{'docid': '1#0_0', 'title': 'Asam deoksiribonukleat', 'text': 'Asam deoksiribonukleat, lebih dikenal dengan singkatan DNA (bahasa Inggris: d eoxyribo n ucleic a cid), adalah sejenis biomolekul yang menyimpan dan menyandi instruksi-instruksi genetika setiap organisme dan banyak jenis virus. Instruksi-instruksi genetika ini berperan penting dalam pertumbuhan, perkembangan, dan fungsi organisme dan virus. DNA merupakan asam nukleat; bersamaan dengan protein'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "MAX_TOK = 180\n",
    "MIN_TOK = 50\n",
    "\n",
    "# ---------- Normalisasi ----------\n",
    "\n",
    "def normalize_batch(batch):\n",
    "    batch[\"title\"] = [normalize(t, lowercase=False) for t in batch[\"title\"]]\n",
    "    batch[\"text\"] = [normalize(t, lowercase=False) for t in batch[\"text\"]]\n",
    "    return batch\n",
    "\n",
    "# ---------- Chunking ----------\n",
    "def chunk_batch(batch):\n",
    "    out_docid, out_title, out_text = [], [], []\n",
    "    for docid, title, text in zip(batch[\"docid\"], batch[\"title\"], batch[\"text\"]):\n",
    "        token_ids = t5_tokenizer.encode(text, add_special_tokens=False)\n",
    "        if not token_ids:\n",
    "            continue\n",
    "\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(token_ids):\n",
    "            end = min(start + MAX_TOK, len(token_ids))\n",
    "\n",
    "            # pastikan akhir chunk di batas kata\n",
    "            if end < len(token_ids):\n",
    "                tok_str = t5_tokenizer.convert_ids_to_tokens([token_ids[end]])[0]\n",
    "                while end > start and not tok_str.startswith(\"▁\"):\n",
    "                    end -= 1\n",
    "                    tok_str = t5_tokenizer.convert_ids_to_tokens([token_ids[end]])[0]\n",
    "                if end == start:\n",
    "                    end = min(start + MAX_TOK, len(token_ids))\n",
    "\n",
    "            chunks.append(token_ids[start:end])\n",
    "            start = end\n",
    "\n",
    "        # gabungkan segmen terakhir kalau <50 token\n",
    "        if len(chunks) >= 2 and len(chunks[-1]) < MIN_TOK:\n",
    "            chunks[-2].extend(chunks[-1])\n",
    "            chunks = chunks[:-1]\n",
    "        elif len(chunks) == 1 and len(chunks[0]) < MIN_TOK:\n",
    "            chunks = []  # terlalu pendek → buang\n",
    "\n",
    "        # simpan segmen valid\n",
    "        for idx, seg in enumerate(chunks):\n",
    "            if len(seg) < MIN_TOK:\n",
    "                continue\n",
    "            out_docid.append(f\"{docid}_{idx}\")\n",
    "            out_title.append(title)\n",
    "            out_text.append(t5_tokenizer.decode(seg, skip_special_tokens=True))\n",
    "\n",
    "    return {\"docid\": out_docid, \"title\": out_title, \"text\": out_text}\n",
    "\n",
    "# ---------- Pipeline ----------\n",
    "# 1. Normalisasi dulu\n",
    "normalized_corpus = corpus.map(\n",
    "    normalize_batch,\n",
    "    batched=True,\n",
    "    desc=\"Normalizing title & text\"\n",
    ")\n",
    "\n",
    "# 2. Chunking setelah normalisasi\n",
    "split_dataset = normalized_corpus[\"train\"].map(\n",
    "    chunk_batch,\n",
    "    batched=True,\n",
    "    remove_columns=normalized_corpus[\"train\"].column_names,\n",
    "    desc=\"Chunking into <=180 tokens (word-aware) & merging last <50\"\n",
    ")\n",
    "\n",
    "print(split_dataset)\n",
    "print(split_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75cd836b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah duplikat (berdasarkan kolom text): 38\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ambil 10.000 baris pertama\n",
    "sample = split_dataset.select(range(10000))\n",
    "\n",
    "# konversi ke DataFrame untuk analisis cepat\n",
    "df = pd.DataFrame(sample)\n",
    "\n",
    "# cek duplikasi berdasarkan kolom 'text'\n",
    "dup_mask = df.duplicated(subset=[\"text\"], keep=False)\n",
    "duplicates = df[dup_mask].sort_values(by=\"text\")\n",
    "\n",
    "# simpan hasil duplikat ke Excel\n",
    "duplicates.to_excel(\"duplicates_first_10k.xlsx\", index=False)\n",
    "\n",
    "print(f\"Jumlah duplikat (berdasarkan kolom text): {duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac9e46e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah awal: 1588236, jumlah setelah buang duplikat: 1521373\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Konversi dulu ke pandas DataFrame\n",
    "df = split_dataset.to_pandas()\n",
    "\n",
    "# Hapus duplikasi berdasarkan kolom \"text\"\n",
    "# keep='first' artinya baris pertama dipertahankan\n",
    "df_nodup = df.drop_duplicates(subset=[\"text\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# Konversi kembali ke HuggingFace Dataset\n",
    "split_dataset_nodup = Dataset.from_pandas(df_nodup)\n",
    "\n",
    "print(f\"Jumlah awal: {len(split_dataset)}, jumlah setelah buang duplikat: {len(split_dataset_nodup)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3599ef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 795/795 [00:02<00:00, 283.55ba/s]\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Creating parquet from Arrow format: 100%|██████████| 795/795 [00:02<00:00, 315.18ba/s]\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:52<00:00, 26.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# pip install datasets huggingface_hub\n",
    "\n",
    "from datasets import DatasetDict\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from datetime import datetime\n",
    "\n",
    "# ====== asumsikan split_dataset sudah ada dari langkahmu sebelumnya ======\n",
    "# split_dataset = normalized_corpus[\"train\"].map(...)\n",
    "\n",
    "# Bungkus ke DatasetDict agar rapi (boleh tambah 'validation'/'test' kalau ada)\n",
    "split_dataset_dict = DatasetDict({\"train\": split_dataset})\n",
    "\n",
    "# ====== Konfigurasi repo ======\n",
    "repo_id = \"khalidrizki/indonesian-wiki-chunked-180tok\"   # ganti\n",
    "private = False                                          # True kalau mau privat\n",
    "token = HfFolder.get_token()\n",
    "api = HfApi()\n",
    "\n",
    "# 1) Buat repo (jika belum ada)\n",
    "api.create_repo(\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    private=private,\n",
    "    exist_ok=True,\n",
    "    token=token,\n",
    ")\n",
    "\n",
    "# 2) Push dataset\n",
    "split_dataset_dict.push_to_hub(\n",
    "    repo_id=repo_id,\n",
    "    private=private,\n",
    ")\n",
    "\n",
    "# 3) Siapkan README (dataset card)\n",
    "readme = f\"\"\"# mr-tydi-indonesian-chunked-180tok\n",
    "\n",
    "Hasil preprocessing dari [`castorini/mr-tydi-corpus`](https://huggingface.co/datasets/castorini/mr-tydi-corpus) split **Indonesian**.\n",
    "\n",
    "## Pipeline\n",
    "1. **Normalisasi teks**\n",
    "   - Bersihkan artefak Parsoid/markup wiki (mis. `data-parsoid`, `dsr`, tag HTML patah)\n",
    "   - Normalisasi whitespace & tanda baca\n",
    "2. **Chunking word-aware**\n",
    "   - Tokenizer: **google/flan-t5-base** (SentencePiece)\n",
    "   - Panjang segmen ≤ **180 token**\n",
    "   - Pemotongan berhenti di **batas kata** (hindari potong subword di tengah)\n",
    "   - Jika segmen terakhir < **50 token**, **digabung** ke segmen sebelumnya\n",
    "3. **docid baru**\n",
    "   - Format: `docidAsli#partisiOlehMrTydi_nomorSegmen` (mis. `12345#67_0`, `12345#67_1`, …)\n",
    "\n",
    "## Statistik\n",
    "- Dataset asli (train, ID): ≈ 1.47M dokumen\n",
    "- Dataset hasil chunking: lebih besar (tergantung distribusi panjang)\n",
    "- Panjang segmen: 50–180 token (bisa >180 bila segmen terakhir digabung)\n",
    "\n",
    "## Contoh\n",
    "```json\n",
    "{{\n",
    "  \"docid\": \"12345#0_0\",\n",
    "  \"title\": \"Dokar\",\n",
    "  \"text\": \"Dokar, kendaraan dengan kuda sebagai penarik ...\"\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d6a6cf",
   "metadata": {},
   "source": [
    "# TyDi QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316fa759",
   "metadata": {},
   "source": [
    "Gabungkan semua split jadi 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74325716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
      "        num_rows: 4902\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
      "        num_rows: 1224\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
      "        num_rows: 829\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Muat dataset\n",
    "mr_tydi = load_dataset(\"castorini/mr-tydi\", \"indonesian\")\n",
    "tydiqa_gold = load_dataset(\"khalidalt/tydiqa-goldp\", \"indonesian\", trust_remote_code=True)\n",
    "\n",
    "# Gabungkan semua split di tydiqa_gold\n",
    "tydiqa_gold_all = concatenate_datasets([tydiqa_gold[split] for split in tydiqa_gold.keys()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d917ef",
   "metadata": {},
   "source": [
    "Rename kolom question pada tydiqa menjadi query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3527940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tydiqa_gold_all = tydiqa_gold_all.rename_column(\"question_text\", \"query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479c36e",
   "metadata": {},
   "source": [
    "Normalisasi dan buat kolom question_stem, yakni query yang sudah di-stemming. Ini gunanya untuk memudahkan membuang duplikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993eb651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def add_norm_and_stem(batch, column):\n",
    "    qs = batch[column]\n",
    "    norm = [normalize(x) for x in qs]\n",
    "    stem = [stemmer.stem(x) for x in norm]  # stem setelah normalisasi\n",
    "    return {f\"{column}_norm\": norm, f\"{column}_stem\": stem}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359ef825",
   "metadata": {},
   "outputs": [],
   "source": [
    "tydiqa_gold_all = tydiqa_gold_all.map(\n",
    "    add_norm_and_stem, \n",
    "    batched=True, \n",
    "    fn_kwargs={\"column\": \"question\"}   # sesuaikan dengan nama kolom di tydiqa_gold_all\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7444977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6267/6267 [00:03<00:00, 1777.07 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'language', 'document_title', 'passage_text', 'query', 'answers', 'question_norm', 'question_stem'],\n",
       "    num_rows: 6267\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tydiqa_gold_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547c54e",
   "metadata": {},
   "source": [
    "Dengan melakukan inspeksi manual, dari baris yang question_stem nya terduplikasi, sudah ditentukan mana saja baris yang answer nya salah. \n",
    "\n",
    "Dengan demikian, baris-baris yang question_stem terduplikasi dan answer salah tersebut dihapus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfedd19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 6267/6267 [00:00<00:00, 15508.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before: 6267 | after: 6206 | removed: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6267/6267 [00:01<00:00, 4064.30 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'language', 'document_title', 'passage_text', 'query', 'answers', 'question_norm', 'question_stem'],\n",
       "    num_rows: 6206\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====== 2) Daftar pasangan dari Excel (sudah dituliskan ulang ke kode) ======\n",
    "EXCEL_PAIRS = [\n",
    "    (\"apa bilang ganjil\", \"t dibuktikan bahwa bilangan ganjil\"),\n",
    "    ('apa definisi dari budaya', 'cara hidup yang berkembang, dan dimiliki bersama oleh sebuah kelompok orang, dan diwariskan dari generasi ke generasi'),\n",
    "    ('apa erti pasar dalam ilmu ekonomi', 'empat bertemunya penjual dan pembeli. Transaksi jual-beli yang terjadi tidak selalu memerlukan lokasi fisik'),\n",
    "    ('apa ibukota israel', 'Yerusalem'),\n",
    "    ('apa ibukota jerman', 'Jerman Timur memilih Berlin Timur sebagai ibukota, sedangkan Jerman Barat memilih Bonn'),\n",
    "    ('apa itu dna', '\"dsr\":[417,424,3,3]}\\'>n</b>ucleic a</b>cid), adalah sejenis biomolekul yang menyimpan dan menyandi instruksi-instru'),\n",
    "    ('apa itu manga', 'komik yang dibuat di Jepang, kata tersebut digunakan khusus untuk membicarakan tentang komik Jepang, sesuai dengan gaya yang dikembangkan di Jepang pada akhir abad ke-19'),\n",
    "    ('apa itu rna', 'molekul polimer yang terlibat dalam berbagai peran biologis dalam mengkode, dekode, regulasi, dan ekspresi gen'),\n",
    "    ('apa mata uang israel', 'Shekel baru Israel'),\n",
    "    ('apa nama ibukota israel', 'Yerusalem'),\n",
    "    ('apa nama ibukota jerman', 'Jerman Timur memilih Berlin Timur sebagai ibukota, sedangkan Jerman Barat memilih Bonn'),\n",
    "    ('apa nama mata uang israel', 'Shekel baru Israel'),\n",
    "    ('apa warna bendera jepang', '6.2R 4/15.2 untuk merah dan N9.2 untuk putih'),\n",
    "    ('apa yang maksud dengan budaya', 'suatu cara hidup yang berkembang, dan dimiliki bersama oleh sebuah kelompok orang, dan diwariskan dari generasi ke generasi'),\n",
    "    ('apa yang maksud dengan monarki', 'merupakan sejenis pemerintahan yang dipimpin oleh seorang penguasa monarki'),\n",
    "    ('apa yang maksud dengan surah makkiyah', 'yat-ayat yang turun sebelum Rasulullah SAW hijrah ke Madinah'),\n",
    "    ('apa yang maksud dengan zaman meiji', 'salah satu nama zaman pemerintahan kaisar Jepang sewaktu Kaisar Meiji memerintah Jepang, sesudah tahun Keiō(慶応) dan sebelum zaman zaman Taishō(大正'),\n",
    "    ('berapa luas danau toba', '100 kilometres (62 miles), lebar 30 kilometres (19mi), dan kedalaman 505 metres'),\n",
    "    ('berapa luas jakarta timur', '187,75km² (menurut Pemerintah Kota Administrasi Jakarta Timur)[1], atau seluas 188,19km² (menurut Badan Perencanaan Pembangunan Daerah Khusus Ibu Kota Jakarta)'),\n",
    "    ('berapa luas jerman', '357,021km'),\n",
    "    ('berapa luas kolombia', \"828,660km² dan yang kedua yang terbentuk oleh pegunungan Andes dan lempeng Llanos yang berbagi bersama Venezuela dengan area seluas 1'143,748km²\"), \n",
    "    ('berapa luas kota lampung', '35.376,50km²'),\n",
    "    ('berapa luas kuasa sultan mataram', 'Jawa dan sekitarnya, termasuk Madura'),\n",
    "    ('berapa luas negara israel',\t'27.799km2'),\n",
    "    (\"berapa luas papua\",\"808.105km persegi\"),\n",
    "    ('berapa luas pulau jawa','160 juta'),\n",
    "    ('berapa luas singapura', '5,815km'),\n",
    "    ('berapa luas sulawesi selatan','45.764,53'),\n",
    "    ('berapa panjang sungai kuning'\t,'40km'),\n",
    "    ('dari mana asal suku sunda','bagian barat pulau Jawa, Indonesia'),\n",
    "    (\"kapan dinasti tang diri\",\"618\"),\n",
    "    (\"kapan internet pertama masuk ke indonesia\",\"1994\"),\n",
    "    (\"kapan perintah orde baru mulai\",\"1966\"),\n",
    "    (\"kapan pt kereta api indonesia diri\",\"Mei 2010\"),\n",
    "    (\"kapan pt kereta api indonesia diri\",\"17 Juni 1864\"),\n",
    "    (\"kapan radio pertama kali cipta\",\"Guglielmo Marconi\"),\n",
    "    (\"kapan universitas indonesia diri\",\"1955\"),\n",
    "    (\"mana letak jam gadang\",\"kota Bukittinggi, Sumatera Barat, Indonesia\"),\n",
    "    (\"mana letak jepang\",\"ujung barat Samudra Pasifik, di sebelah timur Laut Jepang, dan bertetangga dengan Republik Rakyat Tiongkok, Korea, dan Rusia\"),\n",
    "    (\"mana letak mesir\",\"antara garis lintang 22 ° dan 32 ° N, dan garis bujur 25 ° dan 35 ° E\"),\n",
    "    (\"mana letak polandia\",\"Eropa Tengah yang berbatasan dengan Jerman di sebelah barat Perbatasan Oder-Neisse, Ceko, dan Slowakia di sebelah selatan, Rusia (Kaliningrad), Lituania di sebelah timur laut dan Belarus serta Ukraina di sebelah barat (Garis Curzon)\"),\n",
    "    (\"siapa diri dinasti han timur\",\"Liu Bang\"),\n",
    "    (\"siapa diri toyota motor corporation\",\"Toyoda\"),\n",
    "    (\"siapa nama kaisar pertama dinasti ming\",\"Zhu Yuanzhang\"),\n",
    "    (\"siapa yang cipta wiracarita mahabharata\",\"Begawan Byasa atau Vyasa\"), \n",
    "    (\"apa erti dari digital\", \"penggambaran dari suatu keadaan bilangan yang terdiri dari angka 0 dan 1 atau off dan on (bilangan biner)\"),\n",
    "    (\"berapa luas jambi\", \"53.435 km2\"),\n",
    "    (\"kapan sultan utsmaniyah diri\", \"bawa\"),\n",
    "    (\"apa itu dvd\", \"cakram padat yang dapat digunakan untuk menyimpan data, termasuk film dengan kualitas video dan audio yang lebih baik dari kualitas VCD\"),\n",
    "    (\"apa nama ibukota provinsi sulawesi utara\", \"kota Manado\"), \n",
    "    (\"apa nama mata uang korea utara\", \"Won\"), \n",
    "    (\"berapa luas samudera atlantik\", \"106.450.000km²\"),\n",
    "    (\"mana letak danau toba\", \"kaldera Gunung Berapi Super\")\n",
    "]\n",
    "\n",
    "# Ubah ke set untuk lookup cepat; trimming spasi di kiri/kanan biar \"persis\" tapi toleran spasi ekstra\n",
    "PAIRS_SET = set((qs.strip(), ans.strip()) for qs, ans in EXCEL_PAIRS)\n",
    "\n",
    "# ====== 3) Hapus baris di tydiqa_gold_all yang match kombinasi (question_stem, answers.text) ======\n",
    "# Asumsi: kolom 'question_stem' SUDAH ada di tydiqa_gold_all\n",
    "def _should_keep(example):\n",
    "    qstem = (example.get('question_stem') or \"\").strip()\n",
    "    texts = example.get('answers', {}).get('text', []) or []\n",
    "    # hapus baris bila ADA salah satu jawaban yang match persis\n",
    "    for t in texts:\n",
    "        if (qstem, (t or \"\").strip()) in PAIRS_SET:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "before_n = len(tydiqa_gold_all)\n",
    "filtered_train = tydiqa_gold_all.filter(_should_keep)\n",
    "after_n = len(filtered_train)\n",
    "removed = before_n - after_n\n",
    "\n",
    "print(f\"Rows before: {before_n} | after: {after_n} | removed: {removed}\")\n",
    "\n",
    "# (Opsional) laporan pasangan Excel yang tidak ketemu sama sekali di dataset\n",
    "# Kita cek apakah ada contoh yang match; kalau tidak ada, kita laporkan\n",
    "def _has_match(example):\n",
    "    qstem = (example.get('question_stem') or \"\").strip()\n",
    "    texts = example.get('answers', {}).get('text', []) or []\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        key = (qstem, (t or \"\").strip())\n",
    "        if key in PAIRS_SET:\n",
    "            out.append(key)\n",
    "    return {\"_hits\": out}\n",
    "\n",
    "hits_ds = tydiqa_gold_all.map(_has_match, batched=False)\n",
    "found_pairs = set()\n",
    "for row in hits_ds:\n",
    "    for h in row.get(\"_hits\", []):\n",
    "        found_pairs.add(tuple(h))\n",
    "\n",
    "not_found = PAIRS_SET - found_pairs\n",
    "if not_found:\n",
    "    print(\"\\nPairs from Excel NOT FOUND in dataset:\")\n",
    "    for qs, ans in sorted(not_found):\n",
    "        print(f\"- question_stem={qs!r} | answers.text={ans!r}\")\n",
    "\n",
    "# Commit hasil filter\n",
    "tydiqa_gold_all_dupe_wo_bad_labels = filtered_train\n",
    "tydiqa_gold_all_dupe_wo_bad_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd5a049",
   "metadata": {},
   "source": [
    "Ekstrak jawaban dari yang awalnya berada pada answers.text menjadi answer saja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9938f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6206/6206 [00:01<00:00, 3501.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_text(example):\n",
    "    example[\"answer\"] = example[\"answers\"][\"text\"]  # Ambil hanya bagian text, start_byte dan limit_byte dibuang saja\n",
    "    return example\n",
    "\n",
    "tydiqa_gold_all_dupe_wo_bad_labels = tydiqa_gold_all_dupe_wo_bad_labels.map(extract_text)\n",
    "tydiqa_gold_all_dupe_wo_bad_labels = tydiqa_gold_all_dupe_wo_bad_labels.remove_columns([\"language\", \"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f638c7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 6206/6206 [00:00<00:00, 263060.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Simpan ke folder lokal\n",
    "tydiqa_gold_all_dupe_wo_bad_labels.save_to_disk(\"tydiqa_gold_all_dupe_wo_bad_labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fde1844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 6955/6955 [00:00<00:00, 47878.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "mr_tydi_all.save_to_disk('mr_tydi_all_stem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04004d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document_title', 'passage_text', 'query', 'question_norm', 'question_stem', 'answer'],\n",
       "    num_rows: 6206\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "tydiqa_gold_all_dupe_wo_bad_labels = load_from_disk(\"tydiqa_gold_all_dupe_wo_bad_labels\")\n",
    "tydiqa_gold_all_dupe_wo_bad_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df066546",
   "metadata": {},
   "source": [
    "Inspect lagi, apakah untuk kelompok question_stem sama (terduplikasi), apakah ada passage_text yang passage_text nya berbeda satu sama lain. Kalau berbeda, cek apakah jawaban berbeda juga. Jika jawaban berbeda, cari baris yang jawabannya benar dan sesuai dengan passage_text. Jawaban yang salah kemudian dihapus. Pencarian jawaban salah dilakukan dengan manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794b328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total question_stem duplikat: 224\n",
      "Grup konsisten (judul & passage sama semua): 205\n",
      "Grup konflik (judul/passage berbeda): 19\n",
      "\n",
      "=== Contoh konflik (maks 5) ===\n",
      "- question_stem: 'apa ibukota rusia' | n_rows=2, n_titles=1, n_passages=2\n",
      "  titles_sample: ['Rusia']\n",
      "  passages_sample: ['Sebagian besar jalur air Rusia sepanjang 102,000km (63,380mi) terdiri dari sungai atau danau alam. Ibukota Moskwa disebut sebagai \"pelabuhan lima laut\" karena memiliki 5 jalur air menuju Baltik, Laut Putih, Kaspia, Laut Azov dan Laut Hitam.']\n",
      "------------------------------------------------------------\n",
      "- question_stem: 'apa mata uang yang guna di jerman' | n_rows=2, n_titles=1, n_passages=2\n",
      "  titles_sample: ['Mark Jerman']\n",
      "  passages_sample: ['Peralihan dari mata uang resmi yang lama ke euro di Jerman berbeda dengan negara-negara Zona Euro lainnya. Di negara-negara lain, saat peralihan mata uang resmi lama dan euro beredar berdampingan selama dua bulan. Tetapi, di Jerman, koin dan uang kertas mark masih diterima sebagai pembayaran yang sah hingga 28 Februari 2002. Dan pihak Deutsche Bundesbank menjamin bahwa semua uang mark tunai dapat diganti ke Euro tanpa batas waktu dan dapat dilakukan di semua cabang Bundesbank di Jerman. Uang kertas bahkan dapat dikirim ke bank melalui pos.[1]']\n",
      "------------------------------------------------------------\n",
      "- question_stem: 'apa nama ibukota jepang' | n_rows=2, n_titles=1, n_passages=2\n",
      "  titles_sample: ['Ibu kota Jepang']\n",
      "  passages_sample: ['Tokyo diperlakukan secara de facto sebagai ibu kota karena menurut Konstitusi Jepang, Kaisar Jepang sebagai \"lambang negara Jepang dan simbol pemersatu rakyat Jepang\" dan istana kaisar berkedudukan di Tokyo. Selain itu, lembaga-lembaga pemerintah seperti Parlemen Jepang, Kantor Perdana Menteri (Kantei) dan Mahkamah Agung Jepang yang ditetapkan konstitusi sebagai \"lembaga tertinggi negara\" berada di distrik Chiyoda, Tokyo.']\n",
      "------------------------------------------------------------\n",
      "- question_stem: 'apa nama pulau tengah danau toba' | n_rows=2, n_titles=1, n_passages=2\n",
      "  titles_sample: ['Danau Toba']\n",
      "  passages_sample: ['Kejadian ini menyebabkan kematian massal dan kepunahan pada beberapa spesies makhluk hidup. Menurut beberapa bukti DNA, letusan ini juga menyusutkan jumlah manusia sampai sekitar 60% dari jumlah populasi manusia bumi saat itu, yaitu sekitar 60 juta manusia. Letusan itu juga ikut menyebabkan terjadinya zaman es, walaupun para ahli masih memperdebatkannya. Setelah letusan tersebut, terbentuk kaldera yang kemudian terisi oleh air dan menjadi yang sekarang dikenal sebagai Danau Toba. Tekanan ke atas oleh magma yang belum keluar menyebabkan munculnya Pulau Samosir.\\n\\n\\nTim peneliti multidisiplin internasional, yang dipimpin oleh Dr. Michael Petraglia, mengungkapkan dalam suatu konferensi pers di Oxford, Amerika Serikat bahwa telah ditemukan situs arkeologi baru yang cukup spektakuler oleh para ahli geologi di selatan dan utara India. Di situs itu terungkap bagaimana orang bertahan hidup, sebelum dan sesudah letusan gunung berapi (supervolcano) Toba pada 74.000 tahun yang lalu, dan bukti tentang adanya kehidupan di bawah timbunan abu Gunung Toba. Padahal sumber letusan berjarak 3.000 mil, dari sebaran abunya.']\n",
      "------------------------------------------------------------\n",
      "- question_stem: 'apa nama video game pertama di dunia' | n_rows=2, n_titles=1, n_passages=2\n",
      "  titles_sample: ['OXO']\n",
      "  passages_sample: ['OXO memiliki gambar seperti papan skor Olimpiade atau \"pod\". Ada keraguan OXO adalah permainan video pertama, yang lebih dulu terbit daripada permainan Tennis For Two yang terbit tahun 1958']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# pastikan kolom yang dibutuhkan ada\n",
    "required_cols = {\"question_stem\", \"document_title\", \"passage_text\"}\n",
    "missing = required_cols - set(tydiqa_gold_all_dupe_wo_bad_labels.column_names)\n",
    "assert not missing, f\"Kolom hilang: {missing}\"\n",
    "\n",
    "# ke pandas untuk analisis cepat\n",
    "df = tydiqa_gold_all_dupe_wo_bad_labels.to_pandas()\n",
    "\n",
    "# ambil hanya question_stem yang duplikat (>1 kemunculan)\n",
    "dup_keys = (\n",
    "    df.groupby(\"question_stem\", dropna=False).size()\n",
    "      .reset_index(name=\"n\")\n",
    "      .query(\"n > 1\")[\"question_stem\"]\n",
    ")\n",
    "\n",
    "dup_df = df[df[\"question_stem\"].isin(dup_keys)].copy()\n",
    "\n",
    "# ringkasan per question_stem\n",
    "def _unique_list(s, k=3):\n",
    "    # ambil contoh unik maksimal k item untuk pratinjau\n",
    "    vals = pd.unique(s.dropna())\n",
    "    return list(vals[:k])\n",
    "\n",
    "summary = (\n",
    "    dup_df.groupby(\"question_stem\", dropna=False)\n",
    "          .agg(\n",
    "              n_rows=(\"question_stem\", \"size\"),\n",
    "              n_titles=(\"document_title\", pd.Series.nunique),\n",
    "              n_passages=(\"passage_text\", pd.Series.nunique),\n",
    "              titles_sample=(\"document_title\", lambda s: _unique_list(s, k=3)),\n",
    "              passages_sample=(\"passage_text\", lambda s: _unique_list(s, k=1)),\n",
    "          )\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "summary[\"same_title\"] = summary[\"n_titles\"].eq(1)\n",
    "summary[\"same_passage\"] = summary[\"n_passages\"].eq(1)\n",
    "summary[\"all_same\"] = summary[\"same_title\"] & summary[\"same_passage\"]\n",
    "\n",
    "# baris yang tidak konsisten (judul atau passage berbeda)\n",
    "conflicts = summary.query(\"~all_same\").copy()\n",
    "\n",
    "print(f\"Total question_stem duplikat: {len(summary)}\")\n",
    "print(f\"Grup konsisten (judul & passage sama semua): {summary['all_same'].sum()}\")\n",
    "print(f\"Grup konflik (judul/passage berbeda): {len(conflicts)}\")\n",
    "\n",
    "# contoh menampilkan beberapa konflik\n",
    "if len(conflicts):\n",
    "    print(\"\\n=== Contoh konflik (maks 5) ===\")\n",
    "    for _, r in conflicts.head(5).iterrows():\n",
    "        print(f\"- question_stem: {r['question_stem']!r} | n_rows={r['n_rows']}, \"\n",
    "              f\"n_titles={r['n_titles']}, n_passages={r['n_passages']}\")\n",
    "        print(f\"  titles_sample: {r['titles_sample']}\")\n",
    "        print(f\"  passages_sample: {r['passages_sample']}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "# (Opsional) simpan hasil\n",
    "summary.to_excel(\"tydiqa_gold_dupe_groups_summary.xlsx\", index=False)\n",
    "conflicts.to_excel(\"tydiqa_gold_dupe_groups_conflicts.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563ec15",
   "metadata": {},
   "source": [
    "Penghapusan baris berdasarkan question_stem dan passage_text/answer yang salah. Baris berikut sudah diketahui query nya terduplikasi. Berhubung jawabannya pun salah, sekalian aja hapus dulu, biar nanti pas proses penghapusan query ga sengaja meninggalkan baris dengan answer yang salah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b22d519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6206/6206 [00:00<00:00, 6633.72 examples/s]\n",
      "Filter: 100%|██████████| 6206/6206 [00:00<00:00, 38111.06 examples/s]\n",
      "Filter: 100%|██████████| 6206/6206 [00:00<00:00, 38009.10 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before: 6206 | after: 6200 | removed: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ====== daftar pasangan ======\n",
    "Q_ANSWER_PAIRS = [\n",
    "    (\"apa yang maksud dengan uskup\",\n",
    "     \"pimpinan Gereja setempat yang bernama Keuskupan dan merupakan bagian dari hierarki Gereja Katolik Roma setelah Sri Paus (Uskup Agung Roma) dan Kardinal\"),\n",
    "    (\"berapa lama kaisar romawi kuasa\", \"27 SM sampai 284 M\"),\n",
    "    (\"berapa luas korea selatan\", \"99.274km2\"),\n",
    "    (\"berapa luas negara jepang\", \"377.835km²\"),\n",
    "]\n",
    "\n",
    "Q_PSGTEXT_STARTSWITH_PAIRS = [\n",
    "    (\"kapan internet masuk ke indonesia pertama kali\",\n",
    "     \"Sejarah internet Indonesia dimulai pada awal tahun 1990-an.\"),\n",
    "    (\"kapan komputer pertama kali cipta\",\n",
    "     \"Sebelum Internet muncul, telah ada beberapa sistem komunikasi yang berbasis digital\"),\n",
    "]\n",
    "\n",
    "# Lookup cepat\n",
    "ANSWER_SET = set((q.strip(), a.strip()) for q, a in Q_ANSWER_PAIRS)\n",
    "PFX_LIST  = [(q.strip(), pfx.strip()) for q, pfx in Q_PSGTEXT_STARTSWITH_PAIRS]\n",
    "\n",
    "def _iter_answers(val):\n",
    "    \"\"\"Yield jawaban sebagai string-strip, terlepas val= str / list / tuple / np.ndarray / None.\"\"\"\n",
    "    if val is None:\n",
    "        return\n",
    "    if isinstance(val, (list, tuple, np.ndarray)):\n",
    "        for x in val:\n",
    "            if x is None:\n",
    "                continue\n",
    "            yield str(x).strip()\n",
    "    else:\n",
    "        yield str(val).strip()\n",
    "\n",
    "def _should_drop(example):\n",
    "    qstem = (example.get(\"question_stem\") or \"\").strip()\n",
    "    psg   = (example.get(\"passage_text\") or \"\").strip()\n",
    "\n",
    "    # Rule 1: (question_stem, answer) exact match\n",
    "    for a in _iter_answers(example.get(\"answer\")):\n",
    "        if (qstem, a) in ANSWER_SET:\n",
    "            return True\n",
    "\n",
    "    # Rule 2: (question_stem, passage_text startswith prefix)\n",
    "    for q, pfx in PFX_LIST:\n",
    "        if qstem == q and psg.startswith(pfx):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# === Flag → filter ===\n",
    "before_n = len(tydiqa_gold_all_dupe_wo_bad_labels)\n",
    "\n",
    "flagged = tydiqa_gold_all_dupe_wo_bad_labels.map(\n",
    "    lambda ex: {\"_drop\": _should_drop(ex)}, batched=False\n",
    ")\n",
    "\n",
    "removed_rows = flagged.filter(lambda ex: ex[\"_drop\"])\n",
    "kept         = flagged.filter(lambda ex: not ex[\"_drop\"]).remove_columns([\"_drop\"])\n",
    "\n",
    "after_n   = len(kept)\n",
    "removed_n = before_n - after_n\n",
    "print(f\"Rows before: {before_n} | after: {after_n} | removed: {removed_n}\")\n",
    "\n",
    "# Commit hasil bersih kembali ke variabel asal (kalau diinginkan)\n",
    "tydiqa_gold_all_dupe_wo_bad_labels = kept\n",
    "\n",
    "# (Opsional) simpan audit baris yang dihapus\n",
    "# removed_rows.to_csv(\"tydiqa_removed_rows.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ab06fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document_title', 'passage_text', 'query', 'question_norm', 'question_stem', 'answer'],\n",
       "    num_rows: 6200\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tydiqa_gold_all_dupe_wo_bad_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c17cf",
   "metadata": {},
   "source": [
    "Penghapusan baris yang terduplikasi dengan menyisakan passage_text yang lebih pendek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b933cdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 6200 | After: 5928 | Removed: 272\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# convert ke pandas\n",
    "df = tydiqa_gold_all_dupe_wo_bad_labels.to_pandas()\n",
    "\n",
    "# hitung panjang passage_text (kalau NaN → inf supaya tidak kepilih)\n",
    "df[\"_passage_len\"] = df[\"passage_text\"].fillna(\"\").str.len()\n",
    "\n",
    "# untuk tiap question_stem, ambil index baris dengan passage_text terpendek\n",
    "idx_min = df.groupby(\"question_stem\")[\"_passage_len\"].idxmin()\n",
    "\n",
    "# ambil hanya baris-baris itu\n",
    "df_kept = df.loc[idx_min].reset_index(drop=True)\n",
    "\n",
    "print(f\"Before: {len(df)} | After: {len(df_kept)} | Removed: {len(df) - len(df_kept)}\")\n",
    "\n",
    "# buang kolom bantu\n",
    "df_kept = df_kept.drop(columns=[\"_passage_len\"])\n",
    "\n",
    "# convert balik ke HuggingFace Dataset\n",
    "tydiqa_gold_all_dupe_wo_bad_labels = Dataset.from_pandas(df_kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a03d8ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document_title', 'passage_text', 'query', 'question_norm', 'question_stem', 'answer'],\n",
       "    num_rows: 5928\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tydiqa_gold_all_dupe_wo_bad_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d177bc9c",
   "metadata": {},
   "source": [
    "Isi text dari split dev dan test mr_tydi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db38c22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query_id': '3', 'query': 'Dimana James Hepburn meninggal?', 'positive_passages': [{'docid': '2386357#15', 'text': 'Dia dipenjarakan di Puri Dragsholm, 75 kilometer Kopenhagen. Dia ditahan dalam apa yang dikatakan sebagai kondisi yang mengerikan. Dia meninggal pada bulan April 1578.[8][10]', 'title': 'James Hepburn'}], 'negative_passages': []}\n",
      "{'query_id': '6148', 'query': 'Siapakah yang menemuka benua Amerika ?', 'positive_passages': [{'docid': '6874#1', 'text': \"Kolumbus bukanlah orang pertama yang tiba di Amerika, yang ia dapati sudah diduduki. Ia juga bukan orang Eropa pertama yang sampai ke benua itu karena sekarang telah diakui secara meluas bahwa orang-orang Viking dari Eropa Utara telah berkunjung ke Amerika Utara pada abad ke 11 dan mendirikan koloni L'Anse aux Meadows untuk jangka waktu singkat. Terdapat perkiraan bahwa pelayar yang tidak dikenali pernah melawat ke Amerika sebelum Kolumbus dan membekalkannya dengan sumber untuk kejayaannya. Terdapat juga banyak teori mengenai ekspedisi ke Amerika oleh berbagai orang sepanjang masa itu.\", 'title': 'Kristoforus Kolumbus'}], 'negative_passages': []}\n"
     ]
    }
   ],
   "source": [
    "# Buat dictionary {docid: (title, text)} untuk pencarian cepat\n",
    "corpus_dict = {row[\"docid\"]: (row[\"title\"], row[\"text\"]) for row in corpus[\"train\"]}\n",
    "\n",
    "# Fungsi untuk melengkapi positive_passages dalam dataset mr_tydi\n",
    "def fill_passage_info(example):\n",
    "    for passage in example[\"positive_passages\"]:\n",
    "        docid = passage[\"docid\"]\n",
    "        if docid in corpus_dict:  # Cek apakah docid ada di corpus\n",
    "            passage[\"title\"], passage[\"text\"] = corpus_dict[docid]\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Terapkan fungsi untuk melengkapi positive_passages di split 'dev' dan 'test'\n",
    "mr_tydi[\"dev\"] = mr_tydi[\"dev\"].map(fill_passage_info)\n",
    "mr_tydi[\"test\"] = mr_tydi[\"test\"].map(fill_passage_info)\n",
    "\n",
    "# Cek hasilnya\n",
    "print(mr_tydi[\"dev\"][0])\n",
    "print(mr_tydi[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4dbe07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan semua split di mr_tydi\n",
    "mr_tydi_all = concatenate_datasets([mr_tydi[split] for split in mr_tydi.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708db2ac",
   "metadata": {},
   "source": [
    "Normalisasi dan stemming query mr_tydi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60676053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6955/6955 [11:13<00:00, 10.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "mr_tydi_all = mr_tydi_all.map(\n",
    "    add_norm_and_stem, \n",
    "    batched=True, \n",
    "    fn_kwargs={\"column\": \"query\"}     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae89066",
   "metadata": {},
   "source": [
    "Join kedua dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e615ea0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mr_tydi_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 1) Bangun index dari mr_tydi_all: query_norm -> list kandidat mr\u001b[39;00m\n\u001b[0;32m      4\u001b[0m mr_index \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# query_norm -> [ {query_id, positive_passages_raw, negative_passages_raw, positives_norm:[...]} ]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qnorm, qid, pos_list, neg_list \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m----> 6\u001b[0m     mr_tydi_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      7\u001b[0m     mr_tydi_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      8\u001b[0m     mr_tydi_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive_passages\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      9\u001b[0m     mr_tydi_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative_passages\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     10\u001b[0m ):\n\u001b[0;32m     11\u001b[0m     rec \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: qid,\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive_passages_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m: pos_list \u001b[38;5;129;01mor\u001b[39;00m [],\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative_passages_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m: neg_list \u001b[38;5;129;01mor\u001b[39;00m [],\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositives_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[0;32m     16\u001b[0m     }\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# siapkan versi ternormalisasi utk pencocokan cepat\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mr_tydi_all' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Bangun index dari mr_tydi_all: query_norm -> list kandidat mr\n",
    "mr_index = {}  # query_norm -> [ {query_id, positive_passages_raw, negative_passages_raw, positives_norm:[...]} ]\n",
    "for qnorm, qid, pos_list, neg_list in zip(\n",
    "    mr_tydi_all[\"query_norm\"],\n",
    "    mr_tydi_all[\"query_id\"],\n",
    "    mr_tydi_all[\"positive_passages\"],\n",
    "    mr_tydi_all[\"negative_passages\"],\n",
    "):\n",
    "    rec = {\n",
    "        \"query_id\": qid,\n",
    "        \"positive_passages_raw\": pos_list or [],\n",
    "        \"negative_passages_raw\": neg_list or [],\n",
    "        \"positives_norm\": []\n",
    "    }\n",
    "    # siapkan versi ternormalisasi utk pencocokan cepat\n",
    "    for p in (pos_list or []):\n",
    "        title = (p.get(\"title\") or \"\")\n",
    "        text  = (p.get(\"text\")  or \"\")\n",
    "        rec[\"positives_norm\"].append({\n",
    "            \"docid\": p.get(\"docid\"),\n",
    "            \"title\": title,\n",
    "            \"title_norm\": normalize(title),\n",
    "            \"text\": text,\n",
    "            \"text_norm\": normalize(text),\n",
    "        })\n",
    "    mr_index.setdefault(qnorm, []).append(rec)\n",
    "\n",
    "# 2) Map ke tiap baris tydiqa: tambahkan kolom hasil join\n",
    "def attach_mr_match(example):\n",
    "    qnorm = example.get(\"question_norm\") or \"\"\n",
    "    cands = mr_index.get(qnorm, [])\n",
    "\n",
    "    title_norm = normalize(example.get(\"document_title\") or \"\")\n",
    "    psg_norm   = normalize(example.get(\"passage_text\")   or \"\")\n",
    "\n",
    "    # default (tidak ketemu query_norm)\n",
    "    if not cands:\n",
    "        return {\n",
    "            \"mr_query_id\": None,\n",
    "            \"mr_docid\": None,\n",
    "            \"mr_title\": None,\n",
    "            \"mr_text\": None,\n",
    "            \"mr_negative_passages\": [],   # <- kolom yang kamu minta ikut dibawa\n",
    "            \"mr_query_norm_match\": False, # hanya query_norm?\n",
    "            \"mr_pos_match\": False,        # apakah positive_passages (title+text) match?\n",
    "        }\n",
    "\n",
    "    # ada kandidat dengan query_norm sama\n",
    "    # coba cari positive yang match (title & text)\n",
    "    for cand in cands:\n",
    "        for pos in cand[\"positives_norm\"]:\n",
    "            if pos[\"title_norm\"] == title_norm and pos[\"text_norm\"] == psg_norm:\n",
    "                return {\n",
    "                    \"mr_query_id\": cand[\"query_id\"],\n",
    "                    \"mr_docid\": pos[\"docid\"],\n",
    "                    \"mr_title\": pos[\"title\"],\n",
    "                    \"mr_text\": pos[\"text\"],\n",
    "                    \"mr_negative_passages\": cand[\"negative_passages_raw\"],  # ikutkan apa adanya\n",
    "                    \"mr_query_norm_match\": True,\n",
    "                    \"mr_pos_match\": True,\n",
    "                }\n",
    "\n",
    "    # tidak ada positive yang match; tetap pilih kandidat pertama agar negative_passages tetap terisi\n",
    "    cand0 = cands[0]\n",
    "    return {\n",
    "        \"mr_query_id\": cand0[\"query_id\"],\n",
    "        \"mr_docid\": None,\n",
    "        \"mr_title\": None,\n",
    "        \"mr_text\": None,\n",
    "        \"mr_negative_passages\": cand0[\"negative_passages_raw\"],\n",
    "        \"mr_query_norm_match\": True,\n",
    "        \"mr_pos_match\": False,\n",
    "    }\n",
    "\n",
    "# 3) Terapkan ke dataset tydiqa (tanpa menduplikasi baris)\n",
    "tydiqa_with_mr = tydiqa_gold_all_dupe_wo_bad_labels.map(\n",
    "    attach_mr_match,\n",
    "    batched=False,\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "# (opsional) ringkas hasil\n",
    "total = len(tydiqa_with_mr)\n",
    "qnorm_hits = sum(1 for b in tydiqa_with_mr[\"mr_query_norm_match\"] if b)\n",
    "pos_hits   = sum(1 for b in tydiqa_with_mr[\"mr_pos_match\"] if b)\n",
    "print(f\"Query-norm matched: {qnorm_hits}/{total}\")\n",
    "print(f\"Positive-passage fully matched (title+text): {pos_hits}/{total}\")\n",
    "\n",
    "# hasil akhirnya: tydiqa_with_mr punya kolom tambahan:\n",
    "# ['mr_query_id','mr_docid','mr_title','mr_text','mr_negative_passages','mr_query_norm_match','mr_pos_match']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb9d1d",
   "metadata": {},
   "source": [
    "Buang baris yang tidak nemu pasangan di mr_tydi_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1be2cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5928/5928 [00:01<00:00, 3085.92 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah orphan: 16\n",
      "File tydiqa_orphans.xlsx berhasil dibuat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5928/5928 [00:01<00:00, 3129.85 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sekarang tydiqa_with_mr berisi 5912 baris (orphan sudah dihapus).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) ambil orphan (tidak ada pasangan query_norm di mr_tydi_all)\n",
    "orphans = tydiqa_with_mr.filter(lambda ex: not ex[\"mr_query_norm_match\"])\n",
    "print(f\"Jumlah orphan: {len(orphans)}\")\n",
    "\n",
    "# convert ke pandas biar bisa dicek/ekspor\n",
    "orphans_df = orphans.to_pandas()\n",
    "orphans_df.to_excel(\"tydiqa_orphans.xlsx\", index=False)\n",
    "print(\"File tydiqa_orphans.xlsx berhasil dibuat.\")\n",
    "\n",
    "# 2) buang orphan dari dataset utama\n",
    "tydiqa_with_mr = tydiqa_with_mr.filter(lambda ex: ex[\"mr_query_norm_match\"])\n",
    "print(f\"Sekarang tydiqa_with_mr berisi {len(tydiqa_with_mr)} baris (orphan sudah dihapus).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2795f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# pola 'jmpl|' toleran spasi & case-insensitive\n",
    "JMPL_RE = re.compile(r\"jmpl\\s*\\|\", flags=re.IGNORECASE)\n",
    "\n",
    "# 1) ambil baris yang mengandung 'jmpl|'\n",
    "def has_jmpl(example):\n",
    "    txt = example.get(\"passage_text\") or \"\"\n",
    "    return bool(JMPL_RE.search(txt))\n",
    "\n",
    "jmpl_ds = tydiqa_with_mr.filter(has_jmpl)\n",
    "print(f\"Baris yang mengandung 'jmpl|': {len(jmpl_ds)}\")\n",
    "\n",
    "# 2) tampilkan sampai 5 contoh (ke layar)\n",
    "n_show = min(5, len(jmpl_ds))\n",
    "if n_show > 0:\n",
    "    print(\"\\n=== Contoh passage dengan 'jmpl|' (maks 5) ===\")\n",
    "    for r in jmpl_ds.select(range(n_show)):\n",
    "        print(f\"id: {r.get('id')}\")\n",
    "        print(f\"question_stem: {r.get('question_stem')}\")\n",
    "        print(f\"document_title: {r.get('document_title')}\")\n",
    "        # tampilkan potongan teks agar mudah dibaca\n",
    "        psg = (r.get('passage_text') or \"\").replace(\"\\n\", \" \")\n",
    "        print(\"passage_text (snippet):\", psg[:400], \"...\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"Tidak ada 'jmpl|' yang ditemukan. Ambil 5 sampel acak untuk inspeksi manual.\")\n",
    "    n_show = min(5, len(tydiqa_with_mr))\n",
    "    for r in tydiqa_with_mr.shuffle(seed=42).select(range(n_show)):\n",
    "        print(f\"id: {r.get('id')}\")\n",
    "        print(f\"question_stem: {r.get('question_stem')}\")\n",
    "        print(f\"document_title: {r.get('document_title')}\")\n",
    "        psg = (r.get('passage_text') or \"\").replace(\"\\n\", \" \")\n",
    "        print(\"passage_text (snippet):\", psg[:400], \"...\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# 3) simpan ke Excel untuk inspeksi lebih nyaman\n",
    "#    - jika ada 'jmpl|', simpan contoh baris tersebut (maks 200 baris agar ringan)\n",
    "#    - kalau tidak ada, simpan 200 sampel acak\n",
    "if len(jmpl_ds) > 0:\n",
    "    out_df = jmpl_ds.select(range(min(200, len(jmpl_ds)))).to_pandas()\n",
    "    out_path = \"tydiqa_passages_with_jmpl.xlsx\"\n",
    "else:\n",
    "    out_df = tydiqa_with_mr.shuffle(seed=42).select(range(min(200, len(tydiqa_with_mr)))).to_pandas()\n",
    "    out_path = \"tydiqa_passages_sample.xlsx\"\n",
    "\n",
    "out_df.to_excel(out_path, index=False)\n",
    "print(f\"Disimpan ke: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "649571b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5912/5912 [00:00<00:00, 23094.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tydiqa_with_mr.save_to_disk(\"tydiqa_with_mr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81865f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "tydiqa_with_mr = load_from_disk(\"tydiqa_with_mr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da5f62f",
   "metadata": {},
   "source": [
    "memperbaiki pasangan query dan answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef9b86b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_update = [\n",
    "    {\n",
    "        \"q_to_find\": \"dimanakah Dr. Ernest François Eugène Douwes Dekker meninggal?\", \n",
    "        \"new_answer\" : \"TMP Cikutra, Bandung\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimanakah Kucing Pallas pertama kali ditemukan ?\", \n",
    "        \"new_answer\": \"Asia Tengah wilayah Mongolia, Cina, dan Dataran Tinggi Tibet\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimanakah Joseph Patrick \\\"Joe\\\" Kennedy dilahirkan?\", \n",
    "        \"new_q\" : \"Kapan Joseph Patrick \\\"Joe\\\" Kennedy dilahirkan?\",\n",
    "        \"new_answer\" : \"1888\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimanakah produksi film pertama dilakukan ?\", \n",
    "        \"new_q\" : \"Kapan pemutaran film pertama dilakukan ?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimanakah Fiber optik pertama kali diciptakan?\", \n",
    "        \"new_q\" : \"Kapan film mulai diproduksi menggunakan videotape?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Muhammad bin ʿAbd al-Wahhāb dilahirkan?\", \n",
    "        \"new_answer\" : \"kampung Uyainah (Najd)\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimanakah balap sepeda professional Eropa pertama diadakan ?\", \n",
    "        \"new_q\" : \"Sejak kapan Tour of Flanders diadakan?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Benito Mussolini meninggal ?\", \n",
    "        \"new_answer\" : \"Giulino di Mezzegra\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimanakah kanon Muratori ditemukan?\", \n",
    "        \"new_q\" : \"Kapan Papirus 46 diperkirakan dibuat?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Friedrich Nietzsche meninggal?\", \n",
    "        \"new_q\" : \"Kapan Friedrich Nietzsche meninggal?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana al-Ikhshīd lahir ?\", \n",
    "        \"new_answer\" : \"Baghdad\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Pangeran Xia meninggal?\", \n",
    "        \"new_q\" : \"Pada tahun berapa Dou Jiande meninggal?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Robert I Estienne lahir?\", \n",
    "        \"new_q\" : \"Kapankah Robert I Estienne lahir?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Paus Yohanes XXIII meninggal?\", \n",
    "        \"new_q\" : \"Kapan Paus Yohanes XXIII meninggal?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimanakah Napoleon meninggal?\", \n",
    "        \"new_q\" : \"Tanggal berapa Napoleon meninggal?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Utsmani berdiri ?\", \n",
    "        \"new_answer\" : \"1299\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan CIA dibentuk ?\", \n",
    "        \"new_q\" : \"Apa itu CIA?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapankah Radio pertama kali diciptakan?\", \n",
    "        \"new_q\" : \"Siapa pencipta radio?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Mariah lahir ?\", \n",
    "        \"new_answer\" : \"27 Maret 1970\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Kekaisaran Romawi mengalami masa jaya?\", \n",
    "        \"new_answer\" : \"Dua abad pertama kekaisaran\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapankah provinsi Sulawesi Barat diresmikan?\", \n",
    "        \"new_q\" : \"Berapa luas provinsi Sulawesi Barat?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Gereja Asiria Timur didirikan?\", \n",
    "        \"new_q\" : \"Siapa yang mendirikan Gereja Asiria Timur?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapankah Windows XP pertama kali diciptakan?\", \n",
    "        \"new_q\" : \"kapankah Windows XP pertama kali dirilis?\",\n",
    "        \"new_answer\" : \"25 Oktober 2001\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan musik rock pertama kali masuk ke Indonesia?\", \n",
    "        \"new_q\" : \"Siapa yang dipercaya lebih dulu memperkenalkan musik beraliran rock?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Sayf Allāh al-Maslūl lahir ?\", \n",
    "        \"new_answer\" : \"585\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Katherine Anne lahir ?\", \n",
    "        \"new_q\" : \"Dimana Katie Couric lahir?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapankah ilmu kimia dipelajari oleh manusia?\", \n",
    "        \"new_q\" : \"Reaksi kimia apa yang sudah diketahui sejak dahulu kala?\",\n",
    "        \"new_answer\" : \"pembakaran, fermentasi, dan reduksi dari bijih menjadi logam\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Ernest Miller Hemingway pertama kali menulis novel?\", \n",
    "        \"new_q\" : \"Apa judul buku pertama Ernest Hemingway yang diterbitkan?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapankah Kesultanan Utsmaniyah berdiri ?\", \n",
    "        \"new_answer\" : \"1299\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas wilayah kekuasan Romawi pada zaman dahulu ?\", \n",
    "        \"new_q\" : \"Kekaisaran Romawi menguasai daerah sekitar apa?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas kekuasaan Kerajaan Sriwijaya?\", \n",
    "        \"new_q\" : \"Wilayah mana saja yang dikuasai oleh Sriwijaya?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas kekuasaan kerajaan Banjar?\", \n",
    "        \"new_q\" : \"Apa nama kerajaan pertama di Kalimantan bagian selatan menurut mitologi\",\n",
    "        \"new_answer\" : \"Kerajaan Nan Sarunai\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"berapakah jumlah album Tantowi Yahya?\", \n",
    "        \"new_q\" : \"Apa album kedua Tantowi Yahya?\",\n",
    "        \"new_answer\" : \"Southern Dreams\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas kekuasaan dinasti Utsmaniyah ?\", \n",
    "        \"new_q\" : \"Kesultanan Utsmaniyah berkuasa atas wilayah luas di mana?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"berapakah jenis senjata mesin yang digunakan di PD II?\", \n",
    "        \"new_q\" : \"Senapan apa yang digunakan di banyak negara Persemakmuran Inggris pasca perang dunia II?\",\n",
    "        \"new_answer\" : \"Bren\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas daerah kekuasan Turki Seljuk ?\", \n",
    "        \"new_q\" : \"Sejauh mana luas daerah kekuasan Turki Seljuk ?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas kerajaan Banjar?\", \n",
    "        \"new_q\" : \"Sejauh mana luas kerajaan Banjar?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas wilayah kekaisaran Romawi?\", \n",
    "        \"new_q\" : \"Di wilayah mana saja Kekaisaran Romawi berkuasa?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas kekuasaan Kesultanan Utsmaniyah?\", \n",
    "        \"new_q\" : \"Di daerah mana kekuasaaan Kesultanan Utsmaniyah?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Siapakah yang mendirikan Sekolah Tinggi Agama Islam Balai Selasa?\", \n",
    "        \"new_answer\" : \"M. Dinar Moely dibantu Sekretaris Muchtar Komar dan Bendahara Ny. Martini\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Siapakah yang menemukan alarm pertama kali ?\", \n",
    "        \"new_passage_text\" : \"Alarm | Awalnya, penemuan baterai pada tahun 1799 dan telegraf pada tahun 1841 secara umum mengubah dunia dan menginspirasi para penyuka hobi, ahli listrik dan ilmuwan yang ada di seluruh dunia untuk melihat dan memperdalam ilmu komunikasi. Tidak lama setelah telegraf diperkenalkan, seorang dokter muda yang kaya bernama William Channing membuat sebuah sistem dari pemerintah untuk menyalurkan sinyal alarm kebakaran kepada stasiun pemadam kebakaran yang ada di sekeliling kota Boston, Amerika Serikat. Menggunakan morse yang ditemukan oleh Samuel Morse dalam sistem telegram yang memadukan kode dengan teknologi, Channing membuat rencana elaborasi untuk menyalurkan sinyal dari pusat sistem pemerintah menuju stasiun pemadam kebakaran untuk memberitahu titik lokasi terjadinya kebakaran. Rencana Channing memiliki masalah karena besar bunyi alarm tidak dapat dikendalikan dari stasiun pemadam kebakaran.\"\"\"\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Siapakah yang menemukan kamera?\", \n",
    "        \"new_q\" : \"Pada tahun berapa foto pertama di dunia dihasilkan?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apakah penyebab Uni Soviet dibubarkan?\", \n",
    "        \"new_answer\" : \"kebijakan glasnost dan perestroika, tetapi justru memicu perpecahan di Uni Soviet\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apakah penyebab utama kematian Fidiricu II?\", \n",
    "        \"new_q\" : \"Kapan wafatnya Fidiricu II?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Apakah penyebab utama Dinasti Ayyubiyyah runtuh?\", \n",
    "        \"new_q\" : \"Perjanjian antara Mamluk dan Ayyubiyah ditandatangani kapan?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Apakah kepanjangan dari DNA?\", \n",
    "        \"new_answer\" : \"deoxyribonucleid acid\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Di hari apakah Amstel Gold Race diadakan pertama kali?\",\n",
    "        \"new_q\" : \"Kapan Amstel Gold Race mulai tercatat dalam kalender resmi UCI?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Mengapa Kerajaan Joseon berakhir?\", \n",
    "        \"new_q\" : \"Kapan Kerajaan Joseon berakhir?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Gianfranco Chiarini belajar memasak?\", \n",
    "        \"new_answer\" : \"Instituto de Alta Gastronomia de Caracas\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Apa penyebab utama runtuhnya kekaisaran Bizantium ?\", \n",
    "        \"new_answer\" : \"Perang Salib Keempat tahun 1204, ketika kekaisaran ini dibubarkan secara paksa\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Apakah film pertama Alexa Ellesse Vega?\", \n",
    "        \"new_answer\" : \"Twister\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapankah Bebop mulai dipopulerkan?\", \n",
    "        \"new_q\" : \"Kapan genre musik hip hop mulai muncul di Amerika Serikat?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Apa definisi dari endokrinolog?\", \n",
    "        \"new_q\" : \"Apa definisi dari endokrinologi?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa berat Mesin kepung?\", \n",
    "        \"new_q\" : \"Hingga berapa berat mesin kepung besar\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb745fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5912/5912 [00:00<00:00, 20288.93 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baris yang diupdate: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def apply_rows_updates(\n",
    "    ds,\n",
    "    rows_to_update: List[Dict[str, Any]],\n",
    "    lowercase_norm: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Terapkan banyak update berdasarkan q_to_find (dicocokkan via question_norm).\n",
    "    Setiap item bisa berisi beberapa field opsional:\n",
    "      {\n",
    "        \"q_to_find\": str,               # WAJIB (akan dinormalisasi)\n",
    "        \"new_q\": Optional[str],         # ubah query (+ regen question_norm & question_stem)\n",
    "        \"new_answer\": Optional[str|list[str]],  # ubah kolom 'answer' (list[str]); sinkronkan answers['text'] jika ada\n",
    "        \"new_passage_text\": Optional[str],      # ubah passage_text\n",
    "      }\n",
    "    \"\"\"\n",
    "    # Precompute mapping: find_norm -> payload perubahan\n",
    "    fix_map = {}\n",
    "    for row in rows_to_update:\n",
    "        q_raw = row[\"q_to_find\"]\n",
    "        find_norm = normalize(q_raw, lowercase=lowercase_norm)\n",
    "        payload = {}\n",
    "\n",
    "        # new_q\n",
    "        if \"new_q\" in row and row[\"new_q\"] is not None:\n",
    "            new_q = str(row[\"new_q\"])\n",
    "            new_norm = normalize(new_q, lowercase=lowercase_norm)\n",
    "            new_stem = stemmer.stem(new_norm)\n",
    "            payload.update({\n",
    "                \"new_q\": new_q,\n",
    "                \"new_norm\": new_norm,\n",
    "                \"new_stem\": new_stem,\n",
    "            })\n",
    "\n",
    "        # new_answer\n",
    "        if \"new_answer\" in row and row[\"new_answer\"] is not None:\n",
    "            na = row[\"new_answer\"]\n",
    "            if isinstance(na, (list, tuple)):\n",
    "                new_answer = [str(x) for x in na]\n",
    "            else:\n",
    "                new_answer = [str(na)]\n",
    "            payload[\"new_answer\"] = new_answer\n",
    "\n",
    "        # new_passage_text\n",
    "        if \"new_passage_text\" in row and row[\"new_passage_text\"] is not None:\n",
    "            payload[\"new_passage_text\"] = str(row[\"new_passage_text\"])\n",
    "\n",
    "        # Gabungkan jika ada beberapa entry dengan q_to_find sama (yang terakhir override)\n",
    "        fix_map[find_norm] = payload\n",
    "\n",
    "    # Flag baris yang kena perubahan dan terapkan\n",
    "    def _apply(batch):\n",
    "        qnorms = batch[\"question_norm\"]\n",
    "        has_answer_col = \"answer\" in batch\n",
    "        has_answers_dict = \"answers\" in batch\n",
    "\n",
    "        # siapkan output default (copy)\n",
    "        out_query         = list(batch[\"query\"])\n",
    "        out_question_norm = list(batch[\"question_norm\"])\n",
    "        out_question_stem = list(batch[\"question_stem\"])\n",
    "        out_answer        = list(batch[\"answer\"]) if has_answer_col else None\n",
    "        out_answers_dict  = list(batch[\"answers\"]) if has_answers_dict else None\n",
    "        out_passage_text  = list(batch[\"passage_text\"]) if \"passage_text\" in batch else None\n",
    "\n",
    "        touched = [False] * len(qnorms)\n",
    "\n",
    "        for i, qn in enumerate(qnorms):\n",
    "            if qn not in fix_map:\n",
    "                continue\n",
    "            payload = fix_map[qn]\n",
    "            touched[i] = True\n",
    "\n",
    "            # update query (+norm+stem)\n",
    "            if \"new_q\" in payload:\n",
    "                out_query[i]         = payload[\"new_q\"]\n",
    "                out_question_norm[i] = payload[\"new_norm\"]\n",
    "                out_question_stem[i] = payload[\"new_stem\"]\n",
    "\n",
    "            # update answer (+sinkron answers['text'] jika ada)\n",
    "            if has_answer_col and \"new_answer\" in payload:\n",
    "                out_answer[i] = payload[\"new_answer\"]\n",
    "                if has_answers_dict:\n",
    "                    curr = dict(out_answers_dict[i]) if out_answers_dict[i] is not None else {}\n",
    "                    curr[\"text\"] = payload[\"new_answer\"]\n",
    "                    out_answers_dict[i] = curr\n",
    "\n",
    "            # update passage_text\n",
    "            if out_passage_text is not None and \"new_passage_text\" in payload:\n",
    "                out_passage_text[i] = payload[\"new_passage_text\"]\n",
    "\n",
    "        out = {\n",
    "            \"query\": out_query,\n",
    "            \"question_norm\": out_question_norm,\n",
    "            \"question_stem\": out_question_stem,\n",
    "        }\n",
    "        if has_answer_col:\n",
    "            out[\"answer\"] = out_answer\n",
    "        if has_answers_dict:\n",
    "            out[\"answers\"] = out_answers_dict\n",
    "        if out_passage_text is not None:\n",
    "            out[\"passage_text\"] = out_passage_text\n",
    "\n",
    "        # untuk ringkasan\n",
    "        out[\"_touched\"] = touched\n",
    "        return out\n",
    "\n",
    "    updated = ds.map(_apply, batched=True, load_from_cache_file=False)\n",
    "    touched_count = sum(1 for x in updated[\"_touched\"] if x)\n",
    "    print(f\"Baris yang diupdate: {touched_count}\")\n",
    "\n",
    "    if \"_touched\" in updated.column_names:\n",
    "        updated = updated.remove_columns([\"_touched\"])\n",
    "\n",
    "    return updated\n",
    "\n",
    "# Terapkan\n",
    "tydiqa_with_mr = apply_rows_updates(tydiqa_with_mr, rows_to_update, lowercase_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c8be35",
   "metadata": {},
   "source": [
    "cek apakah answer hanya satu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ee421c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total baris asli   : 5912\n",
      "Total baris subset : 558\n",
      "['ke-12 hingga abad ke-19', 'ke-12 hingga abad ke-19', 'abad ke-12 hingga abad ke-19']\n"
     ]
    }
   ],
   "source": [
    "# subset hanya baris dengan jumlah elemen answer > 1\n",
    "tydiqa_multi_answer = tydiqa_with_mr.filter(lambda ex: len(ex[\"answer\"]) > 1)\n",
    "\n",
    "print(f\"Total baris asli   : {len(tydiqa_with_mr)}\")\n",
    "print(f\"Total baris subset : {len(tydiqa_multi_answer)}\")\n",
    "\n",
    "# cek contoh\n",
    "print(tydiqa_multi_answer[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 558/558 [00:00<00:00, 6625.17 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total multi-answer: 558\n",
      "Total konflik     : 45\n",
      "['ilmu atau seni dalam menciptakan dan menghias lambang, beserta kajian tentang makna, asal-usul, sejarah, dan perkembangannya', 'suatu ilmu atau seni dalam menciptakan dan menghias lambang, beserta kajian tentang makna, asal-usul, sejarah, dan perkembangannya', 'panglima perang']\n",
      "['konsep Hindu, tetapi juga dipakai dalam konteks agama Buddha, untuk merujuk pada berbagai benda nyata', 'lingkaran', 'lingkaran']\n",
      "['avtur', 'aviation turbine fuel', 'jenis bahan bakar penerbangan yang dirancang untuk digunakan pada pesawat terbang yang bermesin turbin gas']\n",
      "['mata hitam, rambut hitam yang tak pernah memanjang dan memiliki ekor', 'gaya rambut yang secara tegap berdiri ke atas dengan ujung-ujung runcing']\n",
      "['pola permukiman dan ritualnya', 'Vinča']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def _simple_norm(s: str) -> str:\n",
    "    return (s or \"\").strip().lower()\n",
    "\n",
    "def is_conflict(example):\n",
    "    ans = example.get(\"answer\") or []\n",
    "    if len(ans) < 2:\n",
    "        return False\n",
    "\n",
    "    # normalisasi sederhana\n",
    "    norm = [_simple_norm(a) for a in ans if a is not None and _simple_norm(a) != \"\"]\n",
    "    if len(norm) < 2:\n",
    "        return False\n",
    "\n",
    "    # jika semua sama persis → bukan konflik\n",
    "    if len(set(norm)) == 1:\n",
    "        return False\n",
    "\n",
    "    # ambil yang terpendek\n",
    "    shortest = min(norm, key=len)\n",
    "\n",
    "    # cek apakah shortest subset dari elemen lain YANG BERBEDA\n",
    "    subset_of_other = any(\n",
    "        (shortest != other) and (shortest in other)\n",
    "        for other in norm\n",
    "    )\n",
    "\n",
    "    # konflik jika BUKAN subset dari elemen lain (dan kita sudah pastikan tidak semuanya identik)\n",
    "    return not subset_of_other\n",
    "\n",
    "# buat subset konflik baru\n",
    "tydiqa_multi_answer_conflict = tydiqa_multi_answer.filter(is_conflict)\n",
    "\n",
    "print(f\"Total multi-answer: {len(tydiqa_multi_answer)}\")\n",
    "print(f\"Total konflik     : {len(tydiqa_multi_answer_conflict)}\")\n",
    "\n",
    "# intip beberapa contoh\n",
    "for i in range(min(5, len(tydiqa_multi_answer_conflict))):\n",
    "    print(tydiqa_multi_answer_conflict[i][\"answer\"])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# konversi ke pandas DataFrame\n",
    "conflict_df = tydiqa_multi_answer_conflict.to_pandas()\n",
    "\n",
    "# simpan ke Excel\n",
    "conflict_df.to_excel(\"tydiqa_multi_answer_conflict.xlsx\", index=False)\n",
    "\n",
    "print(\"✅ File berhasil disimpan ke tydiqa_multi_answer_conflict.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe5fbd",
   "metadata": {},
   "source": [
    "Atur answer sebagai satu elemen saja, yakni string terpendek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b9a3044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/5912 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5912/5912 [00:00<00:00, 11470.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selesai. Baris dengan answer akhir 1 elemen: 5912 dari 5912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Asumsi: fungsi normalize(q: str, lowercase: bool=True) sudah ada di sistem\n",
    "\n",
    "def _shortest_normalized_answer(ans_list, lowercase=True):\n",
    "    \"\"\"\n",
    "    Ambil elemen terpendek SETELAH dinormalisasi.\n",
    "    Tie-breaker: kalau ada beberapa dengan panjang sama, ambil yang pertama.\n",
    "    Kembalikan string normalized (bukan versi aslinya).\n",
    "    \"\"\"\n",
    "    if ans_list is None:\n",
    "        return None\n",
    "    if not isinstance(ans_list, (list, tuple)):\n",
    "        ans_list = [ans_list]\n",
    "\n",
    "    # Normalisasi tiap elemen, buang None/kosong\n",
    "    normed = [normalize(str(a), lowercase=lowercase) for a in ans_list if a is not None and str(a).strip() != \"\"]\n",
    "    if not normed:\n",
    "        return None\n",
    "    # pilih yang terpendek\n",
    "    shortest = min(normed, key=len)\n",
    "    return shortest\n",
    "\n",
    "def keep_shortest_answer(batch, lowercase_norm=True):\n",
    "    out_answer = []\n",
    "    out_answers_dict = None\n",
    "    has_answers_dict = \"answers\" in batch\n",
    "\n",
    "    if has_answers_dict:\n",
    "        out_answers_dict = []\n",
    "\n",
    "    for i in range(len(batch[\"answer\"])):\n",
    "        shortest = _shortest_normalized_answer(batch[\"answer\"][i], lowercase=lowercase_norm)\n",
    "        # simpan sebagai list (konsisten dengan skema)\n",
    "        new_list = [shortest] if shortest is not None else []\n",
    "\n",
    "        out_answer.append(new_list)\n",
    "\n",
    "        if has_answers_dict:\n",
    "            curr = dict(batch[\"answers\"][i]) if batch[\"answers\"][i] is not None else {}\n",
    "            curr[\"text\"] = new_list\n",
    "            out_answers_dict.append(curr)\n",
    "\n",
    "    out = {\"answer\": out_answer}\n",
    "    if has_answers_dict:\n",
    "        out[\"answers\"] = out_answers_dict\n",
    "    return out\n",
    "\n",
    "# Terapkan ke dataset\n",
    "tydiqa_with_mr = tydiqa_with_mr.map(\n",
    "    keep_shortest_answer,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"lowercase_norm\": True},   # ganti ke False jika ingin pertahankan kapitalisasi\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "# Opsional: hitung berapa yang berubah (baris yang awalnya >1 jawaban)\n",
    "# (sekadar info)\n",
    "changed = sum(1 for a in tydiqa_with_mr[\"answer\"] if isinstance(a, list) and len(a) == 1)\n",
    "print(f\"Selesai. Baris dengan answer akhir 1 elemen: {changed} dari {len(tydiqa_with_mr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd4fc03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5912/5912 [00:00<00:00, 22370.27 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baris yang diupdate: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rows_to_update_2 = [\n",
    "    {\n",
    "        \"q_to_find\": \"apa arti dari heraldik?\", \n",
    "        \"new_answer\" : ['suatu ilmu atau seni dalam menciptakan dan menghias lambang, beserta kajian tentang makna, asal-usul, sejarah, dan perkembangannya'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa arti kata mandala?\", \n",
    "        \"new_answer\" : ['konsep Hindu, tetapi juga dipakai dalam konteks agama Buddha, untuk merujuk pada berbagai benda nyata','lingkaran'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa bahan bakar mesin jet?\", \n",
    "        \"new_answer\" : ['avtur'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa ciri khas bangsa saiya?\", \n",
    "        \"new_answer\" : ['mata hitam, rambut hitam yang tak pernah memanjang dan memiliki ekor'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa ciri khas dari kebudayaan neolitik?\", \n",
    "        \"new_answer\" : ['pola permukiman dan ritualnya'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa pendidikan terakhir mochamad fadjroel rachman?\", \n",
    "        \"new_answer\" : ['Magister Hukum'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa isi surat deuteropauline?\", \n",
    "        \"new_answer\" : ['gagasan eskatologi','wujud kepastian iman yang menjadi suatu pengharapan dalam kehidupan dan bagaimana sikap orang terhadap parousia serta sikap iman terhadap parousia yang mempunyai arti bagi kehidupan saat ini'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa maksud istilah shengnü?\", \n",
    "        \"new_answer\" : ['ejekan yang dipopulerkan oleh Federasi Wanita Seluruh Tiongkok yang mengklasifikasikan wanita yang masih belum menikah pada usia akhir dua puluh tahunan atau lebih','wanita sisa'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa nama bandara di penang?\", \n",
    "        \"new_answer\" : ['Bandar Udara Internasional Penang', 'Bandar Udara Internasional Bayan Lepas'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa nama rumah adat suku banjar?\", \n",
    "        \"new_answer\" : ['Rumah Baanjung', 'Rumah Banjar'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa peran dewa wisnu dalam kepercayaan hindu?\", \n",
    "        \"new_answer\" : ['pemelihara'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa penyebab utama pengeboman pasar tentena 2005?\", \n",
    "        \"new_answer\" : ['konflik sektarian antara Muslim dan Kristen di Poso', 'upaya balas dendam atas kekejaman sebelumnya yang dilakukan terhadap komunitas Muslim di Poso'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa seni islam pertama di baitul qur'an?\", \n",
    "        \"new_answer\" : ['salinan pertama Alquran'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa spesies ubur-ubur yang terbesar?\", \n",
    "        \"new_answer\" : ['Cyanea capillata', 'Surai singa'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa perusahaan pertama tempat david mackenzie ogilvy bekerja?\", \n",
    "        \"new_answer\" : ['Majestic Hotel'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"bagaimana sistem pemerintahan pakistan?\", \n",
    "        \"new_answer\" : ['pemerintah federal', 'republik demokrasi parlementer'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"berapa luas smk negeri 1 cikampek?\", \n",
    "        \"new_answer\" : ['29095m²', '28997m²'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"di tim apakah marc márquez alenta mendapatkan gelar juara dunia kelas 125cc pertama kali?\", \n",
    "        \"new_answer\" : ['KTM'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan kapal pertama diciptakan?\", \n",
    "        \"new_answer\" : ['10.000 tahun yang lalu','masa Neolitikum'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan katrina kaif mulai menjadi model?\", \n",
    "        \"new_answer\" : ['usia empat belas'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan komputer mikro mulai dikembangkan?\", \n",
    "        \"new_answer\" : ['1971'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan need for speed: payback dirilis?\", \n",
    "        \"new_answer\" : ['10 November 2017'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan raja ataulf menikah?\", \n",
    "        \"new_answer\" : ['Januari 414'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan rudi garcia mulai menjadi manajer sepak bola?\", \n",
    "        \"new_answer\" : ['Antara tahun 1994 dan 1996'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan teori hubungan internasional diciptakan?\", \n",
    "        \"new_answer\" : ['setelah Perang Dunia I', '1939'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana pelemparan di praha terjadi?\", \n",
    "        \"new_answer\" : ['Bohemia'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak aquitania?\", \n",
    "        \"new_answer\" : ['Perancis bagian barat daya', 'Pegunungan Pyrenees'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak balai kota dki jakarta?\", \n",
    "        \"new_answer\" : ['Jl. Medan Merdeka Selatan No. 8-9, Jakarta Pusat'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak cekungan tarim?\", \n",
    "        \"new_answer\" : ['Barat Laut China','Xinjiang China'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak daerah malta di eropa?\", \n",
    "        \"new_answer\" : ['Eropa Selatan'],\n",
    "    }, {\n",
    "        \"q_to_find\": \"dimana letak montenegro?\", \n",
    "        \"new_answer\" : ['Eropa Selatan'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak orléans?\", \n",
    "        \"new_answer\" : ['Amerika Serikat' ,'muara Sungai Mississippi'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak samudera pasifik?\", \n",
    "        \"new_answer\" : ['di antara Asia dan Australia di sebelah barat, Amerika di sebelah timur, Antartika di sebelah selatan dan Samudra Arktik di sebelah utara'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak timur dekat kuno?\", \n",
    "        \"new_answer\" : ['Timur Tengah'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak universitas leiden?\", \n",
    "        \"new_answer\" : ['Leiden','Belanda'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana studio pembuatan tom and jerry?\", \n",
    "        \"new_answer\" : ['Hollywood'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana su dingfang lahir?\", \n",
    "        \"new_answer\" : ['Wuyi'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"siapa dewa wisnu menurut agama hindu?\", \n",
    "        \"new_answer\" : ['Dewa yang bergelar sebagai shtiti (pemelihara) yang bertugas memelihara dan melindungi segala ciptaan Brahman'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"siapa ketua perum lkbn pertama?\", \n",
    "        \"new_answer\" : ['Mr. Soemanang'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"siapa nama karakter antagonis utama di dalam avatar: the last airbender?\", \n",
    "        \"new_answer\" : ['Amon'],\n",
    "        \"new_passage_text\": 'Steve Blum sebagai Amon adalah pria bertopeng misterius yang memimpin kelompok Equalists, sebuah kelompok yang anti-Pengendali yang juga menggunakan Ketrampilan Chi-Blokir yang dipakai untuk menaklukkan musuh. Dalam \"Kitab Wahyu\" yang pernah ditulisnya, wajahnya memiliki luka yang disebabkan oleh Mafia Pengendali Api yang suka memeras dan pernah membunuh keluarganya yang kemudian menjadi sebab ia mendirikan kelompok Anti-Pengendali, tetapi sebenarnya cerita itu hanya bohong agar para equalistbisa mngikuti jalannya, Ia memiliki kemampuan untuk menghilangkan kemampuan Pengendalian seseorang. Kemampuan seperti ini telah dimiliki oleh Aang di Avatar serial sebelumnya di Episode terakhir. Dia adalah tokoh Antagonis utama dalam serial ini.'\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"siapa presiden russia pada tahun 2012?\", \n",
    "        \"new_answer\" : ['Vladimir Putin'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"siapa presiden wanita pertama di filipina?\", \n",
    "        \"new_answer\" : ['Maria Corazon Sumulong Cojuangco Aquino', 'Cory Aquino'],\n",
    "    },\n",
    "    {\n",
    "        \"q_to_find\": \"siapa puteri indonesia yang ke 18?\", \n",
    "        \"new_answer\" : ['Sonia Fergina Citra'],\n",
    "    },\n",
    "    {\n",
    "        \"q_to_find\": \"siapa yang mendirikan organisasi palang merah?\", \n",
    "        \"new_answer\" : ['Henry Dunant'],\n",
    "    },\n",
    "    {\n",
    "        \"q_to_find\": \"tahun berapa pertempuran shiffin terjadi?\", \n",
    "        \"new_answer\" : ['657 Masehi', '37 Hijriah'],\n",
    "    },\n",
    "]\n",
    "tydiqa_with_mr = apply_rows_updates(\n",
    "    tydiqa_with_mr,\n",
    "    rows_to_update=rows_to_update_2,\n",
    "    lowercase_norm=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7a562",
   "metadata": {},
   "source": [
    "Hapus kolom 'mr_query_norm_match' dan 'mr_pos_match'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f717e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tydiqa_with_mr = tydiqa_with_mr.remove_columns(['mr_query_norm_match', 'mr_pos_match'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d447a",
   "metadata": {},
   "source": [
    "normalize dokumen (tanpa lowercase), answer (tanpa lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd5eb8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5912/5912 [00:57<00:00, 101.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalize_fields(batch):\n",
    "    new_answers = []\n",
    "    new_passages = []\n",
    "    new_titles = []\n",
    "    new_neg_passages = []\n",
    "\n",
    "    for i in range(len(batch[\"answer\"])):\n",
    "        # ===== answers =====\n",
    "        ans_list = batch[\"answer\"][i] or []\n",
    "        normed_ans = [normalize(a, lowercase=False) for a in ans_list if a is not None]\n",
    "        new_answers.append(normed_ans)\n",
    "\n",
    "        # ===== passage_text =====\n",
    "        passage = batch.get(\"passage_text\", [None])[i]\n",
    "        new_passages.append(normalize(passage, lowercase=False) if passage else passage)\n",
    "\n",
    "        # ===== document_title =====\n",
    "        doc_title = batch.get(\"document_title\", [None])[i]\n",
    "        new_titles.append(normalize(doc_title, lowercase=False) if doc_title else doc_title)\n",
    "\n",
    "        # ===== mr_negative_passages (list of dict) =====\n",
    "        negs = batch.get(\"mr_negative_passages\", [None])[i]\n",
    "        if negs is None:\n",
    "            new_neg_passages.append(None)\n",
    "        else:\n",
    "            normed_negs = []\n",
    "            for d in negs:\n",
    "                new_d = dict(d)\n",
    "                if \"title\" in new_d and new_d[\"title\"] is not None:\n",
    "                    new_d[\"title\"] = normalize(new_d[\"title\"], lowercase=False)\n",
    "                if \"text\" in new_d and new_d[\"text\"] is not None:\n",
    "                    new_d[\"text\"] = normalize(new_d[\"text\"], lowercase=False)\n",
    "                normed_negs.append(new_d)\n",
    "            new_neg_passages.append(normed_negs)\n",
    "\n",
    "    out = {\n",
    "        \"answer\": new_answers,\n",
    "        \"passage_text\": new_passages,\n",
    "        \"document_title\": new_titles,\n",
    "        \"mr_negative_passages\": new_neg_passages,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "# terapkan ke dataset\n",
    "tydiqa_with_mr = tydiqa_with_mr.map(\n",
    "    normalize_fields,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00c732c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tydiqa_with_mr=tydiqa_with_mr.remove_columns(['mr_title', 'mr_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65eaf86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5912/5912 [00:00<00:00, 67757.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tydiqa_with_mr.save_to_disk(\"tydiqa_with_mr_wo_dedupe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ec075",
   "metadata": {},
   "source": [
    "# Indoqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "indoqa = load_dataset(\"SEACrowd/indoqa\", trust_remote_code=True)\n",
    "\n",
    "indoqa = concatenate_datasets([indoqa[split] for split in indoqa.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20fefd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/4413 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 4413/4413 [00:00<00:00, 37497.09 examples/s]\n",
      "Map: 100%|██████████| 4253/4253 [00:00<00:00, 8047.86 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'answer', 'context', 'category', 'span_start', 'span_end', 'question_norm', 'question_stem'],\n",
       "    num_rows: 4253\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "indoqa = indoqa.filter(lambda x: x['category']=='SPAN')\n",
    "\n",
    "indoqa = indoqa.map(\n",
    "    add_norm_and_stem, \n",
    "    batched=True, \n",
    "    fn_kwargs={\"column\": \"question\"}   # sesuaikan dengan nama kolom di tydiqa_gold_all\n",
    ")\n",
    "indoqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff5a0c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebelum: 4253 baris\n",
      "Sesudah: 4247 baris (duplikat dihapus)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# konversi ke pandas\n",
    "df = indoqa.to_pandas()\n",
    "\n",
    "# tambahkan panjang context\n",
    "df[\"_context_len\"] = df[\"context\"].str.len()\n",
    "\n",
    "# urutkan biar idxmin konsisten (misal by context_len)\n",
    "df_sorted = df.sort_values(by=[\"question_stem\", \"_context_len\"])\n",
    "\n",
    "# ambil index dengan context terpendek untuk setiap question_stem\n",
    "idx_keep = df_sorted.groupby(\"question_stem\")[\"_context_len\"].idxmin()\n",
    "\n",
    "# subset baru\n",
    "df_dedup = df.loc[idx_keep].reset_index(drop=True)\n",
    "\n",
    "print(f\"Sebelum: {len(df)} baris\")\n",
    "print(f\"Sesudah: {len(df_dedup)} baris (duplikat dihapus)\")\n",
    "\n",
    "# drop kolom bantu\n",
    "df_dedup = df_dedup.drop(columns=[\"_context_len\"])\n",
    "\n",
    "# konversi balik ke HuggingFace Dataset\n",
    "indoqa_dedup = Dataset.from_pandas(df_dedup, preserve_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a189daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4247/4247 [00:01<00:00, 2331.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def normalize_fields(batch):\n",
    "    new_answers = []\n",
    "    new_passages = []\n",
    "\n",
    "    for i in range(len(batch[\"answer\"])):\n",
    "\n",
    "        answer = batch.get(\"answer\", [None])[i]\n",
    "        new_answers.append(normalize(answer, lowercase=False) if answer else answer)\n",
    "\n",
    "        # ===== passage_text =====\n",
    "        passage = batch.get(\"context\", [None])[i]\n",
    "        new_passages.append(normalize(passage, lowercase=False) if passage else passage)\n",
    "\n",
    "    out = {\n",
    "        \"answer\": new_answers,\n",
    "        \"context\": new_passages,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "indoqa_clean = indoqa_dedup.map(\n",
    "    normalize_fields,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43648be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) Tokenizer Flan-T5\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# 2) Ambil panjang token kolom `context` dengan batching (lebih cepat)\n",
    "def token_lengths_for_column(ds, col=\"context\", batch_size=512):\n",
    "    lengths = []\n",
    "    n = len(ds)\n",
    "    for start in tqdm(range(0, n, batch_size), desc=f\"Tokenizing `{col}`\"):\n",
    "        end = min(start + batch_size, n)\n",
    "        batch_texts = [(ds[i].get(col) or \"\") for i in range(start, end)]\n",
    "        enc = t5_tokenizer(batch_texts, add_special_tokens=True, truncation=False)\n",
    "        # panjang = jumlah token per contoh\n",
    "        lens = [len(ids) for ids in enc[\"input_ids\"]]\n",
    "        lengths.extend(lens)\n",
    "    return np.array(lengths, dtype=np.int32)\n",
    "\n",
    "lengths = token_lengths_for_column(indoqa_clean, col=\"context\", batch_size=512)\n",
    "\n",
    "# 3) Statistik ringkas\n",
    "def describe_lengths(arr):\n",
    "    if arr.size == 0:\n",
    "        return {}\n",
    "    stats = {\n",
    "        \"count\": int(arr.size),\n",
    "        \"mean\": float(np.mean(arr)),\n",
    "        \"std\": float(np.std(arr)),\n",
    "        \"min\": int(np.min(arr)),\n",
    "        \"p50_median\": float(np.median(arr)),\n",
    "        \"p90\": float(np.percentile(arr, 90)),\n",
    "        \"p95\": float(np.percentile(arr, 95)),\n",
    "        \"p99\": float(np.percentile(arr, 99)),\n",
    "        \"max\": int(np.max(arr)),\n",
    "    }\n",
    "    # proporsi melewati beberapa ambang umum (opsional)\n",
    "    for thr in (256, 512, 600, 1024):\n",
    "        stats[f\"> {thr}\"] = float((arr > thr).mean() * 100.0)\n",
    "    return stats\n",
    "\n",
    "stats = describe_lengths(lengths)\n",
    "\n",
    "# 4) Cetak statistik\n",
    "print(\"=== Statistik panjang token `context` (Flan-T5) ===\")\n",
    "for k in [\"count\",\"mean\",\"std\",\"min\",\"p50_median\",\"p90\",\"p95\",\"p99\",\"max\",\"> 256\",\"> 512\",\"> 600\",\"> 1024\"]:\n",
    "    if k in stats:\n",
    "        if k.startswith(\"> \"):\n",
    "            print(f\"{k:>8}: {stats[k]:6.2f}%\")\n",
    "        elif k in {\"mean\",\"std\",\"p50_median\",\"p90\",\"p95\",\"p99\"}:\n",
    "            print(f\"{k:>12}: {stats[k]:.2f}\")\n",
    "        else:\n",
    "            print(f\"{k:>12}: {stats[k]}\")\n",
    "\n",
    "# 5) Visualisasi histogram\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(lengths, bins=60, edgecolor=\"black\")\n",
    "plt.title(\"Distribusi Panjang Token `context` (Flan-T5)\")\n",
    "plt.xlabel(\"Jumlah token\"); plt.ylabel(\"Frekuensi\")\n",
    "\n",
    "# Garis bantu mean/median/p95\n",
    "mean_v = np.mean(lengths) if lengths.size else 0\n",
    "med_v  = np.median(lengths) if lengths.size else 0\n",
    "p95_v  = np.percentile(lengths, 95) if lengths.size else 0\n",
    "plt.axvline(mean_v, linestyle=\"--\", label=f\"Mean={mean_v:.1f}\")\n",
    "plt.axvline(med_v,  linestyle=\":\",  label=f\"Median={med_v:.1f}\")\n",
    "plt.axvline(p95_v,  linestyle=\"-.\", label=f\"P95={p95_v:.1f}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ce27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indoqa_clean = indoqa_clean.remove_columns(['category', 'span_start', 'span_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "80a827bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4247/4247 [00:00<00:00, 240946.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "indoqa_clean.save_to_disk(\"indoqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee3d0d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'answer', 'passage_text', 'question_norm', 'question_stem'],\n",
       "    num_rows: 4247\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indoqa = indoqa.remove_columns(['category', 'span_start', 'span_end'])\n",
    "indoqa=indoqa.rename_column('context', 'passage_text')\n",
    "indoqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb3cf8",
   "metadata": {},
   "source": [
    "# FacQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63985e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'question', 'passage', 'seq_label'],\n",
       "    num_rows: 3117\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "facqa = load_dataset(\"SEACrowd/facqa\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "facqa = concatenate_datasets([facqa[split] for split in facqa.keys()])\n",
    "facqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c140030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3117/3117 [00:01<00:00, 3096.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['index', 'question', 'passage', 'seq_label', 'question_text', 'passage_text', 'answer_text'],\n",
      "    num_rows: 3117\n",
      "})\n",
      "Kelompok apakah yang menyatakan bertanggung jawab atas ledakan di Srinagar ?\n",
      "Lewat telepon ke kantor berita lokal Current News Service , Hezb-ul Mujahedeen , kelompok militan Kashmir yang terbesar , menyatakan bertanggung jawab atas ledakan di Srinagar .\n",
      "['Hezb-ul Mujahedeen']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# fungsi ekstraksi jawaban dari BIO\n",
    "def extract_answer(tokens, labels):\n",
    "    answers = []\n",
    "    current = []\n",
    "    for tok, lab in zip(tokens, labels):\n",
    "        if lab == \"B\":\n",
    "            if current:\n",
    "                answers.append(\" \".join(current))\n",
    "            current = [tok]\n",
    "        elif lab == \"I\":\n",
    "            current.append(tok)\n",
    "        else:  # O\n",
    "            if current:\n",
    "                answers.append(\" \".join(current))\n",
    "                current = []\n",
    "    if current:\n",
    "        answers.append(\" \".join(current))\n",
    "    return answers\n",
    "\n",
    "# fungsi untuk map ke dataset\n",
    "def preprocess(example):\n",
    "    # gabungkan token jadi string utuh\n",
    "    example[\"question_text\"] = \" \".join(example[\"question\"])\n",
    "    example[\"passage_text\"]  = \" \".join(example[\"passage\"])\n",
    "    # ekstrak jawaban\n",
    "    example[\"answer_text\"]   = extract_answer(example[\"passage\"], example[\"seq_label\"])\n",
    "    return example\n",
    "\n",
    "# terapkan ke semua split\n",
    "facqa = facqa.map(preprocess)\n",
    "\n",
    "# cek hasil\n",
    "print(facqa)\n",
    "print(facqa[0][\"question_text\"])\n",
    "print(facqa[0][\"passage_text\"])\n",
    "print(facqa[0][\"answer_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "254366eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3117/3117 [00:00<00:00, 4599.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total B: 3169\n",
      "Total rows: 3117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 3117/3117 [00:00<00:00, 5156.09 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with multi-span answers: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) hitung jumlah 'B' total & distribusi 'B' per baris\n",
    "def count_B_I_O(ex):\n",
    "    c = Counter(ex[\"seq_label\"])\n",
    "    ex[\"num_B\"] = c.get(\"B\", 0)\n",
    "    ex[\"num_I\"] = c.get(\"I\", 0)\n",
    "    ex[\"num_O\"] = c.get(\"O\", 0)\n",
    "    return ex\n",
    "\n",
    "facqa = facqa.map(count_B_I_O)\n",
    "\n",
    "total_B = int(sum(facqa[\"num_B\"]))\n",
    "print(\"Total B:\", total_B)\n",
    "print(\"Total rows:\", len(facqa))\n",
    "\n",
    "# 2) filter baris yang punya >1 span (num_B > 1)\n",
    "multi_span = facqa.filter(lambda ex: ex[\"num_B\"] > 1)\n",
    "print(\"Rows with multi-span answers:\", len(multi_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "712da5b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'add_norm_and_stem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m facqa \u001b[38;5;241m=\u001b[39m facqa\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m----> 2\u001b[0m     add_norm_and_stem, \n\u001b[0;32m      3\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m      4\u001b[0m     fn_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_text\u001b[39m\u001b[38;5;124m\"\u001b[39m}   \u001b[38;5;66;03m# sesuaikan dengan nama kolom di tydiqa_gold_all\u001b[39;00m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m facqa\n",
      "\u001b[1;31mNameError\u001b[0m: name 'add_norm_and_stem' is not defined"
     ]
    }
   ],
   "source": [
    "facqa = facqa.map(\n",
    "    add_norm_and_stem, \n",
    "    batched=True, \n",
    "    fn_kwargs={\"column\": \"question_text\"}   # sesuaikan dengan nama kolom di tydiqa_gold_all\n",
    ")\n",
    "facqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8080b415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah question_stem yang terduplikasi: 73\n",
      "Jumlah baris yang termasuk duplikat  : 189\n",
      "✅ File 'facqa_question_stem_duplicates.xlsx' berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# konversi dataset HF ke pandas\n",
    "df = facqa.to_pandas()\n",
    "\n",
    "# cari duplikasi berdasarkan kolom question_stem\n",
    "dupes_mask = df.duplicated(subset=[\"question_text_stem\"], keep=False)\n",
    "\n",
    "# subset baris duplikat\n",
    "dupes_df = df[dupes_mask].sort_values(\"question_text_stem\")\n",
    "\n",
    "# hitung jumlah pertanyaan yang duplikat\n",
    "n_dupes = dupes_df[\"question_text_stem\"].nunique()\n",
    "print(f\"Jumlah question_stem yang terduplikasi: {n_dupes}\")\n",
    "print(f\"Jumlah baris yang termasuk duplikat  : {len(dupes_df)}\")\n",
    "\n",
    "# simpan ke Excel\n",
    "dupes_df.to_excel(\"facqa_question_stem_duplicates.xlsx\", index=False)\n",
    "print(\"✅ File 'facqa_question_stem_duplicates.xlsx' berhasil dibuat.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5330b5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3117/3117 [00:00<00:00, 102360.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "facqa.save_to_disk(\"facqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68cd6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "facqa = load_from_disk(\"facqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0437db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before: 3117 | after: 3108 | removed: 9\n"
     ]
    }
   ],
   "source": [
    "# ====== 2) Daftar pasangan dari Excel (sudah dituliskan ulang ke kode) ======\n",
    "EXCEL_PAIRS = [\n",
    "    (\"apa nama badan bagi dari universitas gadjah mada ugm yang libat dalam ada sistem energi alternatif di yogyakarta\", 'PSE UGM'),\n",
    "    (\"apa nama partai hosni mubarak\", 'NDP'),\n",
    "    ('di badan apakah bastian purnama jabat bagai direktur utama', 'BES'),\n",
    "    ('kapan keluar undang nomor 31 tentang ikan', 'tahun 2004'),\n",
    "    ('kapan mou indonesia-gam ditandatangani', '15/8'),\n",
    "    ('kepala badan apakah mohamed elbaradei', 'IAEA'),\n",
    "    ('kepala balai apakah yohanes sudarto', 'BKSDA Kalteng'),\n",
    "    ('menandatangani apakah perintah indonesia dan gam di helsinki finlandia tanggal 15 agustus 2005', 'nota kesepahaman atau MOU tentang perdamaian di Aceh'),\n",
    "    ('siapa nama putra presiden hosni mubarak','Jamal')\n",
    "]\n",
    "\n",
    "# Ubah ke set untuk lookup cepat; trimming spasi agar toleran spasi ekstra\n",
    "PAIRS_SET = set((qs.strip(), ans.strip()) for qs, ans in EXCEL_PAIRS)\n",
    "\n",
    "# ====== Filter facqa: drop baris yang match (question_text_stem, salah-satu answer_text) ======\n",
    "def _should_keep_facqa(example):\n",
    "    qstem = (example.get(\"question_text_stem\") or \"\").strip()\n",
    "    answers = example.get(\"answer_text\") or []  # list[str]\n",
    "    for t in answers:\n",
    "        if (qstem, (t or \"\").strip()) in PAIRS_SET:\n",
    "            return False  # baris ini DIHAPUS\n",
    "    return True  # keep\n",
    "\n",
    "before_n = len(facqa)\n",
    "facqa_filtered = facqa.filter(_should_keep_facqa)\n",
    "after_n = len(facqa_filtered)\n",
    "removed = before_n - after_n\n",
    "print(f\"Rows before: {before_n} | after: {after_n} | removed: {removed}\")\n",
    "\n",
    "# ====== (Opsional) laporan pasangan Excel yang tidak ketemu di dataset ======\n",
    "def _collect_hits(example):\n",
    "    qstem = (example.get(\"question_text_stem\") or \"\").strip()\n",
    "    answers = example.get(\"answer_text\") or []\n",
    "    hits = []\n",
    "    for t in answers:\n",
    "        key = (qstem, (t or \"\").strip())\n",
    "        if key in PAIRS_SET:\n",
    "            hits.append(key)\n",
    "    return {\"_hits\": hits}\n",
    "\n",
    "hits_ds = facqa.map(_collect_hits, batched=False)\n",
    "found_pairs = set()\n",
    "for row in hits_ds:\n",
    "    for h in row.get(\"_hits\", []):\n",
    "        # 'h' sudah tuple-like (atau list dua elemen)\n",
    "        found_pairs.add(tuple(h))\n",
    "\n",
    "not_found = PAIRS_SET - found_pairs\n",
    "if not_found:\n",
    "    print(\"\\nPairs from Excel NOT FOUND in facqa:\")\n",
    "    for qs, ans in sorted(not_found):\n",
    "        print(f\"- question_text_stem={qs!r} | answer_text element={ans!r}\")\n",
    "\n",
    "# Commit hasil filter\n",
    "facqa = facqa_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ff86b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 3108 | After: 3001 | Removed: 107\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# convert ke pandas\n",
    "df = facqa.to_pandas()\n",
    "\n",
    "# panjang passage; NaN -> +inf agar tidak terpilih sebagai terpendek\n",
    "df[\"_passage_len\"] = df[\"passage_text\"].str.len()\n",
    "df[\"_passage_len\"] = df[\"_passage_len\"].fillna(np.inf)\n",
    "\n",
    "# (opsional) buat hasil deterministik pada tie: sort by _passage_len lalu ambil first\n",
    "df_sorted = df.sort_values(by=[\"question_text_stem\", \"_passage_len\"], kind=\"mergesort\")\n",
    "\n",
    "# ambil indeks dengan passage terpendek per question_text_stem\n",
    "idx_min = df_sorted.groupby(\"question_text_stem\")[\"_passage_len\"].idxmin()\n",
    "\n",
    "# subset baris yang disimpan\n",
    "df_kept = df.loc[idx_min].reset_index(drop=True)\n",
    "\n",
    "print(f\"Before: {len(df)} | After: {len(df_kept)} | Removed: {len(df) - len(df_kept)}\")\n",
    "\n",
    "# buang kolom bantu\n",
    "df_kept = df_kept.drop(columns=[\"_passage_len\"])\n",
    "\n",
    "# convert balik ke HF Dataset (hindari kolom __index_level_0__)\n",
    "facqa_wo_dedupe = Dataset.from_pandas(df_kept, preserve_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0caf723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3001/3001 [00:00<00:00, 3565.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def normalize_fields(batch, answer_col, psg_text):\n",
    "    new_answers = []\n",
    "    new_passages = []\n",
    "\n",
    "    answers_batch  = batch.get(answer_col, [])\n",
    "    passages_batch = batch.get(psg_text, [])\n",
    "\n",
    "    for i in range(len(answers_batch)):\n",
    "        # ===== answers: pastikan list, lalu normalize per elemen =====\n",
    "        ans = answers_batch[i]\n",
    "        if ans is None:\n",
    "            norm_ans = []\n",
    "        elif isinstance(ans, (list, tuple)):\n",
    "            norm_ans = [normalize(a, lowercase=False) for a in ans if a is not None]\n",
    "        else:\n",
    "            # kalau awalnya string tunggal → jadikan list satu elemen\n",
    "            norm_ans = [normalize(ans, lowercase=False)]\n",
    "\n",
    "        new_answers.append(norm_ans)\n",
    "\n",
    "        # ===== passage_text =====\n",
    "        passage = passages_batch[i] if i < len(passages_batch) else None\n",
    "        new_passages.append(normalize(passage, lowercase=False) if passage else passage)\n",
    "\n",
    "    # tulis balik ke kolom yang sama\n",
    "    return {\n",
    "        answer_col: new_answers,\n",
    "        psg_text: new_passages,\n",
    "    }\n",
    "\n",
    "facqa_wo_dedupe = facqa_wo_dedupe.map(\n",
    "    normalize_fields,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    load_from_cache_file=False,\n",
    "    fn_kwargs={\"answer_col\": \"answer_text\", \"psg_text\": \"passage_text\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d04c91b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'question', 'passage', 'seq_label', 'question_text', 'passage_text', 'answer_text', 'num_B', 'num_I', 'num_O', 'question_text_norm', 'question_text_stem'],\n",
       "    num_rows: 3001\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facqa_wo_dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2be43438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing `passage_text`: 100%|██████████| 6/6 [00:01<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Statistik panjang token `context` (Flan-T5) ===\n",
      "       count: 3001\n",
      "        mean: 132.93\n",
      "         std: 44.28\n",
      "         min: 15\n",
      "  p50_median: 127.00\n",
      "         p90: 191.00\n",
      "         p95: 211.00\n",
      "         p99: 266.00\n",
      "         max: 366\n",
      "   > 256:   1.40%\n",
      "   > 512:   0.00%\n",
      "   > 600:   0.00%\n",
      "  > 1024:   0.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = token_lengths_for_column(facqa_wo_dedupe, col=\"passage_text\", batch_size=512)\n",
    "stats = describe_lengths(lengths)\n",
    "# 4) Cetak statistik\n",
    "print(\"=== Statistik panjang token `context` (Flan-T5) ===\")\n",
    "for k in [\"count\",\"mean\",\"std\",\"min\",\"p50_median\",\"p90\",\"p95\",\"p99\",\"max\",\"> 256\",\"> 512\",\"> 600\",\"> 1024\"]:\n",
    "    if k in stats:\n",
    "        if k.startswith(\"> \"):\n",
    "            print(f\"{k:>8}: {stats[k]:6.2f}%\")\n",
    "        elif k in {\"mean\",\"std\",\"p50_median\",\"p90\",\"p95\",\"p99\"}:\n",
    "            print(f\"{k:>12}: {stats[k]:.2f}\")\n",
    "        else:\n",
    "            print(f\"{k:>12}: {stats[k]}\")\n",
    "\n",
    "# 5) Visualisasi histogram\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(lengths, bins=60, edgecolor=\"black\")\n",
    "plt.title(\"Distribusi Panjang Token `context` (Flan-T5)\")\n",
    "plt.xlabel(\"Jumlah token\"); plt.ylabel(\"Frekuensi\")\n",
    "\n",
    "# Garis bantu mean/median/p95\n",
    "mean_v = np.mean(lengths) if lengths.size else 0\n",
    "med_v  = np.median(lengths) if lengths.size else 0\n",
    "p95_v  = np.percentile(lengths, 95) if lengths.size else 0\n",
    "plt.axvline(mean_v, linestyle=\"--\", label=f\"Mean={mean_v:.1f}\")\n",
    "plt.axvline(med_v,  linestyle=\":\",  label=f\"Median={med_v:.1f}\")\n",
    "plt.axvline(p95_v,  linestyle=\"-.\", label=f\"P95={p95_v:.1f}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1513a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'question_text', 'passage_text', 'answer_text', 'question_text_norm', 'question_text_stem'],\n",
       "    num_rows: 3001\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facqa_wo_dedupe=facqa_wo_dedupe.remove_columns(['question', 'passage', 'seq_label','num_B', 'num_I', 'num_O'])\n",
    "facqa_wo_dedupe=facqa_wo_dedupe.rename_column('answer_text', 'answer')\n",
    "\n",
    "# mapping nama lama ke nama baru\n",
    "rename_map = {\n",
    "    \"question_text\": \"question\",\n",
    "    \"question_text_norm\": \"question_norm\",\n",
    "    \"question_text_stem\": \"question_stem\",\n",
    "}\n",
    "\n",
    "# rename kolom\n",
    "facqa_wo_dedupe = facqa_wo_dedupe.rename_columns(rename_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f729f8",
   "metadata": {},
   "source": [
    "### sebelum gabung semua solit dari facqa, sebaiknya ubah dulu id agar mencerminkan train, validation, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13d560",
   "metadata": {},
   "source": [
    "# Semua dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7f466b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document_title', 'passage_text', 'question', 'question_norm', 'question_stem', 'answer', 'mr_query_id', 'mr_docid', 'mr_negative_passages'],\n",
       "    num_rows: 5912\n",
       "})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facqa_wo_dedupe=load_from_disk('facqa_finish')\n",
    "indoqa=load_from_disk('indoqa_finish')\n",
    "tydiqa_with_mr=load_from_disk(\"tydi_finish\")\n",
    "tydiqa_with_mr=tydiqa_with_mr.rename_column('query', 'question')\n",
    "facqa_wo_dedupe= facqa_wo_dedupe.rename_column('index', 'id')\n",
    "tydiqa_with_mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff163e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4247/4247 [00:00<00:00, 113794.79 examples/s]\n",
      "Map: 100%|██████████| 3001/3001 [00:00<00:00, 123553.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'question', 'answer', 'passage_text', 'question_norm', 'question_stem', 'mr_query_id', 'mr_docid', 'mr_negative_passages'],\n",
      "    num_rows: 4247\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'question', 'passage_text', 'answer', 'question_norm', 'question_stem', 'mr_query_id', 'mr_docid', 'mr_negative_passages'],\n",
      "    num_rows: 3001\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def add_empty_cols(batch):\n",
    "#     n = len(batch[\"id\"])\n",
    "#     return {\n",
    "#         \"mr_query_id\": [\"\"] * n,\n",
    "#         \"mr_docid\": [\"\"] * n,\n",
    "#         \"mr_negative_passages\": [[] for _ in range(n)],\n",
    "#     }\n",
    "\n",
    "# indoqa = indoqa.map(\n",
    "#     add_empty_cols,\n",
    "#     batched=True,\n",
    "#     keep_in_memory=True,          # <— kunci\n",
    "#     load_from_cache_file=False,   # jangan pakai cache lama\n",
    "# )\n",
    "\n",
    "# facqa_wo_dedupe = facqa_wo_dedupe.map(\n",
    "#     add_empty_cols,\n",
    "#     batched=True,\n",
    "#     keep_in_memory=True,\n",
    "#     load_from_cache_file=False,\n",
    "# )\n",
    "\n",
    "# print(indoqa)\n",
    "# print(facqa_wo_dedupe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a8c7d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3001/3001 [00:00<00:00, 11338.47 examples/s]\n",
      "Map: 100%|██████████| 5912/5912 [00:01<00:00, 4565.22 examples/s]\n",
      "Map: 100%|██████████| 4247/4247 [00:00<00:00, 9822.87 examples/s] \n",
      "Map: 100%|██████████| 3001/3001 [00:00<00:00, 8853.45 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Tambah kolom 'source' di masing-masing dataset\n",
    "indoqa_with_src = indoqa.map(lambda ex: {\"source\": \"indoqa\"})\n",
    "facqa_with_src  = facqa_wo_dedupe.map(lambda ex: {\"source\": \"facqa\"})\n",
    "tydi_with_src   = tydiqa_with_mr.map(lambda ex: {\"source\": \"tydiqa_with_mr\"})\n",
    "\n",
    "# 2) Samakan skema (kolom) agar bisa concat: buat union kolom, lalu isi kolom yang hilang dengan None/[] sesuai kebutuhan\n",
    "def align_schema(ds, all_cols):\n",
    "    missing = [c for c in all_cols if c not in ds.column_names]\n",
    "    if missing:\n",
    "        n = len(ds)\n",
    "        # default None untuk kolom biasa; kalau kamu tahu kolom tertentu adalah list, bisa disesuaikan\n",
    "        defaults = {c: [None]*n for c in missing}\n",
    "        ds = ds.map(lambda ex, idx: {k: defaults[k][idx] for k in missing}, with_indices=True)\n",
    "    return ds\n",
    "\n",
    "all_columns = set(indoqa_with_src.column_names) | set(facqa_with_src.column_names) | set(tydi_with_src.column_names)\n",
    "\n",
    "indoqa_aligned = align_schema(indoqa_with_src, all_columns)\n",
    "facqa_aligned  = align_schema(facqa_with_src,  all_columns)\n",
    "tydi_aligned   = align_schema(tydi_with_src,   all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e509ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4247/4247 [00:00<00:00, 10298.17 examples/s]\n",
      "Map: 100%|██████████| 4247/4247 [00:00<00:00, 70333.72 examples/s]\n",
      "Map: 100%|██████████| 4247/4247 [00:00<00:00, 22390.41 examples/s]\n",
      "Casting the dataset: 100%|██████████| 4247/4247 [00:00<00:00, 87183.32 examples/s]\n",
      "Map: 100%|██████████| 3001/3001 [00:00<00:00, 10730.81 examples/s]\n",
      "Map: 100%|██████████| 3001/3001 [00:00<00:00, 55266.24 examples/s]\n",
      "Map: 100%|██████████| 3001/3001 [00:00<00:00, 20435.47 examples/s]\n",
      "Casting the dataset: 100%|██████████| 3001/3001 [00:00<00:00, 57415.33 examples/s]\n",
      "Map: 100%|██████████| 5912/5912 [00:01<00:00, 4495.32 examples/s]\n",
      "Map: 100%|██████████| 5912/5912 [00:01<00:00, 3103.92 examples/s]\n",
      "Casting the dataset: 100%|██████████| 5912/5912 [00:00<00:00, 108132.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'question', 'answer', 'passage_text', 'document_title', 'question_norm', 'question_stem', 'mr_query_id', 'mr_docid', 'mr_negative_passages', 'source'],\n",
      "    num_rows: 13160\n",
      "})\n",
      "Total baris dengan question_stem   : 13160\n",
      "Total baris yg termasuk duplikat   : 28\n",
      "Jumlah question_stem yang duplikat : 14\n",
      "\n",
      "Contoh 5 question_stem duplikat (by source):\n",
      "- 'apa definisi dari budaya' -> {'indoqa': 1, 'tydiqa_with_mr': 1}\n",
      "- 'apa lambang negara indonesia' -> {'indoqa': 1, 'tydiqa_with_mr': 1}\n",
      "- 'apa nama lagu bangsa jepang' -> {'indoqa': 1, 'tydiqa_with_mr': 1}\n",
      "- 'apa nama mata uang thailand' -> {'facqa': 1, 'tydiqa_with_mr': 1}\n",
      "- 'apa yang maksud dengan budaya' -> {'indoqa': 1, 'tydiqa_with_mr': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, concatenate_datasets, Features, Value, Sequence\n",
    "import copy\n",
    "\n",
    "# ===== 0) Skema target yang konsisten untuk semua dataset =====\n",
    "TARGET_FEATURES = Features({\n",
    "    \"id\": Value(\"string\"),\n",
    "    \"question\": Value(\"string\"),\n",
    "    \"answer\": Sequence(feature=Value(\"string\")),\n",
    "    \"passage_text\": Value(\"string\"),\n",
    "    \"document_title\": Value(\"string\"),\n",
    "    \"question_norm\": Value(\"string\"),\n",
    "    \"question_stem\": Value(\"string\"),\n",
    "    \"mr_query_id\": Value(\"string\"),\n",
    "    \"mr_docid\": Value(\"string\"),\n",
    "    # Jika ingin menyimpan negatif sebagai list dict {docid,title,text}, uncomment ini dan pakai List[dict]\n",
    "    # \"mr_negative_passages\": Sequence(feature={\n",
    "    #     \"docid\": Value(\"string\"),\n",
    "    #     \"title\": Value(\"string\"),\n",
    "    #     \"text\": Value(\"string\"),\n",
    "    # }),\n",
    "    # Atau kalau mau sederhana: list of string (mis. diset kosong). Di bawah ini kita pakai list-of-string agar mudah selaras.\n",
    "    \"mr_negative_passages\": Sequence(feature=Value(\"string\")),\n",
    "    \"source\": Value(\"string\"),\n",
    "})\n",
    "\n",
    "def _ensure_list_of_str(x):\n",
    "    \"\"\"Pastikan kolom answer jadi list[str].\"\"\"\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return [\"\" if v is None else str(v) for v in x]\n",
    "    # kalau string tunggal → bungkus ke list\n",
    "    return [str(x)]\n",
    "\n",
    "def _to_str_or_empty(x):\n",
    "    return \"\" if x is None else str(x)\n",
    "\n",
    "def _prepare_and_cast(ds, source_name):\n",
    "    \"\"\"\n",
    "    - Tambah kolom 'source'\n",
    "    - Samakan nama kolom: context->passage_text, answer_text->answer\n",
    "    - Isi kolom hilang\n",
    "    - Coerce tipe ke TARGET_FEATURES\n",
    "    \"\"\"\n",
    "    # 1) Tambah kolom source\n",
    "    ds = ds.map(lambda ex: {\"source\": source_name})\n",
    "\n",
    "    # 2) Rename kolom agar konsisten\n",
    "    rename_map = {}\n",
    "    if \"context\" in ds.column_names and \"passage_text\" not in ds.column_names:\n",
    "        rename_map[\"context\"] = \"passage_text\"\n",
    "    if \"question_text\" in ds.column_names and \"question\" not in ds.column_names:\n",
    "        rename_map[\"question_text\"] = \"question\"\n",
    "    if \"question_text_norm\" in ds.column_names and \"question_norm\" not in ds.column_names:\n",
    "        rename_map[\"question_text_norm\"] = \"question_norm\"\n",
    "    if \"question_text_stem\" in ds.column_names and \"question_stem\" not in ds.column_names:\n",
    "        rename_map[\"question_text_stem\"] = \"question_stem\"\n",
    "    if \"answer_text\" in ds.column_names and \"answer\" not in ds.column_names:\n",
    "        rename_map[\"answer_text\"] = \"answer\"\n",
    "\n",
    "    if rename_map:\n",
    "        ds = ds.rename_columns(rename_map)\n",
    "\n",
    "    # 3) Tambah kolom yang hilang (default)\n",
    "    need_cols = set(TARGET_FEATURES.keys()) - set(ds.column_names)\n",
    "    def _add_missing(batch):\n",
    "        n = len(batch[next(iter(batch))]) if batch else 0\n",
    "        out = {}\n",
    "        for c in need_cols:\n",
    "            if c == \"answer\":\n",
    "                out[c] = [[] for _ in range(n)]\n",
    "            elif c == \"mr_negative_passages\":\n",
    "                out[c] = [[] for _ in range(n)]\n",
    "            else:\n",
    "                out[c] = [\"\" for _ in range(n)]\n",
    "        return out\n",
    "    if need_cols:\n",
    "        ds = ds.map(_add_missing, batched=True)\n",
    "\n",
    "    # 4) Coerce nilai & tipe ke target\n",
    "    def _coerce(batch):\n",
    "        out = {}\n",
    "        # id → string\n",
    "        out[\"id\"] = [_to_str_or_empty(x) for x in batch.get(\"id\", [\"\"]*len(batch[\"question\"]))]\n",
    "        # question / passage / title / norms\n",
    "        out[\"question\"] = [_to_str_or_empty(x) for x in batch.get(\"question\", [\"\"]*len(out[\"id\"]))]\n",
    "        out[\"passage_text\"] = [_to_str_or_empty(x) for x in batch.get(\"passage_text\", [\"\"]*len(out[\"id\"]))]\n",
    "        out[\"document_title\"] = [_to_str_or_empty(x) for x in batch.get(\"document_title\", [\"\"]*len(out[\"id\"]))]\n",
    "        out[\"question_norm\"] = [_to_str_or_empty(x) for x in batch.get(\"question_norm\", [\"\"]*len(out[\"id\"]))]\n",
    "        out[\"question_stem\"] = [_to_str_or_empty(x) for x in batch.get(\"question_stem\", [\"\"]*len(out[\"id\"]))]\n",
    "\n",
    "        # answer → list[str]\n",
    "        out[\"answer\"] = [_ensure_list_of_str(x) for x in batch.get(\"answer\", [[]]*len(out[\"id\"]))]\n",
    "\n",
    "        # mr_query_id / mr_docid → string\n",
    "        out[\"mr_query_id\"] = [_to_str_or_empty(x) for x in batch.get(\"mr_query_id\", [\"\"]*len(out[\"id\"]))]\n",
    "        out[\"mr_docid\"]    = [_to_str_or_empty(x) for x in batch.get(\"mr_docid\", [\"\"]*len(out[\"id\"]))]\n",
    "\n",
    "        # mr_negative_passages → list[str] (disederhanakan; jika sudah list-of-dict, jadikan list kosong saja atau string ringkas)\n",
    "        mnp = batch.get(\"mr_negative_passages\", [[]]*len(out[\"id\"]))\n",
    "        norm_mnp = []\n",
    "        for v in mnp:\n",
    "            if v is None:\n",
    "                norm_mnp.append([])\n",
    "            elif isinstance(v, (list, tuple)):\n",
    "                # kalau list of dict {docid,title,text} → bisa ringkas jadi list judul atau kosongkan saja\n",
    "                if len(v) > 0 and isinstance(v[0], dict):\n",
    "                    # ringkas: simpan title|text (bisa juga \"\" agar selaras)\n",
    "                    norm_mnp.append([_to_str_or_empty(d.get(\"title\")) for d in v])\n",
    "                else:\n",
    "                    # asumsikan list of str\n",
    "                    norm_mnp.append([_to_str_or_empty(x) for x in v])\n",
    "            else:\n",
    "                norm_mnp.append([_to_str_or_empty(v)])\n",
    "        out[\"mr_negative_passages\"] = norm_mnp\n",
    "\n",
    "        # source → string\n",
    "        out[\"source\"] = [_to_str_or_empty(x) for x in batch.get(\"source\", [\"\"]*len(out[\"id\"]))]\n",
    "\n",
    "        return out\n",
    "\n",
    "    ds = ds.map(_coerce, batched=True)\n",
    "\n",
    "    # 5) Paksa features (cast) ke TARGET_FEATURES\n",
    "    ds = ds.cast(TARGET_FEATURES)\n",
    "\n",
    "    return ds\n",
    "\n",
    "# ===== 1) Siapkan masing-masing dataset dengan skema yang sama =====\n",
    "indoqa_prepared = _prepare_and_cast(indoqa, \"indoqa\")\n",
    "facqa_prepared  = _prepare_and_cast(facqa_wo_dedupe, \"facqa\")\n",
    "tydi_prepared   = _prepare_and_cast(tydiqa_with_mr, \"tydiqa_with_mr\")\n",
    "\n",
    "# ===== 2) Gabungkan =====\n",
    "all_ds = concatenate_datasets([indoqa_prepared, facqa_prepared, tydi_prepared])\n",
    "print(all_ds)\n",
    "\n",
    "# ===== 3) Cek duplikasi berdasarkan question_stem =====\n",
    "import pandas as pd\n",
    "df = all_ds.to_pandas()\n",
    "df_q = df[df[\"question_stem\"].notna() & (df[\"question_stem\"] != \"\")].copy()\n",
    "\n",
    "dup_mask = df_q.duplicated(subset=[\"question_stem\"], keep=False)\n",
    "dupes_df = df_q[dup_mask].copy()\n",
    "\n",
    "n_rows_total = len(df_q)\n",
    "n_rows_dup   = len(dupes_df)\n",
    "n_qs_dup     = dupes_df[\"question_stem\"].nunique()\n",
    "\n",
    "print(f\"Total baris dengan question_stem   : {n_rows_total}\")\n",
    "print(f\"Total baris yg termasuk duplikat   : {n_rows_dup}\")\n",
    "print(f\"Jumlah question_stem yang duplikat : {n_qs_dup}\")\n",
    "\n",
    "# (opsional) contoh 5 grup duplikat dan sumbernya\n",
    "if n_qs_dup > 0:\n",
    "    cnt = (dupes_df.groupby(\"question_stem\")[\"source\"]\n",
    "           .agg(lambda s: dict(pd.Series(s).value_counts()))\n",
    "           .reset_index(name=\"by_source\"))\n",
    "    print(\"\\nContoh 5 question_stem duplikat (by source):\")\n",
    "    for _, row in cnt.head(5).iterrows():\n",
    "        print(f\"- {row['question_stem']!r} -> {row['by_source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c8326602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File berhasil disimpan ke duplicate_ids.xlsx dengan 897 baris duplikat.\n"
     ]
    }
   ],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# # ambil semua nilai kolom id\n",
    "# ids = all_ds[\"id\"]\n",
    "\n",
    "# # cek jumlah total id dan jumlah unik\n",
    "# total = len(ids)\n",
    "# unique = len(set(ids))\n",
    "\n",
    "# print(f\"Total id     : {total}\")\n",
    "# print(f\"Unique id    : {unique}\")\n",
    "# print(f\"Jumlah duplikat: {total - unique}\")\n",
    "\n",
    "# # kalau mau lihat id yang terduplikasi beserta jumlahnya\n",
    "# duplicates = [item for item, count in Counter(ids).items() if count > 1]\n",
    "# print(\"Contoh id terduplikasi:\", duplicates[:10])  # tampilkan sebagian\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# konversi ke pandas DataFrame\n",
    "df = all_ds.to_pandas()\n",
    "\n",
    "# cari baris dengan id duplikat\n",
    "df_dup = df[df.duplicated(subset=[\"id\"], keep=False)]\n",
    "\n",
    "# sort berdasarkan kolom id\n",
    "df_dup = df_dup.sort_values(by=\"id\")\n",
    "\n",
    "# simpan ke excel\n",
    "output_path = \"duplicate_ids.xlsx\"\n",
    "df_dup.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"File berhasil disimpan ke {output_path} dengan {len(df_dup)} baris duplikat.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ee1dd095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File 'all_datasets_question_stem_duplicates.xlsx' berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "# Urutkan duplikat berdasarkan question_stem\n",
    "dupes_sorted = dupes_df.sort_values(by=\"question_stem\").reset_index(drop=True)\n",
    "\n",
    "# Simpan ke Excel\n",
    "dupes_sorted.to_excel(\"all_datasets_question_stem_duplicates.xlsx\", index=False)\n",
    "\n",
    "print(\"✅ File 'all_datasets_question_stem_duplicates.xlsx' berhasil dibuat.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c51086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 2) Daftar pasangan dari Excel (sudah dituliskan ulang ke kode) ======\n",
    "tydi_wrong_labels_index = [\"7087322768323666798-2\", \"-1483127104798159762-2\", \"-954542885931683301-58\", \"-2854454553098103373-6\"]\n",
    "\n",
    "# Ubah ke set untuk lookup cepat; trimming spasi agar toleran spasi ekstra\n",
    "PAIRS_SET = set((qs.strip(), ans.strip()) for qs, ans in EXCEL_PAIRS)\n",
    "\n",
    "# ====== Filter facqa: drop baris yang match (question_text_stem, salah-satu answer_text) ======\n",
    "def _should_keep_facqa(example):\n",
    "    qstem = (example.get(\"question_text_stem\") or \"\").strip()\n",
    "    answers = example.get(\"answer_text\") or []  # list[str]\n",
    "    for t in answers:\n",
    "        if (qstem, (t or \"\").strip()) in PAIRS_SET:\n",
    "            return False  # baris ini DIHAPUS\n",
    "    return True  # keep\n",
    "\n",
    "before_n = len(facqa)\n",
    "facqa_filtered = facqa.filter(_should_keep_facqa)\n",
    "after_n = len(facqa_filtered)\n",
    "removed = before_n - after_n\n",
    "print(f\"Rows before: {before_n} | after: {after_n} | removed: {removed}\")\n",
    "\n",
    "# ====== (Opsional) laporan pasangan Excel yang tidak ketemu di dataset ======\n",
    "def _collect_hits(example):\n",
    "    qstem = (example.get(\"question_text_stem\") or \"\").strip()\n",
    "    answers = example.get(\"answer_text\") or []\n",
    "    hits = []\n",
    "    for t in answers:\n",
    "        key = (qstem, (t or \"\").strip())\n",
    "        if key in PAIRS_SET:\n",
    "            hits.append(key)\n",
    "    return {\"_hits\": hits}\n",
    "\n",
    "hits_ds = facqa.map(_collect_hits, batched=False)\n",
    "found_pairs = set()\n",
    "for row in hits_ds:\n",
    "    for h in row.get(\"_hits\", []):\n",
    "        # 'h' sudah tuple-like (atau list dua elemen)\n",
    "        found_pairs.add(tuple(h))\n",
    "\n",
    "not_found = PAIRS_SET - found_pairs\n",
    "if not_found:\n",
    "    print(\"\\nPairs from Excel NOT FOUND in facqa:\")\n",
    "    for qs, ans in sorted(not_found):\n",
    "        print(f\"- question_text_stem={qs!r} | answer_text element={ans!r}\")\n",
    "\n",
    "# Commit hasil filter\n",
    "facqa = facqa_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d4441",
   "metadata": {},
   "source": [
    "ambil negative passages buat yg belum punya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a1bfa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4298/4298 [38:27<00:00,  1.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import gc\n",
    "\n",
    "# ===== Model & tokenizer (Multilingual-E5-Small) =====\n",
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embedding_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = embedding_model.to(device)\n",
    "\n",
    "# ===== Helper: average pooling =====\n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    masked = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return masked.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "# ===== Top-K selector over mr_negative_passages using query_norm =====\n",
    "TOP_K = 4  # ganti ke 2 jika hanya ingin top-2\n",
    "\n",
    "def select_topk_mr_negative_passages(example):\n",
    "    negs = example.get(\"mr_negative_passages\") or []\n",
    "    # Jika kosong atau sudah <= TOP_K, tidak perlu proses\n",
    "    if len(negs) <= TOP_K:\n",
    "        return example\n",
    "\n",
    "    # E5 expects prefixes: 'query: ...' dan 'passage: ...'\n",
    "    query_text = f'query: {example.get(\"query_norm\", \"\")}'\n",
    "\n",
    "    # Rakit teks passage: \"passage: {title} | {text}\"\n",
    "    neg_texts = []\n",
    "    for neg in negs:\n",
    "        title = (neg.get(\"title\") or \"\").strip()\n",
    "        text  = (neg.get(\"text\")  or \"\").strip()\n",
    "        neg_texts.append(f'passage: {title} | {text}')\n",
    "\n",
    "    # Tokenisasi gabungan (query + semua negative passages)\n",
    "    batch_dict = embedding_tokenizer(\n",
    "        [query_text] + neg_texts,\n",
    "        max_length=512,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**batch_dict)\n",
    "\n",
    "    # Embedding + normalisasi (cosine)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    # Pisah query vs passages\n",
    "    query_emb = embeddings[0].unsqueeze(0)\n",
    "    neg_embs  = embeddings[1:]\n",
    "\n",
    "    # Similarity (cosine) → ambil TOP_K indeks terbaik\n",
    "    scores = (query_emb @ neg_embs.T).squeeze(0)\n",
    "    top_idx = torch.argsort(scores, descending=True)[:TOP_K].tolist()\n",
    "\n",
    "    # Pertahankan hanya TOP_K dokumen terbaik\n",
    "    example[\"mr_negative_passages\"] = [negs[i] for i in top_idx]\n",
    "\n",
    "    # Bersih-bersih memori\n",
    "    del batch_dict, outputs, embeddings, scores, query_emb, neg_embs\n",
    "    torch.cuda.empty_cache() if device.startswith(\"cuda\") else None\n",
    "    gc.collect()\n",
    "\n",
    "    return example\n",
    "\n",
    "# ===== Terapkan ke dataset =====\n",
    "# (single-process aman untuk fungsi berat GPU seperti ini)\n",
    "tydiqa_with_mr_neg_filled = tydiqa_with_mr_neg_filled.map(\n",
    "    select_topk_mr_negative_passages,\n",
    "    batched=False,\n",
    "    load_from_cache_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4841dac",
   "metadata": {},
   "source": [
    "Chunking negative passages yang sudah diatur oleh Mr. TyDi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8169f950",
   "metadata": {},
   "source": [
    "ambil top 2 dokumen negatif utk split train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ea5cd",
   "metadata": {},
   "source": [
    "truncate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d656f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
