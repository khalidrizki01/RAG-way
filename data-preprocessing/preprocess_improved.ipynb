{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f53264a",
   "metadata": {},
   "source": [
    "# Fungsi Dasar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adf6e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata \n",
    "\n",
    "# === helpers ===\n",
    "ZW_RE = re.compile(r\"[\\u200b-\\u200d\\u2060\\ufeff\\u00ad\\u200e\\u200f]\")\n",
    "NBSP_RE = re.compile(r\"[\\u00A0]\")\n",
    "MULTI_SPACE_RE = re.compile(r\"\\s{2,}\")\n",
    "\n",
    "# wiki image markup (case-insensitive)\n",
    "WIKI_IMG_TOKEN_RE = re.compile(\n",
    "    r\"(?i)(?:\\|\\s*(?:jmpl|thumb|kiri|kanan|ka)\\b|\\b(?:jmpl|thumb|kiri|kanan|ka)\\b\\s*\\|?)\"\n",
    ")\n",
    "WIKI_IMG_SIZE_RE = re.compile(\n",
    "    r\"(?i)(?:\\|\\s*\\d{2,4}(?:x\\d{2,4})?px\\b|\\b\\d{2,4}(?:x\\d{2,4})?px\\b\\s*\\|?)\"\n",
    ")\n",
    "# sisa pipa/whitespace berlebih\n",
    "PIPE_SPACES_RE = re.compile(r\"(?:\\s*\\|\\s*)+\")\n",
    "\n",
    "_UNITS = r\"(km2|m2|km|m|cm|mm|kg|g|mg|t|l|ml|hz|khz|mhz|ghz|v|kv|w|kw|mw|a|ma|pa|kpa|mpa|bar|%)\"\n",
    "\n",
    "def normalize(q: str, lowercase: bool = True) -> str:\n",
    "    if not q:\n",
    "        return \"\"\n",
    "    # -------- hapus sitasi [angka] --------\n",
    "    q = re.sub(r\"\\[\\d+\\]\", \"\", q)\n",
    "\n",
    "    # -------- unicode & basic cleanup --------\n",
    "    q = unicodedata.normalize(\"NFKC\", q)\n",
    "    q = ZW_RE.sub(\"\", q)\n",
    "    q = NBSP_RE.sub(\" \", q)\n",
    "\n",
    "    # -------- bersihkan artefak gambar wiki --------\n",
    "    for _ in range(2):\n",
    "        q = WIKI_IMG_TOKEN_RE.sub(\" \", q)\n",
    "        q = WIKI_IMG_SIZE_RE.sub(\" \", q)\n",
    "    q = PIPE_SPACES_RE.sub(\" \", q)\n",
    "\n",
    "    # -------- bersihkan artefak HTML/parsoid --------\n",
    "    q = q.replace(r\"\\'\", \"'\").replace(r'\\\"', '\"')\n",
    "    # A) buang tag italic <i ...> / </i> lengkap/terpenggal\n",
    "    q = re.sub(r\"(?i)(?<=\\w)<\\s*/?\\s*i\\b[^>]*>?(?=\\w)\", \"\", q)\n",
    "    q = re.sub(r\"(?i)<\\s*/?\\s*i\\b[^>]*>?\", \" \", q)\n",
    "    q = re.sub(r\"(?i)<\\s*/?\\s*i\\b[^\\n<]*$\", \" \", q)\n",
    "\n",
    "    # 1) hapus atribut data-parsoid (toleran kutip/penutup tidak lengkap)\n",
    "    q = re.sub(\n",
    "        r\"\\s*data-parsoid\\s*=\\s*(?:'[^']*'?|\\\"[^\\\"]*\\\"?|[^>\\s]+)\",\" \",q,flags=re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "    # C) HAPUS \"dsr\":[<deret angka/titik>] MESKI TANPA PENUTUP\n",
    "    #    Contoh yang disapu: \"dsr\":[417424.3.3   atau   'dsr':[12.34\n",
    "    q = re.sub(r'''(?i)[\"']dsr[\"']\\s*:\\s*\\[\\s*[0-9.,-]*''', \" \", q)\n",
    "    #    Tambahan: kalau key-nya (korup) tanpa kutip\n",
    "    q = re.sub(r'''(?i)\\bdsr\\b\\s*:\\s*\\[\\s*[0-9.,-]*''', \" \", q)\n",
    "\n",
    "    # D) setelah dsr dihapus sering tersisa buntut seperti \"}'>\", buang:\n",
    "    q = re.sub(r\"\\}\\s*['\\\"]?\\s*>\", \" \", q)          # }'>  atau }\">  → spasi\n",
    "    q = re.sub(r\"(?<=\\S)['\\\"]\\s*>\", \" \", q)         # '>, \"> setelah token lain → spasi\n",
    "    # D) buang penutup yang menggantung (], }, ) + opsional ' atau \" atau >) tepat sebelum huruf\n",
    "    q = re.sub(r'(?:(?<=^)|(?<=\\s))[\\]\\}\\)]+(?:\\s*[\"\\']?\\s*>?)?\\s*(?=\\w)', '', q)\n",
    "\n",
    "\n",
    "    # 1) Hapus tag yang berada DI DALAM kata tanpa meninggalkan spasi\n",
    "    q = re.sub(r\"(?<=\\w)</?\\s*[a-zA-Z][^>]*>(?=\\w)\", \"\", q)\n",
    "\n",
    "    # hapus tag HTML normal (misalnya <b>...</b>)\n",
    "    q = re.sub(r\"</?\\s*[a-zA-Z][^>]*>\", \" \", q)\n",
    "\n",
    "    # sambungkan huruf yang terpisah oleh tag patah /b>, /i>, /u>, /s>\n",
    "    q = re.sub(r\"(?<=\\w)/[bius]>(?=\\w)\", \"\", q, flags=re.IGNORECASE)\n",
    "\n",
    "    # hilangkan sisa /b> /i> /u> /s> sendirian\n",
    "    q = re.sub(r\"/[bius]>\", \" \", q, flags=re.IGNORECASE)\n",
    "\n",
    "    # -------- lower & normalisasi tanda baca --------\n",
    "\n",
    "    q = q.strip()\n",
    "    q = (q.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\")\n",
    "           .replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "           .replace(\"…\", \"...\")\n",
    "           .replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "           .replace(\"؟\", \"?\").replace(\"．\", \".\").replace(\"؟\", \"?\"))\n",
    "    \n",
    "    if lowercase:\n",
    "        q = q.lower()\n",
    "\n",
    "    # --- spasi + tanda baca (ramah tanda kurung) ---\n",
    "    q = re.sub(r\"\\s+\\?\", \"?\", q)                            # hapus spasi sebelum '?'\n",
    "    q = re.sub(r\"\\s+([,.:;!?/\\)\\-])\", r\"\\1\", q)             # hapus spasi sebelum tanda baca (kecuali '(')\n",
    "    q = re.sub(r\"([/(])\\s+\", r\"\\1\", q)                      # hapus spasi setelah '/' dan '('\n",
    "    q = re.sub(r\"(?<=\\w)\\(\", r\" (\", q)                      # pastikan ada spasi sebelum '(' bila didahului huruf/angka\n",
    "    q = re.sub(r\"\\)(?=\\w)\", r\") \", q)                       # pastikan ada spasi setelah ')' bila diikuti huruf/angka\n",
    "\n",
    "    # rapikan spasi\n",
    "    q = MULTI_SPACE_RE.sub(\" \", q).strip()\n",
    "\n",
    "    # normalisasi ringan kosakata/angka\n",
    "    q = re.sub(r\"\\bdi\\s+mana\\b\", \"dimana\", q)\n",
    "    q = re.sub(r\"\\byg\\b\", \"yang\", q)\n",
    "    q = q.replace(\"km²\", \"km2\").replace(\"m²\", \"m2\")\n",
    "    # --- sisipkan spasi antara angka dan satuan ---\n",
    "\n",
    "    # 12km -> 12 km, 5m2 -> 5 m2, 100% -> 100 %\n",
    "    q = re.sub(rf\"(?i)(\\d)\\s*({_UNITS})\\b\", r\"\\1 \\2\", q)\n",
    "\n",
    "    # suhu: 50°C / 50 °C -> 50 °C  (pastikan ada spasi sebelum '°' dan antara ° dengan huruf)\n",
    "    q = re.sub(r\"(?i)(\\d)\\s*°\\s*([cf])\\b\", r\"\\1 °\\2\", q)\n",
    "\n",
    "    q = re.sub(r\"\\s*&\\s*\", \" dan \", q)\n",
    "    q = re.sub(r\"\\?{2,}\", \"?\", q)\n",
    "\n",
    "    # final cleanup\n",
    "    q = MULTI_SPACE_RE.sub(\" \", q).strip()\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "674fd368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def add_norm_and_stem(batch, column):\n",
    "    qs = batch[column]\n",
    "    norm = [normalize(x) for x in qs]\n",
    "    stem = [stemmer.stem(x) for x in norm]  # stem setelah normalisasi\n",
    "    return {f\"{column}_norm\": norm, f\"{column}_stem\": stem}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c31939b",
   "metadata": {},
   "source": [
    "# Membuat Korpus Panjang 180 Token nan Sudah Dinormalisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87106a",
   "metadata": {},
   "source": [
    "Gunanya untuk \n",
    "1) Pretraining model xRAG\n",
    "2) Menyusun data negative passages dari bagian dataset yang belum punya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331ef9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['docid', 'title', 'text'],\n",
       "        num_rows: 1469399\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "# Load corpus & tokenizer\n",
    "corpus = load_dataset(\"castorini/mr-tydi-corpus\", \"indonesian\", trust_remote_code=True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f81d1",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "nengok kasus dimana terdapat tag html atau yang mirip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60f79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil disimpan ke mr_tydi_matches.xlsx, total baris: 97\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Muat dataset (ganti sesuai punyamu)\n",
    "ds = load_dataset(\"castorini/mr-tydi-corpus\", \"indonesian\", split=\"train\")\n",
    "\n",
    "# Pola rawan\n",
    "PATTERNS = {\n",
    "    \"slash_bius\": re.compile(r\"/[bius]>\"),            # /b> /i> /u> /s>\n",
    "    \"broken_join\": re.compile(r\"\\w/[bius]>\\w\"),       # d/b>e\n",
    "    \"data_parsoid\": re.compile(r\"data-parsoid\\s*=\", re.I),\n",
    "    \"dsr\": re.compile(r'\"dsr\"\\s*:\\s*\\[', re.I),\n",
    "    \"html_tag\": re.compile(r\"</?\\s*[a-zA-Z][^>]*>\"),  # <...> tag\n",
    "}\n",
    "\n",
    "# Simpan hasil match\n",
    "records = []\n",
    "for ex in ds.select(range(100000)):\n",
    "    text = ex[\"text\"]\n",
    "    matches = {name: bool(pat.search(text)) for name, pat in PATTERNS.items()}\n",
    "    if any(matches.values()):\n",
    "        record = {\n",
    "            \"docid\": ex[\"docid\"],\n",
    "            \"title\": ex[\"title\"],\n",
    "            \"text_snippet\": text,  # simpan potongan teks saja biar file tidak terlalu berat\n",
    "        }\n",
    "        record.update(matches)\n",
    "        records.append(record)\n",
    "\n",
    "# Buat DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Simpan ke Excel\n",
    "output_file = \"mr_tydi_matches.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Hasil disimpan ke {output_file}, total baris: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507449f0",
   "metadata": {},
   "source": [
    "## Normalisasi dan Chunking 180 Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8462dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "MAX_TOK = 180\n",
    "MIN_TOK = 50\n",
    "\n",
    "# ---------- Normalisasi ----------\n",
    "\n",
    "def normalize_batch(batch):\n",
    "    batch[\"title\"] = [normalize(t, lowercase=False) for t in batch[\"title\"]]\n",
    "    batch[\"text\"] = [normalize(t, lowercase=False) for t in batch[\"text\"]]\n",
    "    return batch\n",
    "\n",
    "# ---------- Chunking ----------\n",
    "def chunk_batch(batch):\n",
    "    out_docid, out_title, out_text = [], [], []\n",
    "    for docid, title, text in zip(batch[\"docid\"], batch[\"title\"], batch[\"text\"]):\n",
    "        token_ids = t5_tokenizer.encode(text, add_special_tokens=False)\n",
    "        if not token_ids:\n",
    "            continue\n",
    "\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(token_ids):\n",
    "            end = min(start + MAX_TOK, len(token_ids))\n",
    "\n",
    "            # pastikan akhir chunk di batas kata\n",
    "            if end < len(token_ids):\n",
    "                tok_str = t5_tokenizer.convert_ids_to_tokens([token_ids[end]])[0]\n",
    "                while end > start and not tok_str.startswith(\"▁\"):\n",
    "                    end -= 1\n",
    "                    tok_str = t5_tokenizer.convert_ids_to_tokens([token_ids[end]])[0]\n",
    "                if end == start:\n",
    "                    end = min(start + MAX_TOK, len(token_ids))\n",
    "\n",
    "            chunks.append(token_ids[start:end])\n",
    "            start = end\n",
    "\n",
    "        # gabungkan segmen terakhir kalau <50 token\n",
    "        if len(chunks) >= 2 and len(chunks[-1]) < MIN_TOK:\n",
    "            chunks[-2].extend(chunks[-1])\n",
    "            chunks = chunks[:-1]\n",
    "        elif len(chunks) == 1 and len(chunks[0]) < MIN_TOK:\n",
    "            chunks = []  # terlalu pendek → buang\n",
    "\n",
    "        # simpan segmen valid\n",
    "        for idx, seg in enumerate(chunks):\n",
    "            if len(seg) < MIN_TOK:\n",
    "                continue\n",
    "            out_docid.append(f\"{docid}_{idx}\")\n",
    "            out_title.append(title)\n",
    "            out_text.append(t5_tokenizer.decode(seg, skip_special_tokens=True))\n",
    "\n",
    "    return {\"docid\": out_docid, \"title\": out_title, \"text\": out_text}\n",
    "\n",
    "# ---------- Pipeline ----------\n",
    "# 1. Normalisasi dulu\n",
    "normalized_corpus = corpus.map(\n",
    "    normalize_batch,\n",
    "    batched=True,\n",
    "    desc=\"Normalizing title & text\"\n",
    ")\n",
    "\n",
    "# 2. Chunking setelah normalisasi\n",
    "split_dataset = normalized_corpus[\"train\"].map(\n",
    "    chunk_batch,\n",
    "    batched=True,\n",
    "    remove_columns=normalized_corpus[\"train\"].column_names,\n",
    "    desc=\"Chunking into <=180 tokens (word-aware) & merging last <50\"\n",
    ")\n",
    "\n",
    "print(split_dataset)\n",
    "print(split_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513645e4",
   "metadata": {},
   "source": [
    "## Hapus Duplikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75cd836b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah duplikat (berdasarkan kolom text): 38\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ambil 10.000 baris pertama\n",
    "sample = split_dataset.select(range(10000))\n",
    "\n",
    "# konversi ke DataFrame untuk analisis cepat\n",
    "df = pd.DataFrame(sample)\n",
    "\n",
    "# cek duplikasi berdasarkan kolom 'text'\n",
    "dup_mask = df.duplicated(subset=[\"text\"], keep=False)\n",
    "duplicates = df[dup_mask].sort_values(by=\"text\")\n",
    "\n",
    "# simpan hasil duplikat ke Excel\n",
    "duplicates.to_excel(\"duplicates_first_10k.xlsx\", index=False)\n",
    "\n",
    "print(f\"Jumlah duplikat (berdasarkan kolom text): {duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac9e46e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah awal: 1588236, jumlah setelah buang duplikat: 1521373\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Konversi dulu ke pandas DataFrame\n",
    "df = split_dataset.to_pandas()\n",
    "\n",
    "# Hapus duplikasi berdasarkan kolom \"text\"\n",
    "# keep='first' artinya baris pertama dipertahankan\n",
    "df_nodup = df.drop_duplicates(subset=[\"text\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# Konversi kembali ke HuggingFace Dataset\n",
    "split_dataset_nodup = Dataset.from_pandas(df_nodup)\n",
    "\n",
    "print(f\"Jumlah awal: {len(split_dataset)}, jumlah setelah buang duplikat: {len(split_dataset_nodup)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da46813c",
   "metadata": {},
   "source": [
    "## Push ke Huggingface (Tanpa Pembagian Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3599ef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 795/795 [00:02<00:00, 283.55ba/s]\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Creating parquet from Arrow format: 100%|██████████| 795/795 [00:02<00:00, 315.18ba/s]\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:52<00:00, 26.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# pip install datasets huggingface_hub\n",
    "\n",
    "from datasets import DatasetDict\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from datetime import datetime\n",
    "\n",
    "# ====== asumsikan split_dataset sudah ada dari langkahmu sebelumnya ======\n",
    "# split_dataset = normalized_corpus[\"train\"].map(...)\n",
    "\n",
    "# Bungkus ke DatasetDict agar rapi (boleh tambah 'validation'/'test' kalau ada)\n",
    "split_dataset_dict = DatasetDict({\"train\": split_dataset})\n",
    "\n",
    "# ====== Konfigurasi repo ======\n",
    "repo_id = \"khalidrizki/indonesian-wiki-chunked-180tok\"   # ganti\n",
    "private = False                                          # True kalau mau privat\n",
    "token = HfFolder.get_token()\n",
    "api = HfApi()\n",
    "\n",
    "# 1) Buat repo (jika belum ada)\n",
    "api.create_repo(\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    private=private,\n",
    "    exist_ok=True,\n",
    "    token=token,\n",
    ")\n",
    "\n",
    "# 2) Push dataset\n",
    "split_dataset_dict.push_to_hub(\n",
    "    repo_id=repo_id,\n",
    "    private=private,\n",
    ")\n",
    "\n",
    "# 3) Siapkan README (dataset card)\n",
    "readme = f\"\"\"# mr-tydi-indonesian-chunked-180tok\n",
    "\n",
    "Hasil preprocessing dari [`castorini/mr-tydi-corpus`](https://huggingface.co/datasets/castorini/mr-tydi-corpus) split **Indonesian**.\n",
    "\n",
    "## Pipeline\n",
    "1. **Normalisasi teks**\n",
    "   - Bersihkan artefak Parsoid/markup wiki (mis. `data-parsoid`, `dsr`, tag HTML patah)\n",
    "   - Normalisasi whitespace & tanda baca\n",
    "2. **Chunking word-aware**\n",
    "   - Tokenizer: **google/flan-t5-base** (SentencePiece)\n",
    "   - Panjang segmen ≤ **180 token**\n",
    "   - Pemotongan berhenti di **batas kata** (hindari potong subword di tengah)\n",
    "   - Jika segmen terakhir < **50 token**, **digabung** ke segmen sebelumnya\n",
    "3. **docid baru**\n",
    "   - Format: `docidAsli#partisiOlehMrTydi_nomorSegmen` (mis. `12345#67_0`, `12345#67_1`, …)\n",
    "\n",
    "## Statistik\n",
    "- Dataset asli (train, ID): ≈ 1.47M dokumen\n",
    "- Dataset hasil chunking: lebih besar (tergantung distribusi panjang)\n",
    "- Panjang segmen: 50–180 token (bisa >180 bila segmen terakhir digabung)\n",
    "\n",
    "## Contoh\n",
    "```json\n",
    "{{\n",
    "  \"docid\": \"12345#0_0\",\n",
    "  \"title\": \"Dokar\",\n",
    "  \"text\": \"Dokar, kendaraan dengan kuda sebagai penarik ...\"\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872934c6",
   "metadata": {},
   "source": [
    "## 180 Token Menurut Flan T5 itu Berapa Token Multilingual E5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64abf2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 1.38kB [00:00, ?B/s]\n",
      "Downloading data: 100%|██████████| 146M/146M [00:14<00:00, 10.2MB/s] \n",
      "Downloading data: 100%|██████████| 124M/124M [00:11<00:00, 10.5MB/s] \n",
      "Generating train split: 100%|██████████| 1588236/1588236 [00:02<00:00, 641382.23 examples/s]\n",
      "Map: 100%|██████████| 100000/100000 [00:08<00:00, 11193.50 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rata-rata panjang token dari 100.000 sampel: 65.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load dataset & tokenizer\n",
    "corpus = load_dataset(\"khalidrizki/indonesian-wiki-chunked-180tok\", split=\"train\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-base\")\n",
    "\n",
    "# 2. Ambil sampel 100.000 data secara acak\n",
    "sampled = corpus.shuffle(seed=42).select(range(100_000))\n",
    "\n",
    "# 3. Tokenisasi batch agar lebih cepat\n",
    "def tokenize_batch(batch):\n",
    "    tokens = tokenizer(batch[\"text\"], truncation=False, padding=False)[\"input_ids\"]\n",
    "    return {\"lengths\": [len(t) for t in tokens]}\n",
    "\n",
    "sampled = sampled.map(tokenize_batch, batched=True, batch_size=1000)\n",
    "\n",
    "# 4. Hitung rata-rata panjang token\n",
    "avg_length = np.mean(sampled[\"lengths\"])\n",
    "print(f\"Rata-rata panjang token dari 100.000 sampel: {avg_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f24d4ed",
   "metadata": {},
   "source": [
    "## Pembagian Split (Train dan Validasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76482684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation rows: 7941, Target: 7941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1588236/1588236 [01:10<00:00, 22418.92 examples/s]\n",
      "Filter: 100%|██████████| 1588236/1588236 [01:11<00:00, 22218.61 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['docid', 'title', 'text', 'doc_prefix'],\n",
      "    num_rows: 1580295\n",
      "})\n",
      "Dataset({\n",
      "    features: ['docid', 'title', 'text', 'doc_prefix'],\n",
      "    num_rows: 7941\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"khalidrizki/indonesian-wiki-chunked-180tok\", split=\"train\")\n",
    "\n",
    "# Ekstrak prefix sebelum '#'\n",
    "def get_doc_prefix(example):\n",
    "    return {\"doc_prefix\": example[\"docid\"].split(\"#\")[0]}\n",
    "\n",
    "dataset = dataset.map(get_doc_prefix)\n",
    "\n",
    "# Group dokumen unik\n",
    "all_docs = list(set(dataset[\"doc_prefix\"]))\n",
    "random.seed(42)\n",
    "random.shuffle(all_docs)\n",
    "\n",
    "# Hitung target size\n",
    "target_val_size = 7941\n",
    "\n",
    "# Iteratif pilih dokumen sampai mendekati target\n",
    "val_docs = []\n",
    "val_count = 0\n",
    "for d in all_docs:\n",
    "    count_d = (np.array(dataset[\"doc_prefix\"]) == d).sum()\n",
    "    if val_count + count_d <= target_val_size:\n",
    "        val_docs.append(d)\n",
    "        val_count += count_d\n",
    "    if val_count >= target_val_size:\n",
    "        break\n",
    "\n",
    "print(f\"Validation rows: {val_count}, Target: {target_val_size}\")\n",
    "\n",
    "# Split dataset\n",
    "val_dataset = dataset.filter(lambda x: x[\"doc_prefix\"] in val_docs)\n",
    "train_dataset = dataset.filter(lambda x: x[\"doc_prefix\"] not in val_docs)\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ba914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Gabungkan ke dalam DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"dev\": val_dataset\n",
    "})\n",
    "\n",
    "dataset.push_to_hub(\"khalidrizki/indonesian-wiki-chunked-180tok-splitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d6a6cf",
   "metadata": {},
   "source": [
    "# TyDi QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07787b35",
   "metadata": {},
   "source": [
    "## Khusus Data Question dan Answer (Tanpa Retrieval/Negative Passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316fa759",
   "metadata": {},
   "source": [
    "Gabungkan semua split jadi 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74325716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Muat dataset\n",
    "tydiqa_gold = load_dataset(\"khalidalt/tydiqa-goldp\", \"indonesian\", trust_remote_code=True)\n",
    "\n",
    "# Gabungkan semua split di tydiqa_gold\n",
    "tydiqa_gold_all = concatenate_datasets([tydiqa_gold[split] for split in tydiqa_gold.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "231fe85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'language', 'document_title', 'passage_text', 'question_text', 'answers'],\n",
       "    num_rows: 6267\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tydiqa_gold_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d917ef",
   "metadata": {},
   "source": [
    "Rename kolom question pada tydiqa menjadi query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3527940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tydiqa_gold_all = tydiqa_gold_all.rename_column(\"question_text\", \"question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479c36e",
   "metadata": {},
   "source": [
    "Normalisasi dan buat kolom question_stem, yakni query yang sudah di-stemming. Ini gunanya untuk memudahkan membuang duplikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7444977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6267/6267 [09:34<00:00, 10.91 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'language', 'document_title', 'passage_text', 'question', 'answers', 'question_norm', 'question_stem'],\n",
       "    num_rows: 6267\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tydiqa_gold_all = tydiqa_gold_all.map(\n",
    "    add_norm_and_stem, \n",
    "    batched=True, \n",
    "    fn_kwargs={\"column\": \"question\"}   # sesuaikan dengan nama kolom di tydiqa_gold_all\n",
    ")\n",
    "tydiqa_gold_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547c54e",
   "metadata": {},
   "source": [
    "Dengan melakukan inspeksi manual, dari baris yang question_stem nya terduplikasi, sudah ditentukan mana saja baris yang answer nya salah. \n",
    "\n",
    "Dengan demikian, baris-baris yang question_stem terduplikasi dan answer salah tersebut dihapus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfedd19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 6267/6267 [00:00<00:00, 22557.43 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before: 6267 | after: 6219 | removed: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6267/6267 [00:01<00:00, 4946.92 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairs from Excel NOT FOUND in dataset:\n",
      "- question_stem='apa bilang ganjil' | answers.text='t dibuktikan bahwa bilangan ganjil'\n",
      "- question_stem='apa ibukota israel' | answers.text='Yerusalem'\n",
      "- question_stem='apa ibukota jerman' | answers.text='Jerman Timur memilih Berlin Timur sebagai ibukota, sedangkan Jerman Barat memilih Bonn'\n",
      "- question_stem='apa nama ibukota israel' | answers.text='Yerusalem'\n",
      "- question_stem='apa nama ibukota jerman' | answers.text='Jerman Timur memilih Berlin Timur sebagai ibukota, sedangkan Jerman Barat memilih Bonn'\n",
      "- question_stem='apa nama ibukota provinsi sulawesi utara' | answers.text='kota Manado'\n",
      "- question_stem='apa nama mata uang korea utara' | answers.text='Won'\n",
      "- question_stem='apa warna bendera jepang' | answers.text='6.2R 4/15.2 untuk merah dan N9.2 untuk putih'\n",
      "- question_stem='apa yang maksud dengan surah makkiyah' | answers.text='yat-ayat yang turun sebelum Rasulullah SAW hijrah ke Madinah'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'language', 'document_title', 'passage_text', 'question', 'answers', 'question_norm', 'question_stem'],\n",
       "    num_rows: 6219\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====== 2) Daftar pasangan dari Excel (sudah dituliskan ulang ke kode) ======\n",
    "EXCEL_PAIRS = [\n",
    "    (\"apa bilang ganjil\", \"t dibuktikan bahwa bilangan ganjil\"),\n",
    "    ('apa definisi dari budaya', 'cara hidup yang berkembang, dan dimiliki bersama oleh sebuah kelompok orang, dan diwariskan dari generasi ke generasi'),\n",
    "    ('apa erti pasar dalam ilmu ekonomi', 'empat bertemunya penjual dan pembeli. Transaksi jual-beli yang terjadi tidak selalu memerlukan lokasi fisik'),\n",
    "    ('apa ibukota israel', 'Yerusalem'),\n",
    "    ('apa ibukota jerman', 'Jerman Timur memilih Berlin Timur sebagai ibukota, sedangkan Jerman Barat memilih Bonn'),\n",
    "    ('apa itu dna', '\"dsr\":[417,424,3,3]}\\'>n</b>ucleic a</b>cid), adalah sejenis biomolekul yang menyimpan dan menyandi instruksi-instru'),\n",
    "    ('apa itu manga', 'komik yang dibuat di Jepang, kata tersebut digunakan khusus untuk membicarakan tentang komik Jepang, sesuai dengan gaya yang dikembangkan di Jepang pada akhir abad ke-19'),\n",
    "    ('apa itu rna', 'molekul polimer yang terlibat dalam berbagai peran biologis dalam mengkode, dekode, regulasi, dan ekspresi gen'),\n",
    "    ('apa mata uang israel', 'Shekel baru Israel'),\n",
    "    ('apa nama ibukota israel', 'Yerusalem'),\n",
    "    ('apa nama ibukota jerman', 'Jerman Timur memilih Berlin Timur sebagai ibukota, sedangkan Jerman Barat memilih Bonn'),\n",
    "    ('apa nama mata uang israel', 'Shekel baru Israel'),\n",
    "    ('apa warna bendera jepang', '6.2R 4/15.2 untuk merah dan N9.2 untuk putih'),\n",
    "    ('apa yang maksud dengan budaya', 'suatu cara hidup yang berkembang, dan dimiliki bersama oleh sebuah kelompok orang, dan diwariskan dari generasi ke generasi'),\n",
    "    ('apa yang maksud dengan monarki', 'merupakan sejenis pemerintahan yang dipimpin oleh seorang penguasa monarki'),\n",
    "    ('apa yang maksud dengan surah makkiyah', 'yat-ayat yang turun sebelum Rasulullah SAW hijrah ke Madinah'),\n",
    "    ('apa yang maksud dengan zaman meiji', 'salah satu nama zaman pemerintahan kaisar Jepang sewaktu Kaisar Meiji memerintah Jepang, sesudah tahun Keiō(慶応) dan sebelum zaman zaman Taishō(大正'),\n",
    "    ('berapa luas danau toba', '100 kilometres (62 miles), lebar 30 kilometres (19mi), dan kedalaman 505 metres'),\n",
    "    ('berapa luas jakarta timur', '187,75km² (menurut Pemerintah Kota Administrasi Jakarta Timur)[1], atau seluas 188,19km² (menurut Badan Perencanaan Pembangunan Daerah Khusus Ibu Kota Jakarta)'),\n",
    "    ('berapa luas jerman', '357,021km'),\n",
    "    ('berapa luas kolombia', \"828,660km² dan yang kedua yang terbentuk oleh pegunungan Andes dan lempeng Llanos yang berbagi bersama Venezuela dengan area seluas 1'143,748km²\"), \n",
    "    ('berapa luas kota lampung', '35.376,50km²'),\n",
    "    ('berapa luas kuasa sultan mataram', 'Jawa dan sekitarnya, termasuk Madura'),\n",
    "    ('berapa luas negara israel',\t'27.799km2'),\n",
    "    (\"berapa luas papua\",\"808.105km persegi\"),\n",
    "    ('berapa luas pulau jawa','160 juta'),\n",
    "    ('berapa luas singapura', '5,815km'),\n",
    "    ('berapa luas sulawesi selatan','45.764,53'),\n",
    "    ('berapa panjang sungai kuning'\t,'40km'),\n",
    "    ('dari mana asal suku sunda','bagian barat pulau Jawa, Indonesia'),\n",
    "    (\"kapan dinasti tang diri\",\"618\"),\n",
    "    (\"kapan internet pertama masuk ke indonesia\",\"1994\"),\n",
    "    (\"kapan perintah orde baru mulai\",\"1966\"),\n",
    "    (\"kapan pt kereta api indonesia diri\",\"Mei 2010\"),\n",
    "    (\"kapan pt kereta api indonesia diri\",\"17 Juni 1864\"),\n",
    "    (\"kapan radio pertama kali cipta\",\"Guglielmo Marconi\"),\n",
    "    (\"kapan universitas indonesia diri\",\"1955\"),\n",
    "    (\"mana letak jam gadang\",\"kota Bukittinggi, Sumatera Barat, Indonesia\"),\n",
    "    (\"mana letak jepang\",\"ujung barat Samudra Pasifik, di sebelah timur Laut Jepang, dan bertetangga dengan Republik Rakyat Tiongkok, Korea, dan Rusia\"),\n",
    "    (\"mana letak mesir\",\"antara garis lintang 22 ° dan 32 ° N, dan garis bujur 25 ° dan 35 ° E\"),\n",
    "    (\"mana letak polandia\",\"Eropa Tengah yang berbatasan dengan Jerman di sebelah barat Perbatasan Oder-Neisse, Ceko, dan Slowakia di sebelah selatan, Rusia (Kaliningrad), Lituania di sebelah timur laut dan Belarus serta Ukraina di sebelah barat (Garis Curzon)\"),\n",
    "    (\"siapa diri dinasti han timur\",\"Liu Bang\"),\n",
    "    (\"siapa diri toyota motor corporation\",\"Toyoda\"),\n",
    "    (\"siapa nama kaisar pertama dinasti ming\",\"Zhu Yuanzhang\"),\n",
    "    (\"siapa yang cipta wiracarita mahabharata\",\"Begawan Byasa atau Vyasa\"), \n",
    "    (\"apa erti dari digital\", \"penggambaran dari suatu keadaan bilangan yang terdiri dari angka 0 dan 1 atau off dan on (bilangan biner)\"),\n",
    "    (\"berapa luas jambi\", \"53.435 km2\"),\n",
    "    (\"kapan sultan utsmaniyah diri\", \"bawa\"),\n",
    "    (\"apa itu dvd\", \"cakram padat yang dapat digunakan untuk menyimpan data, termasuk film dengan kualitas video dan audio yang lebih baik dari kualitas VCD\"),\n",
    "    (\"apa nama ibukota provinsi sulawesi utara\", \"kota Manado\"), \n",
    "    (\"apa nama mata uang korea utara\", \"Won\"), \n",
    "    (\"berapa luas samudera atlantik\", \"106.450.000km²\"),\n",
    "    (\"mana letak danau toba\", \"kaldera Gunung Berapi Super\")\n",
    "]\n",
    "\n",
    "# Ubah ke set untuk lookup cepat; trimming spasi di kiri/kanan biar \"persis\" tapi toleran spasi ekstra\n",
    "PAIRS_SET = set((qs.strip(), ans.strip()) for qs, ans in EXCEL_PAIRS)\n",
    "\n",
    "# ====== 3) Hapus baris di tydiqa_gold_all yang match kombinasi (question_stem, answers.text) ======\n",
    "# Asumsi: kolom 'question_stem' SUDAH ada di tydiqa_gold_all\n",
    "def _should_keep(example):\n",
    "    qstem = (example.get('question_stem') or \"\").strip()\n",
    "    texts = example.get('answers', {}).get('text', []) or []\n",
    "    # hapus baris bila ADA salah satu jawaban yang match persis\n",
    "    for t in texts:\n",
    "        if (qstem, (t or \"\").strip()) in PAIRS_SET:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "before_n = len(tydiqa_gold_all)\n",
    "filtered_train = tydiqa_gold_all.filter(_should_keep)\n",
    "after_n = len(filtered_train)\n",
    "removed = before_n - after_n\n",
    "\n",
    "print(f\"Rows before: {before_n} | after: {after_n} | removed: {removed}\")\n",
    "\n",
    "# (Opsional) laporan pasangan Excel yang tidak ketemu sama sekali di dataset\n",
    "# Kita cek apakah ada contoh yang match; kalau tidak ada, kita laporkan\n",
    "def _has_match(example):\n",
    "    qstem = (example.get('question_stem') or \"\").strip()\n",
    "    texts = example.get('answers', {}).get('text', []) or []\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        key = (qstem, (t or \"\").strip())\n",
    "        if key in PAIRS_SET:\n",
    "            out.append(key)\n",
    "    return {\"_hits\": out}\n",
    "\n",
    "hits_ds = tydiqa_gold_all.map(_has_match, batched=False)\n",
    "found_pairs = set()\n",
    "for row in hits_ds:\n",
    "    for h in row.get(\"_hits\", []):\n",
    "        found_pairs.add(tuple(h))\n",
    "\n",
    "not_found = PAIRS_SET - found_pairs\n",
    "if not_found:\n",
    "    print(\"\\nPairs from Excel NOT FOUND in dataset:\")\n",
    "    for qs, ans in sorted(not_found):\n",
    "        print(f\"- question_stem={qs!r} | answers.text={ans!r}\")\n",
    "\n",
    "# Commit hasil filter\n",
    "tydiqa_gold_all_dupe_wo_bad_labels = filtered_train\n",
    "tydiqa_gold_all_dupe_wo_bad_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd5a049",
   "metadata": {},
   "source": [
    "Ekstrak jawaban dari yang awalnya berada pada answers.text menjadi answer saja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9938f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6219/6219 [00:01<00:00, 4696.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_text(example):\n",
    "    example[\"answer\"] = example[\"answers\"][\"text\"]  # Ambil hanya bagian text, start_byte dan limit_byte dibuang saja\n",
    "    return example\n",
    "\n",
    "tydiqa_gold_all_dupe_wo_bad_labels = tydiqa_gold_all_dupe_wo_bad_labels.map(extract_text)\n",
    "tydiqa_gold_all_dupe_wo_bad_labels = tydiqa_gold_all_dupe_wo_bad_labels.remove_columns([\"language\", \"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa82b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 6219/6219 [00:00<00:00, 38414.74 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris yang miss: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def passage_miss_answer(example):\n",
    "    passage = example[\"passage_text\"]\n",
    "    answers = example[\"answer\"] or []  # list of string\n",
    "    \n",
    "    # True kalau tidak ada satu pun jawaban yang muncul di passage_text\n",
    "    return not any(ans in passage for ans in answers)\n",
    "tydi_miss = tydiqa_gold_all_dupe_wo_bad_labels.filter(passage_miss_answer)\n",
    "\n",
    "print(f\"Jumlah baris yang miss: {len(tydi_miss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df066546",
   "metadata": {},
   "source": [
    "Inspect lagi, apakah untuk kelompok question_stem sama (terduplikasi), apakah ada passage_text yang passage_text nya berbeda satu sama lain. Kalau berbeda, cek apakah jawaban berbeda juga. Jika jawaban berbeda, cari baris yang jawabannya benar dan sesuai dengan passage_text. Jawaban yang salah kemudian dihapus. Pencarian jawaban salah dilakukan dengan manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total question_stem duplikat: 224\n",
      "Grup konsisten (judul & passage sama semua): 205\n",
      "Grup konflik (judul/passage berbeda): 19\n",
      "\n",
      "=== Contoh konflik (maks 5) ===\n",
      "- question_stem: 'apa ibukota rusia' | n_rows=2, n_titles=1, n_passages=2\n",
      "  titles_sample: ['Rusia']\n",
      "  passages_sample: ['Sebagian besar jalur air Rusia sepanjang 102,000km (63,380mi) terdiri dari sungai atau danau alam. Ibukota Moskwa disebut sebagai \"pelabuhan lima laut\" karena memiliki 5 jalur air menuju Baltik, Laut Putih, Kaspia, Laut Azov dan Laut Hitam.']\n",
      "------------------------------------------------------------\n",
      "- question_stem: 'apa mata uang yang guna di jerman' | n_rows=2, n_titles=1, n_passages=2\n",
      "  titles_sample: ['Mark Jerman']\n",
      "  passages_sample: ['Peralihan dari mata uang resmi yang lama ke euro di Jerman berbeda dengan negara-negara Zona Euro lainnya. Di negara-negara lain, saat peralihan mata uang resmi lama dan euro beredar berdampingan selama dua bulan. Tetapi, di Jerman, koin dan uang kertas mark masih diterima sebagai pembayaran yang sah hingga 28 Februari 2002. Dan pihak Deutsche Bundesbank menjamin bahwa semua uang mark tunai dapat diganti ke Euro tanpa batas waktu dan dapat dilakukan di semua cabang Bundesbank di Jerman. Uang kertas bahkan dapat dikirim ke bank melalui pos.[1]']\n",
      "------------------------------------------------------------\n",
      "- question_stem: 'apa nama ibukota jepang' | n_rows=2, n_titles=1, n_passages=2\n",
      "  titles_sample: ['Ibu kota Jepang']\n",
      "  passages_sample: ['Tokyo diperlakukan secara de facto sebagai ibu kota karena menurut Konstitusi Jepang, Kaisar Jepang sebagai \"lambang negara Jepang dan simbol pemersatu rakyat Jepang\" dan istana kaisar berkedudukan di Tokyo. Selain itu, lembaga-lembaga pemerintah seperti Parlemen Jepang, Kantor Perdana Menteri (Kantei) dan Mahkamah Agung Jepang yang ditetapkan konstitusi sebagai \"lembaga tertinggi negara\" berada di distrik Chiyoda, Tokyo.']\n",
      "------------------------------------------------------------\n",
      "- question_stem: 'apa nama pulau tengah danau toba' | n_rows=2, n_titles=1, n_passages=2\n",
      "  titles_sample: ['Danau Toba']\n",
      "  passages_sample: ['Kejadian ini menyebabkan kematian massal dan kepunahan pada beberapa spesies makhluk hidup. Menurut beberapa bukti DNA, letusan ini juga menyusutkan jumlah manusia sampai sekitar 60% dari jumlah populasi manusia bumi saat itu, yaitu sekitar 60 juta manusia. Letusan itu juga ikut menyebabkan terjadinya zaman es, walaupun para ahli masih memperdebatkannya. Setelah letusan tersebut, terbentuk kaldera yang kemudian terisi oleh air dan menjadi yang sekarang dikenal sebagai Danau Toba. Tekanan ke atas oleh magma yang belum keluar menyebabkan munculnya Pulau Samosir.\\n\\n\\nTim peneliti multidisiplin internasional, yang dipimpin oleh Dr. Michael Petraglia, mengungkapkan dalam suatu konferensi pers di Oxford, Amerika Serikat bahwa telah ditemukan situs arkeologi baru yang cukup spektakuler oleh para ahli geologi di selatan dan utara India. Di situs itu terungkap bagaimana orang bertahan hidup, sebelum dan sesudah letusan gunung berapi (supervolcano) Toba pada 74.000 tahun yang lalu, dan bukti tentang adanya kehidupan di bawah timbunan abu Gunung Toba. Padahal sumber letusan berjarak 3.000 mil, dari sebaran abunya.']\n",
      "------------------------------------------------------------\n",
      "- question_stem: 'apa nama video game pertama di dunia' | n_rows=2, n_titles=1, n_passages=2\n",
      "  titles_sample: ['OXO']\n",
      "  passages_sample: ['OXO memiliki gambar seperti papan skor Olimpiade atau \"pod\". Ada keraguan OXO adalah permainan video pertama, yang lebih dulu terbit daripada permainan Tennis For Two yang terbit tahun 1958']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # pastikan kolom yang dibutuhkan ada\n",
    "# required_cols = {\"question_stem\", \"document_title\", \"passage_text\"}\n",
    "# missing = required_cols - set(tydiqa_gold_all_dupe_wo_bad_labels.column_names)\n",
    "# assert not missing, f\"Kolom hilang: {missing}\"\n",
    "\n",
    "# # ke pandas untuk analisis cepat\n",
    "# df = tydiqa_gold_all_dupe_wo_bad_labels.to_pandas()\n",
    "\n",
    "# # ambil hanya question_stem yang duplikat (>1 kemunculan)\n",
    "# dup_keys = (\n",
    "#     df.groupby(\"question_stem\", dropna=False).size()\n",
    "#       .reset_index(name=\"n\")\n",
    "#       .query(\"n > 1\")[\"question_stem\"]\n",
    "# )\n",
    "\n",
    "# dup_df = df[df[\"question_stem\"].isin(dup_keys)].copy()\n",
    "\n",
    "# # ringkasan per question_stem\n",
    "# def _unique_list(s, k=3):\n",
    "#     # ambil contoh unik maksimal k item untuk pratinjau\n",
    "#     vals = pd.unique(s.dropna())\n",
    "#     return list(vals[:k])\n",
    "\n",
    "# summary = (\n",
    "#     dup_df.groupby(\"question_stem\", dropna=False)\n",
    "#           .agg(\n",
    "#               n_rows=(\"question_stem\", \"size\"),\n",
    "#               n_titles=(\"document_title\", pd.Series.nunique),\n",
    "#               n_passages=(\"passage_text\", pd.Series.nunique),\n",
    "#               titles_sample=(\"document_title\", lambda s: _unique_list(s, k=3)),\n",
    "#               passages_sample=(\"passage_text\", lambda s: _unique_list(s, k=1)),\n",
    "#           )\n",
    "#           .reset_index()\n",
    "# )\n",
    "\n",
    "# summary[\"same_title\"] = summary[\"n_titles\"].eq(1)\n",
    "# summary[\"same_passage\"] = summary[\"n_passages\"].eq(1)\n",
    "# summary[\"all_same\"] = summary[\"same_title\"] & summary[\"same_passage\"]\n",
    "\n",
    "# # baris yang tidak konsisten (judul atau passage berbeda)\n",
    "# conflicts = summary.query(\"~all_same\").copy()\n",
    "\n",
    "# print(f\"Total question_stem duplikat: {len(summary)}\")\n",
    "# print(f\"Grup konsisten (judul & passage sama semua): {summary['all_same'].sum()}\")\n",
    "# print(f\"Grup konflik (judul/passage berbeda): {len(conflicts)}\")\n",
    "\n",
    "# # contoh menampilkan beberapa konflik\n",
    "# if len(conflicts):\n",
    "#     print(\"\\n=== Contoh konflik (maks 5) ===\")\n",
    "#     for _, r in conflicts.head(5).iterrows():\n",
    "#         print(f\"- question_stem: {r['question_stem']!r} | n_rows={r['n_rows']}, \"\n",
    "#               f\"n_titles={r['n_titles']}, n_passages={r['n_passages']}\")\n",
    "#         print(f\"  titles_sample: {r['titles_sample']}\")\n",
    "#         print(f\"  passages_sample: {r['passages_sample']}\")\n",
    "#         print(\"-\"*60)\n",
    "\n",
    "# # (Opsional) simpan hasil\n",
    "# summary.to_excel(\"tydiqa_gold_dupe_groups_summary.xlsx\", index=False)\n",
    "# conflicts.to_excel(\"tydiqa_gold_dupe_groups_conflicts.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563ec15",
   "metadata": {},
   "source": [
    "Penghapusan baris berdasarkan question_stem dan passage_text/answer yang salah. Baris berikut sudah diketahui query nya terduplikasi. Berhubung jawabannya pun salah, sekalian aja hapus dulu, biar nanti pas proses penghapusan query ga sengaja meninggalkan baris dengan answer yang salah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b22d519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6219/6219 [00:00<00:00, 6310.40 examples/s]\n",
      "Filter: 100%|██████████| 6219/6219 [00:00<00:00, 34951.55 examples/s]\n",
      "Filter: 100%|██████████| 6219/6219 [00:00<00:00, 35384.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before: 6219 | after: 6213 | removed: 6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ====== daftar pasangan ======\n",
    "Q_ANSWER_PAIRS = [\n",
    "    (\"apa yang maksud dengan uskup\",\n",
    "     \"pimpinan Gereja setempat yang bernama Keuskupan dan merupakan bagian dari hierarki Gereja Katolik Roma setelah Sri Paus (Uskup Agung Roma) dan Kardinal\"),\n",
    "    (\"berapa lama kaisar romawi kuasa\", \"27 SM sampai 284 M\"),\n",
    "    (\"berapa luas korea selatan\", \"99.274km2\"),\n",
    "    (\"berapa luas negara jepang\", \"377.835km²\"),\n",
    "]\n",
    "\n",
    "Q_PSGTEXT_STARTSWITH_PAIRS = [\n",
    "    (\"kapan internet masuk ke indonesia pertama kali\",\n",
    "     \"Sejarah internet Indonesia dimulai pada awal tahun 1990-an.\"),\n",
    "    (\"kapan komputer pertama kali cipta\",\n",
    "     \"Sebelum Internet muncul, telah ada beberapa sistem komunikasi yang berbasis digital\"),\n",
    "]\n",
    "\n",
    "# Lookup cepat\n",
    "ANSWER_SET = set((q.strip(), a.strip()) for q, a in Q_ANSWER_PAIRS)\n",
    "PFX_LIST  = [(q.strip(), pfx.strip()) for q, pfx in Q_PSGTEXT_STARTSWITH_PAIRS]\n",
    "\n",
    "def _iter_answers(val):\n",
    "    \"\"\"Yield jawaban sebagai string-strip, terlepas val= str / list / tuple / np.ndarray / None.\"\"\"\n",
    "    if val is None:\n",
    "        return\n",
    "    if isinstance(val, (list, tuple, np.ndarray)):\n",
    "        for x in val:\n",
    "            if x is None:\n",
    "                continue\n",
    "            yield str(x).strip()\n",
    "    else:\n",
    "        yield str(val).strip()\n",
    "\n",
    "def _should_drop(example):\n",
    "    qstem = (example.get(\"question_stem\") or \"\").strip()\n",
    "    psg   = (example.get(\"passage_text\") or \"\").strip()\n",
    "\n",
    "    # Rule 1: (question_stem, answer) exact match\n",
    "    for a in _iter_answers(example.get(\"answer\")):\n",
    "        if (qstem, a) in ANSWER_SET:\n",
    "            return True\n",
    "\n",
    "    # Rule 2: (question_stem, passage_text startswith prefix)\n",
    "    for q, pfx in PFX_LIST:\n",
    "        if qstem == q and psg.startswith(pfx):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# === Flag → filter ===\n",
    "before_n = len(tydiqa_gold_all_dupe_wo_bad_labels)\n",
    "\n",
    "flagged = tydiqa_gold_all_dupe_wo_bad_labels.map(\n",
    "    lambda ex: {\"_drop\": _should_drop(ex)}, batched=False\n",
    ")\n",
    "\n",
    "removed_rows = flagged.filter(lambda ex: ex[\"_drop\"])\n",
    "kept         = flagged.filter(lambda ex: not ex[\"_drop\"]).remove_columns([\"_drop\"])\n",
    "\n",
    "after_n   = len(kept)\n",
    "removed_n = before_n - after_n\n",
    "print(f\"Rows before: {before_n} | after: {after_n} | removed: {removed_n}\")\n",
    "\n",
    "# Commit hasil bersih kembali ke variabel asal (kalau diinginkan)\n",
    "tydiqa_gold_all_dupe_wo_bad_labels = kept\n",
    "\n",
    "# (Opsional) simpan audit baris yang dihapus\n",
    "# removed_rows.to_csv(\"tydiqa_removed_rows.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab06fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 6213/6213 [00:00<00:00, 18761.51 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris yang miss: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tydi_miss = tydiqa_gold_all_dupe_wo_bad_labels.filter(passage_miss_answer)\n",
    "\n",
    "print(f\"Jumlah baris yang miss: {len(tydi_miss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c17cf",
   "metadata": {},
   "source": [
    "Penghapusan duplikasi dengan menyisakan passage_text yang lebih pendek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a03d8ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 6213 | After: 5971 | Removed: 242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document_title', 'passage_text', 'question', 'question_norm', 'question_stem', 'answer'],\n",
       "    num_rows: 5971\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# convert ke pandas\n",
    "df = tydiqa_gold_all_dupe_wo_bad_labels.to_pandas()\n",
    "\n",
    "# hitung panjang passage_text (kalau NaN → inf supaya tidak kepilih)\n",
    "df[\"_passage_len\"] = df[\"passage_text\"].fillna(\"\").str.len()\n",
    "\n",
    "# untuk tiap question_stem, ambil index baris dengan passage_text terpendek\n",
    "idx_min = df.groupby(\"question_stem\")[\"_passage_len\"].idxmin()\n",
    "\n",
    "# ambil hanya baris-baris itu\n",
    "df_kept = df.loc[idx_min].reset_index(drop=True)\n",
    "\n",
    "print(f\"Before: {len(df)} | After: {len(df_kept)} | Removed: {len(df) - len(df_kept)}\")\n",
    "\n",
    "# buang kolom bantu\n",
    "df_kept = df_kept.drop(columns=[\"_passage_len\"])\n",
    "\n",
    "# convert balik ke HuggingFace Dataset\n",
    "tydiqa_gold_all_dupe_wo_bad_labels = Dataset.from_pandas(df_kept)\n",
    "tydiqa_gold_all_dupe_wo_bad_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d82ea09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5971/5971 [00:00<00:00, 48639.03 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris yang miss: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tydi_miss = tydiqa_gold_all_dupe_wo_bad_labels.filter(passage_miss_answer)\n",
    "\n",
    "print(f\"Jumlah baris yang miss: {len(tydi_miss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d177bc9c",
   "metadata": {},
   "source": [
    "Isi text dari split dev dan test mr_tydi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac80b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_dataset(\"castorini/mr-tydi-corpus\", \"indonesian\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db38c22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query_id': '3', 'query': 'Dimana James Hepburn meninggal?', 'positive_passages': [{'docid': '2386357#15', 'text': 'Dia dipenjarakan di Puri Dragsholm, 75 kilometer Kopenhagen. Dia ditahan dalam apa yang dikatakan sebagai kondisi yang mengerikan. Dia meninggal pada bulan April 1578.[8][10]', 'title': 'James Hepburn'}], 'negative_passages': []}\n",
      "{'query_id': '6148', 'query': 'Siapakah yang menemuka benua Amerika ?', 'positive_passages': [{'docid': '6874#1', 'text': \"Kolumbus bukanlah orang pertama yang tiba di Amerika, yang ia dapati sudah diduduki. Ia juga bukan orang Eropa pertama yang sampai ke benua itu karena sekarang telah diakui secara meluas bahwa orang-orang Viking dari Eropa Utara telah berkunjung ke Amerika Utara pada abad ke 11 dan mendirikan koloni L'Anse aux Meadows untuk jangka waktu singkat. Terdapat perkiraan bahwa pelayar yang tidak dikenali pernah melawat ke Amerika sebelum Kolumbus dan membekalkannya dengan sumber untuk kejayaannya. Terdapat juga banyak teori mengenai ekspedisi ke Amerika oleh berbagai orang sepanjang masa itu.\", 'title': 'Kristoforus Kolumbus'}], 'negative_passages': []}\n"
     ]
    }
   ],
   "source": [
    "mr_tydi = load_dataset(\"castorini/mr-tydi\", \"indonesian\")\n",
    "\n",
    "# Buat dictionary {docid: (title, text)} untuk pencarian cepat\n",
    "corpus_dict = {row[\"docid\"]: (row[\"title\"], row[\"text\"]) for row in corpus[\"train\"]}\n",
    "\n",
    "# Fungsi untuk melengkapi positive_passages dalam dataset mr_tydi\n",
    "def fill_passage_info(example):\n",
    "    for passage in example[\"positive_passages\"]:\n",
    "        docid = passage[\"docid\"]\n",
    "        if docid in corpus_dict:  # Cek apakah docid ada di corpus\n",
    "            passage[\"title\"], passage[\"text\"] = corpus_dict[docid]\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Terapkan fungsi untuk melengkapi positive_passages di split 'dev' dan 'test'\n",
    "mr_tydi[\"dev\"] = mr_tydi[\"dev\"].map(fill_passage_info)\n",
    "mr_tydi[\"test\"] = mr_tydi[\"test\"].map(fill_passage_info)\n",
    "\n",
    "# Cek hasilnya\n",
    "print(mr_tydi[\"dev\"][0])\n",
    "print(mr_tydi[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4dbe07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan semua split di mr_tydi\n",
    "mr_tydi_all = concatenate_datasets([mr_tydi[split] for split in mr_tydi.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708db2ac",
   "metadata": {},
   "source": [
    "Normalisasi dan stemming query mr_tydi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60676053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6955/6955 [01:09<00:00, 99.85 examples/s] \n"
     ]
    }
   ],
   "source": [
    "mr_tydi_all = mr_tydi_all.map(\n",
    "    add_norm_and_stem, \n",
    "    batched=True, \n",
    "    fn_kwargs={\"column\": \"query\"}     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae89066",
   "metadata": {},
   "source": [
    "Join kedua dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e615ea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5971/5971 [00:05<00:00, 1063.87 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query-norm matched: 5955/5971\n",
      "Positive-passage fully matched (title+text): 5580/5971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Bangun index dari mr_tydi_all: query_norm -> list kandidat mr\n",
    "mr_index = {}  # query_norm -> [ {query_id, positive_passages_raw, negative_passages_raw, positives_norm:[...]} ]\n",
    "for qnorm, qid, pos_list, neg_list in zip(\n",
    "    mr_tydi_all[\"query_norm\"],\n",
    "    mr_tydi_all[\"query_id\"],\n",
    "    mr_tydi_all[\"positive_passages\"],\n",
    "    mr_tydi_all[\"negative_passages\"],\n",
    "):\n",
    "    rec = {\n",
    "        \"query_id\": qid,\n",
    "        \"positive_passages_raw\": pos_list or [],\n",
    "        \"negative_passages_raw\": neg_list or [],\n",
    "        \"positives_norm\": []\n",
    "    }\n",
    "    # siapkan versi ternormalisasi utk pencocokan cepat\n",
    "    for p in (pos_list or []):\n",
    "        title = (p.get(\"title\") or \"\")\n",
    "        text  = (p.get(\"text\")  or \"\")\n",
    "        rec[\"positives_norm\"].append({\n",
    "            \"docid\": p.get(\"docid\"),\n",
    "            \"title\": title,\n",
    "            \"title_norm\": normalize(title),\n",
    "            \"text\": text,\n",
    "            \"text_norm\": normalize(text),\n",
    "        })\n",
    "    mr_index.setdefault(qnorm, []).append(rec)\n",
    "\n",
    "# 2) Map ke tiap baris tydiqa: tambahkan kolom hasil join\n",
    "def attach_mr_match(example):\n",
    "    qnorm = example.get(\"question_norm\") or \"\"\n",
    "    cands = mr_index.get(qnorm, [])\n",
    "\n",
    "    title_norm = normalize(example.get(\"document_title\") or \"\")\n",
    "    psg_norm   = normalize(example.get(\"passage_text\")   or \"\")\n",
    "\n",
    "    # default (tidak ketemu query_norm)\n",
    "    if not cands:\n",
    "        return {\n",
    "            \"mr_query_id\": None,\n",
    "            \"mr_docid\": None,\n",
    "            \"mr_title\": None,\n",
    "            \"mr_text\": None,\n",
    "            \"mr_negative_passages\": [],   # <- kolom yang kamu minta ikut dibawa\n",
    "            \"mr_query_norm_match\": False, # hanya query_norm?\n",
    "            \"mr_pos_match\": False,        # apakah positive_passages (title+text) match?\n",
    "        }\n",
    "\n",
    "    # ada kandidat dengan query_norm sama\n",
    "    # coba cari positive yang match (title & text)\n",
    "    for cand in cands:\n",
    "        for pos in cand[\"positives_norm\"]:\n",
    "            if pos[\"title_norm\"] == title_norm and pos[\"text_norm\"] == psg_norm:\n",
    "                return {\n",
    "                    \"mr_query_id\": cand[\"query_id\"],\n",
    "                    \"mr_docid\": pos[\"docid\"],\n",
    "                    \"mr_title\": pos[\"title\"],\n",
    "                    \"mr_text\": pos[\"text\"],\n",
    "                    \"mr_negative_passages\": cand[\"negative_passages_raw\"],  # ikutkan apa adanya\n",
    "                    \"mr_query_norm_match\": True,\n",
    "                    \"mr_pos_match\": True,\n",
    "                }\n",
    "\n",
    "    # tidak ada positive yang match; tetap pilih kandidat pertama agar negative_passages tetap terisi\n",
    "    cand0 = cands[0]\n",
    "    return {\n",
    "        \"mr_query_id\": cand0[\"query_id\"],\n",
    "        \"mr_docid\": None,\n",
    "        \"mr_title\": None,\n",
    "        \"mr_text\": None,\n",
    "        \"mr_negative_passages\": cand0[\"negative_passages_raw\"],\n",
    "        \"mr_query_norm_match\": True,\n",
    "        \"mr_pos_match\": False,\n",
    "    }\n",
    "\n",
    "# 3) Terapkan ke dataset tydiqa (tanpa menduplikasi baris)\n",
    "tydiqa_with_mr = tydiqa_gold_all_dupe_wo_bad_labels.map(\n",
    "    attach_mr_match,\n",
    "    batched=False,\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "# (opsional) ringkas hasil\n",
    "total = len(tydiqa_with_mr)\n",
    "qnorm_hits = sum(1 for b in tydiqa_with_mr[\"mr_query_norm_match\"] if b)\n",
    "pos_hits   = sum(1 for b in tydiqa_with_mr[\"mr_pos_match\"] if b)\n",
    "print(f\"Query-norm matched: {qnorm_hits}/{total}\")\n",
    "print(f\"Positive-passage fully matched (title+text): {pos_hits}/{total}\")\n",
    "\n",
    "# hasil akhirnya: tydiqa_with_mr punya kolom tambahan:\n",
    "# ['mr_query_id','mr_docid','mr_title','mr_text','mr_negative_passages','mr_query_norm_match','mr_pos_match']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e53be28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5971/5971 [00:01<00:00, 3498.30 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris yang miss: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tydi_miss = tydiqa_with_mr.filter(passage_miss_answer)\n",
    "\n",
    "print(f\"Jumlah baris yang miss: {len(tydi_miss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb9d1d",
   "metadata": {},
   "source": [
    "Buang baris yang tidak nemu pasangan di mr_tydi_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1be2cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5971/5971 [00:01<00:00, 3504.08 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah orphan: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5971/5971 [00:01<00:00, 3411.75 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sekarang tydiqa_with_mr berisi 5955 baris (orphan sudah dihapus).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) ambil orphan (tidak ada pasangan query_norm di mr_tydi_all)\n",
    "orphans = tydiqa_with_mr.filter(lambda ex: not ex[\"mr_query_norm_match\"])\n",
    "print(f\"Jumlah orphan: {len(orphans)}\")\n",
    "\n",
    "# # convert ke pandas biar bisa dicek/ekspor\n",
    "# orphans_df = orphans.to_pandas()\n",
    "# orphans_df.to_excel(\"tydiqa_orphans.xlsx\", index=False)\n",
    "# print(\"File tydiqa_orphans.xlsx berhasil dibuat.\")\n",
    "\n",
    "# 2) buang orphan dari dataset utama\n",
    "tydiqa_with_mr = tydiqa_with_mr.filter(lambda ex: ex[\"mr_query_norm_match\"])\n",
    "print(f\"Sekarang tydiqa_with_mr berisi {len(tydiqa_with_mr)} baris (orphan sudah dihapus).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b9e578",
   "metadata": {},
   "source": [
    "Inspeksi lebih dalam terkait teks passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2795f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# pola 'jmpl|' toleran spasi & case-insensitive\n",
    "JMPL_RE = re.compile(r\"jmpl\\s*\\|\", flags=re.IGNORECASE)\n",
    "\n",
    "# 1) ambil baris yang mengandung 'jmpl|'\n",
    "def has_jmpl(example):\n",
    "    txt = example.get(\"passage_text\") or \"\"\n",
    "    return bool(JMPL_RE.search(txt))\n",
    "\n",
    "jmpl_ds = tydiqa_with_mr.filter(has_jmpl)\n",
    "print(f\"Baris yang mengandung 'jmpl|': {len(jmpl_ds)}\")\n",
    "\n",
    "# 2) tampilkan sampai 5 contoh (ke layar)\n",
    "n_show = min(5, len(jmpl_ds))\n",
    "if n_show > 0:\n",
    "    print(\"\\n=== Contoh passage dengan 'jmpl|' (maks 5) ===\")\n",
    "    for r in jmpl_ds.select(range(n_show)):\n",
    "        print(f\"id: {r.get('id')}\")\n",
    "        print(f\"question_stem: {r.get('question_stem')}\")\n",
    "        print(f\"document_title: {r.get('document_title')}\")\n",
    "        # tampilkan potongan teks agar mudah dibaca\n",
    "        psg = (r.get('passage_text') or \"\").replace(\"\\n\", \" \")\n",
    "        print(\"passage_text (snippet):\", psg[:400], \"...\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"Tidak ada 'jmpl|' yang ditemukan. Ambil 5 sampel acak untuk inspeksi manual.\")\n",
    "    n_show = min(5, len(tydiqa_with_mr))\n",
    "    for r in tydiqa_with_mr.shuffle(seed=42).select(range(n_show)):\n",
    "        print(f\"id: {r.get('id')}\")\n",
    "        print(f\"question_stem: {r.get('question_stem')}\")\n",
    "        print(f\"document_title: {r.get('document_title')}\")\n",
    "        psg = (r.get('passage_text') or \"\").replace(\"\\n\", \" \")\n",
    "        print(\"passage_text (snippet):\", psg[:400], \"...\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# 3) simpan ke Excel untuk inspeksi lebih nyaman\n",
    "#    - jika ada 'jmpl|', simpan contoh baris tersebut (maks 200 baris agar ringan)\n",
    "#    - kalau tidak ada, simpan 200 sampel acak\n",
    "if len(jmpl_ds) > 0:\n",
    "    out_df = jmpl_ds.select(range(min(200, len(jmpl_ds)))).to_pandas()\n",
    "    out_path = \"tydiqa_passages_with_jmpl.xlsx\"\n",
    "else:\n",
    "    out_df = tydiqa_with_mr.shuffle(seed=42).select(range(min(200, len(tydiqa_with_mr)))).to_pandas()\n",
    "    out_path = \"tydiqa_passages_sample.xlsx\"\n",
    "\n",
    "out_df.to_excel(out_path, index=False)\n",
    "print(f\"Disimpan ke: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649571b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5912/5912 [00:00<00:00, 23094.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sampai tahap ini, sudah dilakukan:\n",
    "1. Penggabungan split TyDi QA\n",
    "2. Normalisasi dan stemming question\n",
    "3. Penghapusan duplikasi\n",
    "\n",
    "4. Mengisi psg positif dari split dev dan test Mr. TyDi\n",
    "5. Penggabungan split Mr. TyDi\n",
    "6. Normalisasi dan stemming question Mr. TyDi\n",
    "\n",
    "7. Join TyDi QA dengan Mr. TyDi\n",
    "\"\"\"\n",
    "tydiqa_with_mr.save_to_disk(\"tydiqa_with_mr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81865f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document_title', 'passage_text', 'query', 'question_norm', 'question_stem', 'answer', 'mr_query_id', 'mr_docid', 'mr_title', 'mr_text', 'mr_negative_passages', 'mr_query_norm_match', 'mr_pos_match'],\n",
       "    num_rows: 5912\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "tydiqa_with_mr = load_from_disk(\"tydiqa_with_mr\")\n",
    "tydiqa_with_mr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da5f62f",
   "metadata": {},
   "source": [
    "memperbaiki pasangan query dan answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef9b86b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_update = [\n",
    "    {\n",
    "        \"q_to_find\": \"dimanakah Dr. Ernest François Eugène Douwes Dekker meninggal?\", \n",
    "        \"new_answer\" : \"TMP Cikutra, Bandung\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimanakah Kucing Pallas pertama kali ditemukan ?\", \n",
    "        \"new_answer\": \"Asia Tengah wilayah Mongolia, Cina dan Dataran Tinggi Tibet\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimanakah Joseph Patrick \\\"Joe\\\" Kennedy dilahirkan?\", \n",
    "        \"new_q\" : \"Kapan Joseph Patrick \\\"Joe\\\" Kennedy dilahirkan?\",\n",
    "        \"new_answer\" : \"1888\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimanakah produksi film pertama dilakukan ?\", \n",
    "        \"new_q\" : \"Kapan pemutaran film pertama dilakukan ?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimanakah Fiber optik pertama kali diciptakan?\", \n",
    "        \"new_q\" : \"Kapan film mulai diproduksi menggunakan videotape?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Muhammad bin ʿAbd al-Wahhāb dilahirkan?\", \n",
    "        \"new_answer\" : \"kampung Uyainah (Najd)\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimanakah balap sepeda professional Eropa pertama diadakan ?\", \n",
    "        \"new_q\" : \"Sejak kapan Tour of Flanders diadakan?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Benito Mussolini meninggal ?\", \n",
    "        \"new_answer\" : \"Giulino di Mezzegra\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimanakah kanon Muratori ditemukan?\", \n",
    "        \"new_q\" : \"Kapan Papirus 46 diperkirakan dibuat?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Friedrich Nietzsche meninggal?\", \n",
    "        \"new_q\" : \"Kapan Friedrich Nietzsche meninggal?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana al-Ikhshīd lahir ?\", \n",
    "        \"new_answer\" : \"Baghdad\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Pangeran Xia meninggal?\", \n",
    "        \"new_q\" : \"Pada tahun berapa Dou Jiande meninggal?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Robert I Estienne lahir?\", \n",
    "        \"new_q\" : \"Kapankah Robert I Estienne lahir?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Paus Yohanes XXIII meninggal?\", \n",
    "        \"new_q\" : \"Kapan Paus Yohanes XXIII meninggal?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimanakah Napoleon meninggal?\", \n",
    "        \"new_q\" : \"Tanggal berapa Napoleon meninggal?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Utsmani berdiri ?\", \n",
    "        \"new_answer\" : \"1299\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan CIA dibentuk ?\", \n",
    "        \"new_q\" : \"Apa itu CIA?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapankah Radio pertama kali diciptakan?\", \n",
    "        \"new_q\" : \"Siapa pencipta radio?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Mariah lahir ?\", \n",
    "        \"new_answer\" : \"27 Maret 1970\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Kekaisaran Romawi mengalami masa jaya?\", \n",
    "        \"new_answer\" : \"Dua abad pertama kekaisaran\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapankah provinsi Sulawesi Barat diresmikan?\", \n",
    "        \"new_q\" : \"Berapa luas provinsi Sulawesi Barat?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Gereja Asiria Timur didirikan?\", \n",
    "        \"new_q\" : \"Siapa yang mendirikan Gereja Asiria Timur?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapankah Windows XP pertama kali diciptakan?\", \n",
    "        \"new_q\" : \"kapankah Windows XP pertama kali dirilis?\",\n",
    "        \"new_answer\" : \"25 Oktober 2001\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan musik rock pertama kali masuk ke Indonesia?\", \n",
    "        \"new_q\" : \"Siapa yang dipercaya lebih dulu memperkenalkan musik beraliran rock?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Sayf Allāh al-Maslūl lahir ?\", \n",
    "        \"new_answer\" : \"585\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Katherine Anne lahir ?\", \n",
    "        \"new_q\" : \"Dimana Katie Couric lahir?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapankah ilmu kimia dipelajari oleh manusia?\", \n",
    "        \"new_q\" : \"Reaksi kimia apa yang sudah diketahui sejak dahulu kala?\",\n",
    "        \"new_answer\" : \"pembakaran, fermentasi, dan reduksi dari bijih menjadi logam\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapan Ernest Miller Hemingway pertama kali menulis novel?\", \n",
    "        \"new_q\" : \"Apa judul buku pertama Ernest Hemingway yang diterbitkan?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Kapankah Kesultanan Utsmaniyah berdiri ?\", \n",
    "        \"new_answer\" : \"1299\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas wilayah kekuasan Romawi pada zaman dahulu ?\", \n",
    "        \"new_q\" : \"Kekaisaran Romawi menguasai daerah sekitar apa?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas kekuasaan Kerajaan Sriwijaya?\", \n",
    "        \"new_q\" : \"Wilayah mana saja yang dikuasai oleh Sriwijaya?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas kekuasaan kerajaan Banjar?\", \n",
    "        \"new_q\" : \"Apa nama kerajaan pertama di Kalimantan bagian selatan menurut mitologi\",\n",
    "        \"new_answer\" : \"Kerajaan Nan Sarunai\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"berapakah jumlah album Tantowi Yahya?\", \n",
    "        \"new_q\" : \"Apa album kedua Tantowi Yahya?\",\n",
    "        \"new_answer\" : \"Southern Dreams\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas kekuasaan dinasti Utsmaniyah ?\", \n",
    "        \"new_q\" : \"Kesultanan Utsmaniyah berkuasa atas wilayah luas di mana?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"berapakah jenis senjata mesin yang digunakan di PD II?\", \n",
    "        \"new_q\" : \"Senapan apa yang digunakan di banyak negara Persemakmuran Inggris pasca perang dunia II?\",\n",
    "        \"new_answer\" : \"Bren\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas daerah kekuasan Turki Seljuk ?\", \n",
    "        \"new_q\" : \"Sejauh mana luas daerah kekuasan Turki Seljuk ?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas kerajaan Banjar?\", \n",
    "        \"new_q\" : \"Sejauh mana luas kerajaan Banjar?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas wilayah kekaisaran Romawi?\", \n",
    "        \"new_q\" : \"Di wilayah mana saja Kekaisaran Romawi berkuasa?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa luas kekuasaan Kesultanan Utsmaniyah?\", \n",
    "        \"new_q\" : \"Di daerah mana kekuasaaan Kesultanan Utsmaniyah?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Siapakah yang mendirikan Sekolah Tinggi Agama Islam Balai Selasa?\", \n",
    "        \"new_answer\" : \"M. Dinar Moely dibantu Sekretaris Muchtar Komar dan Bendahara Ny. Martini\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Siapakah yang menemukan alarm pertama kali ?\", \n",
    "        \"new_passage_text\" : \"Alarm | Awalnya, penemuan baterai pada tahun 1799 dan telegraf pada tahun 1841 secara umum mengubah dunia dan menginspirasi para penyuka hobi, ahli listrik dan ilmuwan yang ada di seluruh dunia untuk melihat dan memperdalam ilmu komunikasi. Tidak lama setelah telegraf diperkenalkan, seorang dokter muda yang kaya bernama William Channing membuat sebuah sistem dari pemerintah untuk menyalurkan sinyal alarm kebakaran kepada stasiun pemadam kebakaran yang ada di sekeliling kota Boston, Amerika Serikat. Menggunakan morse yang ditemukan oleh Samuel Morse dalam sistem telegram yang memadukan kode dengan teknologi, Channing membuat rencana elaborasi untuk menyalurkan sinyal dari pusat sistem pemerintah menuju stasiun pemadam kebakaran untuk memberitahu titik lokasi terjadinya kebakaran. Rencana Channing memiliki masalah karena besar bunyi alarm tidak dapat dikendalikan dari stasiun pemadam kebakaran.\"\"\"\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Siapakah yang menemukan kamera?\", \n",
    "        \"new_q\" : \"Pada tahun berapa foto pertama di dunia dihasilkan?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apakah penyebab Uni Soviet dibubarkan?\", \n",
    "        \"new_answer\" : \"kebijakan glasnost dan perestroika, tetapi justru memicu perpecahan di Uni Soviet\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apakah penyebab utama kematian Fidiricu II?\", \n",
    "        \"new_q\" : \"Kapan wafatnya Fidiricu II?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Apakah penyebab utama Dinasti Ayyubiyyah runtuh?\", \n",
    "        \"new_q\" : \"Perjanjian antara Mamluk dan Ayyubiyah ditandatangani kapan?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Apakah kepanjangan dari DNA?\", \n",
    "        \"new_answer\" : \"deoxyribonucleic acid\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Di hari apakah Amstel Gold Race diadakan pertama kali?\",\n",
    "        \"new_q\" : \"Kapan Amstel Gold Race mulai tercatat dalam kalender resmi UCI?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Mengapa Kerajaan Joseon berakhir?\", \n",
    "        \"new_q\" : \"Kapan Kerajaan Joseon berakhir?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Dimana Gianfranco Chiarini belajar memasak?\", \n",
    "        \"new_answer\" : \"Instituto de Alta Gastronomia de Caracas\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Apa penyebab utama runtuhnya kekaisaran Bizantium ?\", \n",
    "        \"new_answer\" : \"Perang Salib Keempat tahun 1204, ketika kekaisaran ini dibubarkan secara paksa\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Apakah film pertama Alexa Ellesse Vega?\", \n",
    "        \"new_answer\" : \"Twister\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapankah Bebop mulai dipopulerkan?\", \n",
    "        \"new_q\" : \"Kapan genre musik hip hop mulai muncul di Amerika Serikat?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Apa definisi dari endokrinolog?\", \n",
    "        \"new_q\" : \"Apa definisi dari endokrinologi?\",\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"Berapa berat Mesin kepung?\", \n",
    "        \"new_q\" : \"Hingga berapa berat mesin kepung besar\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb745fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5955/5955 [00:01<00:00, 5112.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baris yang diupdate: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def apply_rows_updates(\n",
    "    ds,\n",
    "    rows_to_update: List[Dict[str, Any]],\n",
    "    lowercase_norm: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Terapkan banyak update berdasarkan q_to_find (dicocokkan via question_norm).\n",
    "    Setiap item bisa berisi beberapa field opsional:\n",
    "      {\n",
    "        \"q_to_find\": str,               # WAJIB (akan dinormalisasi)\n",
    "        \"new_q\": Optional[str],         # ubah query (+ regen question_norm & question_stem)\n",
    "        \"new_answer\": Optional[str|list[str]],  # ubah kolom 'answer' (list[str]); sinkronkan answers['text'] jika ada\n",
    "        \"new_passage_text\": Optional[str],      # ubah passage_text\n",
    "      }\n",
    "    \"\"\"\n",
    "    # Precompute mapping: find_norm -> payload perubahan\n",
    "    fix_map = {}\n",
    "    for row in rows_to_update:\n",
    "        q_raw = row[\"q_to_find\"]\n",
    "        find_norm = normalize(q_raw, lowercase=lowercase_norm)\n",
    "        payload = {}\n",
    "\n",
    "        # new_q\n",
    "        if \"new_q\" in row and row[\"new_q\"] is not None:\n",
    "            new_q = str(row[\"new_q\"])\n",
    "            new_norm = normalize(new_q, lowercase=lowercase_norm)\n",
    "            new_stem = stemmer.stem(new_norm)\n",
    "            payload.update({\n",
    "                \"new_q\": new_q,\n",
    "                \"new_norm\": new_norm,\n",
    "                \"new_stem\": new_stem,\n",
    "            })\n",
    "\n",
    "        # new_answer\n",
    "        if \"new_answer\" in row and row[\"new_answer\"] is not None:\n",
    "            na = row[\"new_answer\"]\n",
    "            if isinstance(na, (list, tuple)):\n",
    "                new_answer = [str(x) for x in na]\n",
    "            else:\n",
    "                new_answer = [str(na)]\n",
    "            payload[\"new_answer\"] = new_answer\n",
    "\n",
    "        # new_passage_text\n",
    "        if \"new_passage_text\" in row and row[\"new_passage_text\"] is not None:\n",
    "            payload[\"new_passage_text\"] = str(row[\"new_passage_text\"])\n",
    "\n",
    "        # Gabungkan jika ada beberapa entry dengan q_to_find sama (yang terakhir override)\n",
    "        fix_map[find_norm] = payload\n",
    "\n",
    "    # Flag baris yang kena perubahan dan terapkan\n",
    "    def _apply(batch):\n",
    "        qnorms = batch[\"question_norm\"]\n",
    "        has_answer_col = \"answer\" in batch\n",
    "        has_answers_dict = \"answers\" in batch\n",
    "\n",
    "        # siapkan output default (copy)\n",
    "        out_query         = list(batch[\"question\"])\n",
    "        out_question_norm = list(batch[\"question_norm\"])\n",
    "        out_question_stem = list(batch[\"question_stem\"])\n",
    "        out_answer        = list(batch[\"answer\"]) if has_answer_col else None\n",
    "        out_answers_dict  = list(batch[\"answers\"]) if has_answers_dict else None\n",
    "        out_passage_text  = list(batch[\"passage_text\"]) if \"passage_text\" in batch else None\n",
    "\n",
    "        touched = [False] * len(qnorms)\n",
    "\n",
    "        for i, qn in enumerate(qnorms):\n",
    "            if qn not in fix_map:\n",
    "                continue\n",
    "            payload = fix_map[qn]\n",
    "            touched[i] = True\n",
    "\n",
    "            # update query (+norm+stem)\n",
    "            if \"new_q\" in payload:\n",
    "                out_query[i]         = payload[\"new_q\"]\n",
    "                out_question_norm[i] = payload[\"new_norm\"]\n",
    "                out_question_stem[i] = payload[\"new_stem\"]\n",
    "\n",
    "            # update answer (+sinkron answers['text'] jika ada)\n",
    "            if has_answer_col and \"new_answer\" in payload:\n",
    "                out_answer[i] = payload[\"new_answer\"]\n",
    "                if has_answers_dict:\n",
    "                    curr = dict(out_answers_dict[i]) if out_answers_dict[i] is not None else {}\n",
    "                    curr[\"text\"] = payload[\"new_answer\"]\n",
    "                    out_answers_dict[i] = curr\n",
    "\n",
    "            # update passage_text\n",
    "            if out_passage_text is not None and \"new_passage_text\" in payload:\n",
    "                out_passage_text[i] = payload[\"new_passage_text\"]\n",
    "\n",
    "        out = {\n",
    "            \"query\": out_query,\n",
    "            \"question_norm\": out_question_norm,\n",
    "            \"question_stem\": out_question_stem,\n",
    "        }\n",
    "        if has_answer_col:\n",
    "            out[\"answer\"] = out_answer\n",
    "        if has_answers_dict:\n",
    "            out[\"answers\"] = out_answers_dict\n",
    "        if out_passage_text is not None:\n",
    "            out[\"passage_text\"] = out_passage_text\n",
    "\n",
    "        # untuk ringkasan\n",
    "        out[\"_touched\"] = touched\n",
    "        return out\n",
    "\n",
    "    updated = ds.map(_apply, batched=True, load_from_cache_file=False)\n",
    "    touched_count = sum(1 for x in updated[\"_touched\"] if x)\n",
    "    print(f\"Baris yang diupdate: {touched_count}\")\n",
    "\n",
    "    if \"_touched\" in updated.column_names:\n",
    "        updated = updated.remove_columns([\"_touched\"])\n",
    "\n",
    "    return updated\n",
    "\n",
    "# Terapkan\n",
    "tydiqa_with_mr = apply_rows_updates(tydiqa_with_mr, rows_to_update, lowercase_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b34a3794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5955/5955 [00:01<00:00, 3107.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris yang miss: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tydi_miss = tydiqa_with_mr.filter(passage_miss_answer)\n",
    "\n",
    "print(f\"Jumlah baris yang miss: {len(tydi_miss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2b52bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5955/5955 [01:25<00:00, 69.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalize_fields(batch):\n",
    "    new_answers = []\n",
    "    new_passages = []\n",
    "    new_titles = []\n",
    "    new_neg_passages = []\n",
    "\n",
    "    for i in range(len(batch[\"answer\"])):\n",
    "        # ===== answers =====\n",
    "        import re\n",
    "\n",
    "        ans_list = batch[\"answer\"][i] or []\n",
    "        normed_ans = []\n",
    "        for a in ans_list:\n",
    "            if a is None:\n",
    "                continue\n",
    "            s = normalize(a, lowercase=False)\n",
    "            # hapus tanda kurung penutup di awal string\n",
    "            s = re.sub(r'^[\\]\\}\\)]+', '', s)\n",
    "            # hapus tanda kurung pembuka di akhir string\n",
    "            s = re.sub(r'[\\[\\{\\(]+$', '', s)\n",
    "            normed_ans.append(s)\n",
    "        new_answers.append(normed_ans)\n",
    "\n",
    "        # ===== passage_text =====\n",
    "        passage = batch.get(\"passage_text\", [None])[i]\n",
    "        new_passages.append(normalize(passage, lowercase=False) if passage else passage)\n",
    "\n",
    "        # ===== document_title =====\n",
    "        doc_title = batch.get(\"document_title\", [None])[i]\n",
    "        new_titles.append(normalize(doc_title, lowercase=False) if doc_title else doc_title)\n",
    "\n",
    "        # ===== mr_negative_passages (list of dict) =====\n",
    "        negs = batch.get(\"mr_negative_passages\", [None])[i]\n",
    "        if negs is None:\n",
    "            new_neg_passages.append(None)\n",
    "        else:\n",
    "            normed_negs = []\n",
    "            for d in negs:\n",
    "                new_d = dict(d)\n",
    "                if \"title\" in new_d and new_d[\"title\"] is not None:\n",
    "                    new_d[\"title\"] = normalize(new_d[\"title\"], lowercase=False)\n",
    "                if \"text\" in new_d and new_d[\"text\"] is not None:\n",
    "                    new_d[\"text\"] = normalize(new_d[\"text\"], lowercase=False)\n",
    "                normed_negs.append(new_d)\n",
    "            new_neg_passages.append(normed_negs)\n",
    "\n",
    "    out = {\n",
    "        \"answer\": new_answers,\n",
    "        \"passage_text\": new_passages,\n",
    "        \"document_title\": new_titles,\n",
    "        \"mr_negative_passages\": new_neg_passages,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "# terapkan ke dataset\n",
    "tydiqa = tydiqa_with_mr.map(\n",
    "    normalize_fields,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48bdee0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5955/5955 [00:01<00:00, 3134.76 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris yang miss: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tydi_miss = tydiqa.filter(passage_miss_answer)\n",
    "\n",
    "print(f\"Jumlah baris yang miss: {len(tydi_miss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1b139d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5955/5955 [00:01<00:00, 2996.12 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebelum: 5955\n",
      "Sesudah: 5952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def has_answer(example):\n",
    "    passage = example[\"passage_text\"]\n",
    "    answers = example[\"answer\"] or []\n",
    "    # True kalau ada minimal satu jawaban muncul di passage\n",
    "    return any(ans in passage for ans in answers)\n",
    "\n",
    "# filter dataset\n",
    "tydiqa_filtered = tydiqa.filter(has_answer)\n",
    "\n",
    "print(\"Sebelum:\", len(tydiqa))\n",
    "print(\"Sesudah:\", len(tydiqa_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f64496f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/5952 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5952/5952 [00:00<00:00, 22360.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tydiqa_filtered.save_to_disk(\"./generated_data/tydi_but_still_with_unchecked_multi_answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c8be35",
   "metadata": {},
   "source": [
    "cek apakah answer hanya satu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ee421c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/5952 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5952/5952 [00:02<00:00, 2865.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total baris asli   : 5955\n",
      "Total baris subset : 559\n",
      "['ke-12 hingga abad ke-19', 'ke-12 hingga abad ke-19', 'abad ke-12 hingga abad ke-19']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# subset hanya baris dengan jumlah elemen answer > 1\n",
    "tydiqa_multi_answer = tydiqa_filtered.filter(lambda ex: len(ex[\"answer\"]) > 1)\n",
    "\n",
    "print(f\"Total baris asli   : {len(tydiqa_with_mr)}\")\n",
    "print(f\"Total baris subset : {len(tydiqa_multi_answer)}\")\n",
    "\n",
    "# cek contoh\n",
    "print(tydiqa_multi_answer[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 558/558 [00:00<00:00, 6625.17 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total multi-answer: 558\n",
      "Total konflik     : 45\n",
      "['ilmu atau seni dalam menciptakan dan menghias lambang, beserta kajian tentang makna, asal-usul, sejarah, dan perkembangannya', 'suatu ilmu atau seni dalam menciptakan dan menghias lambang, beserta kajian tentang makna, asal-usul, sejarah, dan perkembangannya', 'panglima perang']\n",
      "['konsep Hindu, tetapi juga dipakai dalam konteks agama Buddha, untuk merujuk pada berbagai benda nyata', 'lingkaran', 'lingkaran']\n",
      "['avtur', 'aviation turbine fuel', 'jenis bahan bakar penerbangan yang dirancang untuk digunakan pada pesawat terbang yang bermesin turbin gas']\n",
      "['mata hitam, rambut hitam yang tak pernah memanjang dan memiliki ekor', 'gaya rambut yang secara tegap berdiri ke atas dengan ujung-ujung runcing']\n",
      "['pola permukiman dan ritualnya', 'Vinča']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def _simple_norm(s: str) -> str:\n",
    "    return (s or \"\").strip().lower()\n",
    "\n",
    "def is_conflict(example):\n",
    "    ans = example.get(\"answer\") or []\n",
    "    if len(ans) < 2:\n",
    "        return False\n",
    "\n",
    "    # normalisasi sederhana\n",
    "    norm = [_simple_norm(a) for a in ans if a is not None and _simple_norm(a) != \"\"]\n",
    "    if len(norm) < 2:\n",
    "        return False\n",
    "\n",
    "    # jika semua sama persis → bukan konflik\n",
    "    if len(set(norm)) == 1:\n",
    "        return False\n",
    "\n",
    "    # ambil yang terpendek\n",
    "    shortest = min(norm, key=len)\n",
    "\n",
    "    # cek apakah shortest subset dari elemen lain YANG BERBEDA\n",
    "    subset_of_other = any(\n",
    "        (shortest != other) and (shortest in other)\n",
    "        for other in norm\n",
    "    )\n",
    "\n",
    "    # konflik jika BUKAN subset dari elemen lain (dan kita sudah pastikan tidak semuanya identik)\n",
    "    return not subset_of_other\n",
    "\n",
    "# buat subset konflik baru\n",
    "tydiqa_multi_answer_conflict = tydiqa_multi_answer.filter(is_conflict)\n",
    "\n",
    "print(f\"Total multi-answer: {len(tydiqa_multi_answer)}\")\n",
    "print(f\"Total konflik     : {len(tydiqa_multi_answer_conflict)}\")\n",
    "\n",
    "# intip beberapa contoh\n",
    "for i in range(min(5, len(tydiqa_multi_answer_conflict))):\n",
    "    print(tydiqa_multi_answer_conflict[i][\"answer\"])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# konversi ke pandas DataFrame\n",
    "conflict_df = tydiqa_multi_answer_conflict.to_pandas()\n",
    "\n",
    "# simpan ke Excel\n",
    "conflict_df.to_excel(\"tydiqa_multi_answer_conflict.xlsx\", index=False)\n",
    "\n",
    "print(\"✅ File berhasil disimpan ke tydiqa_multi_answer_conflict.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe5fbd",
   "metadata": {},
   "source": [
    "Atur answer sebagai satu elemen saja, yakni string terpendek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b9a3044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5952/5952 [00:01<00:00, 3920.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selesai. Baris dengan answer akhir 1 elemen: 5952 dari 5952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Asumsi: fungsi normalize(q: str, lowercase: bool=True) sudah ada di sistem\n",
    "\n",
    "def _shortest_normalized_answer(ans_list, lowercase=False):\n",
    "    \"\"\"\n",
    "    Ambil elemen terpendek SETELAH dinormalisasi.\n",
    "    Tie-breaker: kalau ada beberapa dengan panjang sama, ambil yang pertama.\n",
    "    Kembalikan string normalized (bukan versi aslinya).\n",
    "    \"\"\"\n",
    "    if ans_list is None:\n",
    "        return None\n",
    "    if not isinstance(ans_list, (list, tuple)):\n",
    "        ans_list = [ans_list]\n",
    "\n",
    "    # Normalisasi tiap elemen, buang None/kosong\n",
    "    normed = [normalize(str(a), lowercase=lowercase) for a in ans_list if a is not None and str(a).strip() != \"\"]\n",
    "    if not normed:\n",
    "        return None\n",
    "    # pilih yang terpendek\n",
    "    shortest = min(normed, key=len)\n",
    "    return shortest\n",
    "\n",
    "def keep_shortest_answer(batch, lowercase_norm=False):\n",
    "    out_answer = []\n",
    "    out_answers_dict = None\n",
    "    has_answers_dict = \"answers\" in batch\n",
    "\n",
    "    if has_answers_dict:\n",
    "        out_answers_dict = []\n",
    "\n",
    "    for i in range(len(batch[\"answer\"])):\n",
    "        shortest = _shortest_normalized_answer(batch[\"answer\"][i], lowercase=lowercase_norm)\n",
    "        # simpan sebagai list (konsisten dengan skema)\n",
    "        new_list = [shortest] if shortest is not None else []\n",
    "\n",
    "        out_answer.append(new_list)\n",
    "\n",
    "        if has_answers_dict:\n",
    "            curr = dict(batch[\"answers\"][i]) if batch[\"answers\"][i] is not None else {}\n",
    "            curr[\"text\"] = new_list\n",
    "            out_answers_dict.append(curr)\n",
    "\n",
    "    out = {\"answer\": out_answer}\n",
    "    if has_answers_dict:\n",
    "        out[\"answers\"] = out_answers_dict\n",
    "    return out\n",
    "\n",
    "# Terapkan ke dataset\n",
    "tydiqa_one_answer = tydiqa_filtered.map(\n",
    "    keep_shortest_answer,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"lowercase_norm\": False},   # ganti ke False jika ingin pertahankan kapitalisasi\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "changed = sum(1 for a in tydiqa_one_answer[\"answer\"] if isinstance(a, list) and len(a) == 1)\n",
    "print(f\"Selesai. Baris dengan answer akhir 1 elemen: {changed} dari {len(tydiqa_one_answer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71e8f350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5952/5952 [00:01<00:00, 3164.18 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris yang miss: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tydi_miss = tydiqa_one_answer.filter(passage_miss_answer)\n",
    "\n",
    "print(f\"Jumlah baris yang miss: {len(tydi_miss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3e795c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File berhasil disimpan ke tydi_miss_answers.xlsx dengan 97 baris.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# konversi ke DataFrame\n",
    "df_miss = tydi_miss.to_pandas()\n",
    "\n",
    "# simpan ke Excel\n",
    "output_path = \"tydi_miss_answers.xlsx\"\n",
    "df_miss.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"File berhasil disimpan ke {output_path} dengan {len(df_miss)} baris.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd4fc03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5952/5952 [00:00<00:00, 31901.02 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baris yang diupdate: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rows_to_update_2 = [\n",
    "    {\n",
    "        \"q_to_find\": \"apa arti dari heraldik?\", \n",
    "        \"new_answer\" : ['suatu ilmu atau seni dalam menciptakan dan menghias lambang, beserta kajian tentang makna, asal-usul, sejarah, dan perkembangannya'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa arti kata mandala?\", \n",
    "        \"new_answer\" : ['konsep Hindu, tetapi juga dipakai dalam konteks agama Buddha, untuk merujuk pada berbagai benda nyata','lingkaran'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa bahan bakar mesin jet?\", \n",
    "        \"new_answer\" : ['avtur'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa ciri khas bangsa saiya?\", \n",
    "        \"new_answer\" : ['mata hitam, rambut hitam yang tak pernah memanjang dan memiliki ekor'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa ciri khas dari kebudayaan neolitik?\", \n",
    "        \"new_answer\" : ['pola permukiman dan ritualnya'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa pendidikan terakhir mochamad fadjroel rachman?\", \n",
    "        \"new_answer\" : ['Magister Hukum'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa isi surat deuteropauline?\", \n",
    "        \"new_answer\" : ['gagasan eskatologi','wujud kepastian iman yang menjadi suatu pengharapan dalam kehidupan dan bagaimana sikap orang terhadap parousia serta sikap iman terhadap parousia yang mempunyai arti bagi kehidupan saat ini'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa maksud istilah shengnü?\", \n",
    "        \"new_answer\" : ['ejekan yang dipopulerkan oleh Federasi Wanita Seluruh Tiongkok yang mengklasifikasikan wanita yang masih belum menikah pada usia akhir dua puluh tahunan atau lebih','wanita sisa'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa nama bandara di penang?\", \n",
    "        \"new_answer\" : ['Bandar Udara Internasional Penang', 'Bandar Udara Internasional Bayan Lepas'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa nama rumah adat suku banjar?\", \n",
    "        \"new_answer\" : ['Rumah Baanjung', 'Rumah Banjar'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa peran dewa wisnu dalam kepercayaan hindu?\", \n",
    "        \"new_answer\" : ['pemelihara'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa penyebab utama pengeboman pasar tentena 2005?\", \n",
    "        \"new_answer\" : ['konflik sektarian antara Muslim dan Kristen di Poso', 'upaya balas dendam atas kekejaman sebelumnya yang dilakukan terhadap komunitas Muslim di Poso'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa seni islam pertama di baitul qur'an?\", \n",
    "        \"new_answer\" : ['salinan pertama Alquran'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa spesies ubur-ubur yang terbesar?\", \n",
    "        \"new_answer\" : ['Cyanea capillata', 'Surai singa'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"apa perusahaan pertama tempat david mackenzie ogilvy bekerja?\", \n",
    "        \"new_answer\" : ['Majestic Hotel'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"bagaimana sistem pemerintahan pakistan?\", \n",
    "        \"new_answer\" : ['pemerintah federal', 'republik demokrasi parlementer'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"berapa luas smk negeri 1 cikampek?\", \n",
    "        \"new_answer\" : ['29095m²', '28997m²'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"di tim apakah marc márquez alenta mendapatkan gelar juara dunia kelas 125cc pertama kali?\", \n",
    "        \"new_answer\" : ['KTM'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan kapal pertama diciptakan?\", \n",
    "        \"new_answer\" : ['10.000 tahun yang lalu','masa Neolitikum'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan katrina kaif mulai menjadi model?\", \n",
    "        \"new_answer\" : ['usia empat belas'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan komputer mikro mulai dikembangkan?\", \n",
    "        \"new_answer\" : ['1971'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan need for speed: payback dirilis?\", \n",
    "        \"new_answer\" : ['10 November 2017'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan raja ataulf menikah?\", \n",
    "        \"new_answer\" : ['Januari 414'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan rudi garcia mulai menjadi manajer sepak bola?\", \n",
    "        \"new_answer\" : ['Antara tahun 1994 dan 1996'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"kapan teori hubungan internasional diciptakan?\", \n",
    "        \"new_answer\" : ['setelah Perang Dunia I', '1939'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana pelemparan di praha terjadi?\", \n",
    "        \"new_answer\" : ['Bohemia'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak aquitania?\", \n",
    "        \"new_answer\" : ['Perancis bagian barat daya', 'Pegunungan Pyrenees'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak balai kota dki jakarta?\", \n",
    "        \"new_answer\" : ['Jl. Medan Merdeka Selatan No. 8-9, Jakarta Pusat'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak cekungan tarim?\", \n",
    "        \"new_answer\" : ['Barat Laut China','Xinjiang China'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak daerah malta di eropa?\", \n",
    "        \"new_answer\" : ['Eropa Selatan'],\n",
    "    }, {\n",
    "        \"q_to_find\": \"dimana letak montenegro?\", \n",
    "        \"new_answer\" : ['Eropa Selatan'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak orléans?\", \n",
    "        \"new_answer\" : ['Amerika Serikat' ,'muara Sungai Mississippi'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak samudera pasifik?\", \n",
    "        \"new_answer\" : ['di antara Asia dan Australia di sebelah barat, Amerika di sebelah timur, Antartika di sebelah selatan dan Samudra Arktik di sebelah utara'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak timur dekat kuno?\", \n",
    "        \"new_answer\" : ['Timur Tengah'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana letak universitas leiden?\", \n",
    "        \"new_answer\" : ['Leiden','Belanda'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana studio pembuatan tom and jerry?\", \n",
    "        \"new_answer\" : ['Hollywood'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"dimana su dingfang lahir?\", \n",
    "        \"new_answer\" : ['Wuyi'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"siapa dewa wisnu menurut agama hindu?\", \n",
    "        \"new_answer\" : ['Dewa yang bergelar sebagai shtiti (pemelihara) yang bertugas memelihara dan melindungi segala ciptaan Brahman'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"siapa ketua perum lkbn pertama?\", \n",
    "        \"new_answer\" : ['Mr. Soemanang'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"siapa nama karakter antagonis utama di dalam avatar: the last airbender?\", \n",
    "        \"new_answer\" : ['Amon'],\n",
    "        \"new_passage_text\": 'Steve Blum sebagai Amon adalah pria bertopeng misterius yang memimpin kelompok Equalists, sebuah kelompok yang anti-Pengendali yang juga menggunakan Ketrampilan Chi-Blokir yang dipakai untuk menaklukkan musuh. Dalam \"Kitab Wahyu\" yang pernah ditulisnya, wajahnya memiliki luka yang disebabkan oleh Mafia Pengendali Api yang suka memeras dan pernah membunuh keluarganya yang kemudian menjadi sebab ia mendirikan kelompok Anti-Pengendali, tetapi sebenarnya cerita itu hanya bohong agar para equalistbisa mngikuti jalannya, Ia memiliki kemampuan untuk menghilangkan kemampuan Pengendalian seseorang. Kemampuan seperti ini telah dimiliki oleh Aang di Avatar serial sebelumnya di Episode terakhir. Dia adalah tokoh Antagonis utama dalam serial ini.'\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"siapa presiden russia pada tahun 2012?\", \n",
    "        \"new_answer\" : ['Vladimir Putin'],\n",
    "    }, \n",
    "    {\n",
    "        \"q_to_find\": \"siapa presiden wanita pertama di filipina?\", \n",
    "        \"new_answer\" : ['Maria Corazon Sumulong Cojuangco Aquino', 'Cory Aquino'],\n",
    "    },\n",
    "    {\n",
    "        \"q_to_find\": \"siapa puteri indonesia yang ke 18?\", \n",
    "        \"new_answer\" : ['Sonia Fergina Citra'],\n",
    "    },\n",
    "    {\n",
    "        \"q_to_find\": \"siapa yang mendirikan organisasi palang merah?\", \n",
    "        \"new_answer\" : ['Henry Dunant'],\n",
    "    },\n",
    "    {\n",
    "        \"q_to_find\": \"tahun berapa pertempuran shiffin terjadi?\", \n",
    "        \"new_answer\" : ['657 Masehi', '37 Hijriah'],\n",
    "    },\n",
    "]\n",
    "tydiqa_filtered_answer = apply_rows_updates(\n",
    "    tydiqa_one_answer,\n",
    "    rows_to_update=rows_to_update_2,\n",
    "    lowercase_norm=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bdccc4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5952/5952 [00:01<00:00, 3473.18 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris yang miss: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tydi_miss = tydiqa_filtered_answer.filter(passage_miss_answer)\n",
    "\n",
    "print(f\"Jumlah baris yang miss: {len(tydi_miss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0dbea7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5952/5952 [00:01<00:00, 3098.74 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebelum: 5952\n",
      "Sesudah: 5951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tydiqa_fin = tydiqa_filtered_answer.filter(has_answer)\n",
    "\n",
    "print(\"Sebelum:\", len(tydiqa_filtered_answer))\n",
    "print(\"Sesudah:\", len(tydiqa_fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7a562",
   "metadata": {},
   "source": [
    "Hapus kolom 'mr_query_norm_match' dan 'mr_pos_match'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tydiqa_fin = tydiqa_fin.remove_columns(['mr_query_norm_match', 'mr_pos_match', 'mr_title', 'mr_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "65eaf86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5951/5951 [00:00<00:00, 23513.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sampai tahap ini, sudah:\n",
    "1. Memperbaiki pertanyaan/jawaban yang salah (misal, pertanyaan \"dimana\" malah dijawab dengan tanggal)\n",
    "2. Kalau ada beberapa jawaban, dan jawaban terpendek tidak sama dengan jawaban yang lain, maka perlu ditandai mana jawaban yang benar (bisa multiple)\n",
    "3. Untuk kasus jawaban banyak tapi relatif sama, pertahankan jawaban terpendek dan buang jawaban yang lain\n",
    "4. Atur jawaban multiple yang benar secara manual\n",
    "5. Normalisasi passage positif dan jawaban\n",
    "\"\"\"\n",
    "tydiqa_fin.save_to_disk(\"./generated_data/tydiqa_fin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ec075",
   "metadata": {},
   "source": [
    "# Indoqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "indoqa = load_dataset(\"SEACrowd/indoqa\", trust_remote_code=True)\n",
    "indoqa = concatenate_datasets([indoqa[split] for split in indoqa.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20fefd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/4413 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 4413/4413 [00:00<00:00, 37497.09 examples/s]\n",
      "Map: 100%|██████████| 4253/4253 [00:00<00:00, 8047.86 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'answer', 'context', 'category', 'span_start', 'span_end', 'question_norm', 'question_stem'],\n",
       "    num_rows: 4253\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "indoqa = indoqa.filter(lambda x: x['category']=='SPAN')\n",
    "\n",
    "indoqa = indoqa.map(\n",
    "    add_norm_and_stem, \n",
    "    batched=True, \n",
    "    fn_kwargs={\"column\": \"question\"}   # sesuaikan dengan nama kolom di tydiqa_gold_all\n",
    ")\n",
    "indoqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff5a0c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebelum: 4253 baris\n",
      "Sesudah: 4247 baris (duplikat dihapus)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# konversi ke pandas\n",
    "df = indoqa.to_pandas()\n",
    "\n",
    "# tambahkan panjang context\n",
    "df[\"_context_len\"] = df[\"context\"].str.len()\n",
    "\n",
    "# urutkan biar idxmin konsisten (misal by context_len)\n",
    "df_sorted = df.sort_values(by=[\"question_stem\", \"_context_len\"])\n",
    "\n",
    "# ambil index dengan context terpendek untuk setiap question_stem\n",
    "idx_keep = df_sorted.groupby(\"question_stem\")[\"_context_len\"].idxmin()\n",
    "\n",
    "# subset baru\n",
    "df_dedup = df.loc[idx_keep].reset_index(drop=True)\n",
    "\n",
    "print(f\"Sebelum: {len(df)} baris\")\n",
    "print(f\"Sesudah: {len(df_dedup)} baris (duplikat dihapus)\")\n",
    "\n",
    "# drop kolom bantu\n",
    "df_dedup = df_dedup.drop(columns=[\"_context_len\"])\n",
    "\n",
    "# konversi balik ke HuggingFace Dataset\n",
    "indoqa_dedup = Dataset.from_pandas(df_dedup, preserve_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a189daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4247/4247 [00:01<00:00, 2331.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def normalize_fields(batch):\n",
    "    new_answers = []\n",
    "    new_passages = []\n",
    "\n",
    "    for i in range(len(batch[\"answer\"])):\n",
    "\n",
    "        answer = batch.get(\"answer\", [None])[i]\n",
    "        new_answers.append(normalize(answer, lowercase=False) if answer else answer)\n",
    "\n",
    "        # ===== passage_text =====\n",
    "        passage = batch.get(\"context\", [None])[i]\n",
    "        new_passages.append(normalize(passage, lowercase=False) if passage else passage)\n",
    "\n",
    "    out = {\n",
    "        \"answer\": new_answers,\n",
    "        \"context\": new_passages,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "indoqa_clean = indoqa_dedup.map(\n",
    "    normalize_fields,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43648be7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indoqa_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m         lengths\u001b[38;5;241m.\u001b[39mextend(lens)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(lengths, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m---> 22\u001b[0m lengths \u001b[38;5;241m=\u001b[39m token_lengths_for_column(indoqa_clean, col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 3) Statistik ringkas\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdescribe_lengths\u001b[39m(arr):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'indoqa_clean' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) Tokenizer Flan-T5\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# 2) Ambil panjang token kolom `context` dengan batching (lebih cepat)\n",
    "def token_lengths_for_column(ds, col=\"context\", batch_size=512):\n",
    "    lengths = []\n",
    "    n = len(ds)\n",
    "    for start in tqdm(range(0, n, batch_size), desc=f\"Tokenizing `{col}`\"):\n",
    "        end = min(start + batch_size, n)\n",
    "        batch_texts = [(ds[i].get(col) or \"\") for i in range(start, end)]\n",
    "        enc = t5_tokenizer(batch_texts, add_special_tokens=True, truncation=False)\n",
    "        # panjang = jumlah token per contoh\n",
    "        lens = [len(ids) for ids in enc[\"input_ids\"]]\n",
    "        lengths.extend(lens)\n",
    "    return np.array(lengths, dtype=np.int32)\n",
    "\n",
    "lengths = token_lengths_for_column(indoqa_clean, col=\"context\", batch_size=512)\n",
    "\n",
    "# 3) Statistik ringkas\n",
    "def describe_lengths(arr):\n",
    "    if arr.size == 0:\n",
    "        return {}\n",
    "    stats = {\n",
    "        \"count\": int(arr.size),\n",
    "        \"mean\": float(np.mean(arr)),\n",
    "        \"std\": float(np.std(arr)),\n",
    "        \"min\": int(np.min(arr)),\n",
    "        \"p50_median\": float(np.median(arr)),\n",
    "        \"p90\": float(np.percentile(arr, 90)),\n",
    "        \"p95\": float(np.percentile(arr, 95)),\n",
    "        \"p99\": float(np.percentile(arr, 99)),\n",
    "        \"max\": int(np.max(arr)),\n",
    "    }\n",
    "    # proporsi melewati beberapa ambang umum (opsional)\n",
    "    for thr in (256, 512, 600, 1024):\n",
    "        stats[f\"> {thr}\"] = float((arr > thr).mean() * 100.0)\n",
    "    return stats\n",
    "\n",
    "stats = describe_lengths(lengths)\n",
    "\n",
    "# 4) Cetak statistik\n",
    "print(\"=== Statistik panjang token `context` (Flan-T5) ===\")\n",
    "for k in [\"count\",\"mean\",\"std\",\"min\",\"p50_median\",\"p90\",\"p95\",\"p99\",\"max\",\"> 256\",\"> 512\",\"> 600\",\"> 1024\"]:\n",
    "    if k in stats:\n",
    "        if k.startswith(\"> \"):\n",
    "            print(f\"{k:>8}: {stats[k]:6.2f}%\")\n",
    "        elif k in {\"mean\",\"std\",\"p50_median\",\"p90\",\"p95\",\"p99\"}:\n",
    "            print(f\"{k:>12}: {stats[k]:.2f}\")\n",
    "        else:\n",
    "            print(f\"{k:>12}: {stats[k]}\")\n",
    "\n",
    "# 5) Visualisasi histogram\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(lengths, bins=60, edgecolor=\"black\")\n",
    "plt.title(\"Distribusi Panjang Token `context` (Flan-T5)\")\n",
    "plt.xlabel(\"Jumlah token\"); plt.ylabel(\"Frekuensi\")\n",
    "\n",
    "# Garis bantu mean/median/p95\n",
    "mean_v = np.mean(lengths) if lengths.size else 0\n",
    "med_v  = np.median(lengths) if lengths.size else 0\n",
    "p95_v  = np.percentile(lengths, 95) if lengths.size else 0\n",
    "plt.axvline(mean_v, linestyle=\"--\", label=f\"Mean={mean_v:.1f}\")\n",
    "plt.axvline(med_v,  linestyle=\":\",  label=f\"Median={med_v:.1f}\")\n",
    "plt.axvline(p95_v,  linestyle=\"-.\", label=f\"P95={p95_v:.1f}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ce27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indoqa_clean = indoqa_clean.remove_columns(['category', 'span_start', 'span_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d0d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'answer', 'passage_text', 'question_norm', 'question_stem'],\n",
       "    num_rows: 4247\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indoqa_clean=indoqa_clean.rename_column('context', 'passage_text')\n",
    "indoqa_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ec252",
   "metadata": {},
   "outputs": [],
   "source": [
    "indoqa_clean.save_to_disk(\"indoqa_finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb3cf8",
   "metadata": {},
   "source": [
    "# FacQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63985e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "facqa = load_dataset(\"SEACrowd/facqa\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ea729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'passage', 'seq_label', 'id'],\n",
      "        num_rows: 2495\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'passage', 'seq_label', 'id'],\n",
      "        num_rows: 311\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'passage', 'seq_label', 'id'],\n",
      "        num_rows: 311\n",
      "    })\n",
      "})\n",
      "{'question': ['Kelompok', 'apakah', 'yang', 'menyatakan', 'bertanggung', 'jawab', 'atas', 'ledakan', 'di', 'Srinagar', '?'], 'passage': ['Lewat', 'telepon', 'ke', 'kantor', 'berita', 'lokal', 'Current', 'News', 'Service', ',', 'Hezb-ul', 'Mujahedeen', ',', 'kelompok', 'militan', 'Kashmir', 'yang', 'terbesar', ',', 'menyatakan', 'bertanggung', 'jawab', 'atas', 'ledakan', 'di', 'Srinagar', '.'], 'seq_label': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'id': 'train_0'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load dataset\n",
    "facqa = load_dataset(\"SEACrowd/facqa\", trust_remote_code=True)\n",
    "\n",
    "# ubah kolom \"index\" -> \"id\" dan sesuaikan nilainya per split\n",
    "def rename_and_prefix(example, idx, split_name):\n",
    "    # gabungkan nama split + id lama\n",
    "    return {\"id\": f\"{split_name}_{str(example['index'])}\"}\n",
    "\n",
    "new_splits = {}\n",
    "for split_name, ds in facqa.items():\n",
    "    # map dengan tambahan index\n",
    "    ds_new = ds.map(\n",
    "        lambda ex, idx: rename_and_prefix(ex, idx, split_name),\n",
    "        with_indices=True\n",
    "    )\n",
    "    # hapus kolom \"index\" lama\n",
    "    ds_new = ds_new.remove_columns([\"index\"])\n",
    "    new_splits[split_name] = ds_new\n",
    "\n",
    "facqa = facqa.__class__(new_splits)\n",
    "\n",
    "print(facqa)\n",
    "print(facqa[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a5caff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'passage', 'seq_label', 'id'],\n",
       "    num_rows: 3117\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facqa = concatenate_datasets([facqa[split] for split in facqa.keys()])\n",
    "facqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a761a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'passage', 'seq_label', 'id', 'question_text', 'answer_text'],\n",
      "    num_rows: 3117\n",
      "})\n",
      "Kelompok apakah yang menyatakan bertanggung jawab atas ledakan di Srinagar ?\n",
      "['Hezb-ul Mujahedeen']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# fungsi ekstraksi jawaban dari BIO\n",
    "def extract_answer(tokens, labels):\n",
    "    answers = []\n",
    "    current = []\n",
    "    for tok, lab in zip(tokens, labels):\n",
    "        if lab == \"B\":\n",
    "            if current:\n",
    "                answers.append(\" \".join(current))\n",
    "            current = [tok]\n",
    "        elif lab == \"I\":\n",
    "            current.append(tok)\n",
    "        else:  # O\n",
    "            if current:\n",
    "                answers.append(\" \".join(current))\n",
    "                current = []\n",
    "    if current:\n",
    "        answers.append(\" \".join(current))\n",
    "    return answers\n",
    "\n",
    "# fungsi untuk map ke dataset\n",
    "def preprocess(example):\n",
    "    # gabungkan token jadi string utuh\n",
    "    example[\"question_text\"] = \" \".join(example[\"question\"])\n",
    "    # example[\"passage_text\"]  = \" \".join(example[\"passage\"])\n",
    "    # ekstrak jawaban\n",
    "    example[\"answer_text\"]   = extract_answer(example[\"passage\"], example[\"seq_label\"])\n",
    "    return example\n",
    "\n",
    "# terapkan ke semua split\n",
    "facqa = facqa.map(preprocess)\n",
    "\n",
    "# cek hasil\n",
    "print(facqa)\n",
    "print(facqa[0][\"question_text\"])\n",
    "# print(facqa[0][\"passage_text\"])\n",
    "print(facqa[0][\"answer_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da925156",
   "metadata": {},
   "source": [
    "lakukan segmentasi passage agar panjangnya 180 token. dalam kasus ada segmen yang panjangnya kurang dari 50 token, maka gabungkan segmen tersebut ke segmen sebelumnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2369e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3117/3117 [00:09<00:00, 334.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset  # jika perlu contoh lokal\n",
    "import re\n",
    "\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# --- util: simple detokenizer utk spasi & tanda baca ---\n",
    "_PUNCT_RIGHT = set([\",\", \".\", \":\", \";\", \"!\", \"?\", \"%\", \")\", \"]\", \"}\", \"’\", \"”\"])\n",
    "_PUNCT_LEFT  = set([\"(\", \"[\", \"{\", \"‘\", \"“\", \"Rp\"])\n",
    "\n",
    "def detok(tokens):\n",
    "    out = []\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if i == 0:\n",
    "            out.append(tok)\n",
    "            continue\n",
    "        # tanpa spasi sebelum tanda baca kanan\n",
    "        if tok in _PUNCT_RIGHT:\n",
    "            out[-1] = out[-1] + tok\n",
    "        # tanpa spasi setelah tanda buka kiri\n",
    "        elif out[-1] in _PUNCT_LEFT:\n",
    "            out[-1] = out[-1] + tok\n",
    "        # kasus khusus tanda persen\n",
    "        elif tok == \"%\" and out[-1].isdigit():\n",
    "            out[-1] = out[-1] + tok\n",
    "        else:\n",
    "            out.append(\" \" + tok)\n",
    "    return \"\".join(out)\n",
    "\n",
    "# --- deteksi span B-I: kembalikan [(start, end_inclusive), ...] ---\n",
    "def find_spans(seq_label):\n",
    "    spans = []\n",
    "    start = None\n",
    "    for i, tag in enumerate(seq_label):\n",
    "        if tag == \"B\":\n",
    "            if start is not None:\n",
    "                spans.append((start, i-1))\n",
    "            start = i\n",
    "        elif tag == \"I\":\n",
    "            if start is None:\n",
    "                # Jika ada I tanpa B (data noisy), mulai span di sini\n",
    "                start = i\n",
    "        else:  # \"O\" atau lainnya\n",
    "            if start is not None:\n",
    "                spans.append((start, i-1))\n",
    "                start = None\n",
    "    if start is not None:\n",
    "        spans.append((start, len(seq_label)-1))\n",
    "    return spans\n",
    "\n",
    "# --- helper: apakah index i ada di dalam span manapun; dan cari end span aktif ---\n",
    "def span_end_if_inside(i, spans):\n",
    "    for s, e in spans:\n",
    "        if s <= i <= e:\n",
    "            return e\n",
    "    return None\n",
    "\n",
    "def is_span_start(i, spans):\n",
    "    for s, e in spans:\n",
    "        if s == i:\n",
    "            return (s, e)\n",
    "    return None\n",
    "\n",
    "# --- hitung jumlah wordpieces per token ---\n",
    "def wp_counts_for_tokens(tokens, tokenizer):\n",
    "    counts = []\n",
    "    for tok in tokens:\n",
    "        # add_special_tokens=False agar hitung murni konten\n",
    "        ids = tokenizer(tok, add_special_tokens=False).input_ids\n",
    "        counts.append(len(ids))\n",
    "    return counts\n",
    "\n",
    "# --- segmentasi aman span + MERGE segmen < min_wp ke segmen sebelumnya ---\n",
    "def segment_passage(tokens, seq_label, tokenizer, max_wp=180, min_wp=50):\n",
    "    assert len(tokens) == len(seq_label)\n",
    "    spans = find_spans(seq_label)\n",
    "    wp_counts = wp_counts_for_tokens(tokens, tokenizer)\n",
    "\n",
    "    segments_tok = []  # list of token-slices (masih berupa list token)\n",
    "    i = 0\n",
    "    seg_start = 0\n",
    "    acc_wp = 0\n",
    "\n",
    "    n = len(tokens)\n",
    "    while i < n:\n",
    "        c = wp_counts[i]\n",
    "        # jika muat, lanjut\n",
    "        if acc_wp + c <= max_wp:\n",
    "            acc_wp += c\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # tidak muat bila tambah token i\n",
    "        # cek: apakah kita sedang berada di dalam span (i-1 di dalam span yg berlanjut melewati i-1)?\n",
    "        if i > seg_start:\n",
    "            end_inside = span_end_if_inside(i-1, spans)\n",
    "        else:\n",
    "            end_inside = None\n",
    "\n",
    "        # 1) Jika sedang di dalam span, majukan ke akhir span\n",
    "        if end_inside is not None and end_inside >= i-1:\n",
    "            cut_end = end_inside  # inklusif\n",
    "            if cut_end < i-1:\n",
    "                cut_end = i-1\n",
    "            segments_tok.append(tokens[seg_start:cut_end+1])\n",
    "            # reset ke setelah span\n",
    "            i = cut_end + 1\n",
    "            seg_start = i\n",
    "            acc_wp = 0\n",
    "            continue\n",
    "\n",
    "        # 2) Jika token i adalah awal span, potong sebelum span\n",
    "        span_at_i = is_span_start(i, spans)\n",
    "        if span_at_i is not None:\n",
    "            if i == seg_start:\n",
    "                s, e = span_at_i\n",
    "                segments_tok.append(tokens[s:e+1])  # span > max_wp pun dibolehkan 1 chunk\n",
    "                i = e + 1\n",
    "                seg_start = i\n",
    "                acc_wp = 0\n",
    "            else:\n",
    "                segments_tok.append(tokens[seg_start:i])\n",
    "                seg_start = i\n",
    "                acc_wp = 0\n",
    "            continue\n",
    "\n",
    "        # 3) Bukan di dalam span, bukan awal span -> potong biasa sebelum i\n",
    "        if i == seg_start:\n",
    "            segments_tok.append([tokens[i]])\n",
    "            i += 1\n",
    "            seg_start = i\n",
    "            acc_wp = 0\n",
    "        else:\n",
    "            segments_tok.append(tokens[seg_start:i])\n",
    "            seg_start = i\n",
    "            acc_wp = 0\n",
    "\n",
    "    # sisa\n",
    "    if seg_start < n:\n",
    "        segments_tok.append(tokens[seg_start:])\n",
    "\n",
    "    # konversi ke teks (sementara, sebelum merge)\n",
    "    segment_texts = [detok(seg) for seg in segments_tok]\n",
    "\n",
    "    # === MERGE: gabungkan segmen yang < min_wp ke segmen sebelumnya ===\n",
    "    def wp_len(text):\n",
    "        return len(tokenizer(text, add_special_tokens=False).input_ids)\n",
    "\n",
    "    merged = []\n",
    "    for idx, seg_text in enumerate(segment_texts):\n",
    "        if idx > 0 and wp_len(seg_text) < min_wp:\n",
    "            # append ke sebelumnya (boleh melewati 180 sesuai aturan pengecualian)\n",
    "            merged[-1] = merged[-1].rstrip() + \" \" + seg_text.lstrip()\n",
    "        else:\n",
    "            merged.append(seg_text)\n",
    "\n",
    "    full_text = detok(tokens)\n",
    "    return full_text, merged\n",
    "\n",
    "# ====== Integrasi ke HuggingFace Dataset (tetap SATU kolom akhir) ======\n",
    "def add_text_and_segments(example):\n",
    "    full_text, segs_final = segment_passage(\n",
    "        example[\"passage\"], example[\"seq_label\"], t5_tokenizer, max_wp=180, min_wp=50\n",
    "    )\n",
    "    example[\"passage_text\"] = full_text\n",
    "    example[\"passage_segments_180\"] = segs_final  # sudah final + merged\n",
    "    return example\n",
    "\n",
    "facqa_initial = facqa.map(add_text_and_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb3e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3117/3117 [00:00<00:00, 5756.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# def add_positive_negative(example):\n",
    "#     segments = example.get(\"passage_segments_180\", [])  # list[str], hasil segment_passage final\n",
    "#     answers  = example.get(\"answer_text\", [])           # list[str]\n",
    "\n",
    "#     # bersihkan jawaban kosong/whitespace\n",
    "#     answers = [a for a in answers if isinstance(a, str) and a.strip()]\n",
    "\n",
    "#     # fungsi cek: segmen memuat SEMUA jawaban\n",
    "#     def contains_all(seg, ans_list):\n",
    "#         return all(a in seg for a in ans_list)\n",
    "\n",
    "#     pos_indices = [i for i, seg in enumerate(segments) if contains_all(seg, answers)]\n",
    "\n",
    "#     if pos_indices:\n",
    "#         pos_idx = pos_indices[0]               # ambil yang pertama jika ada beberapa\n",
    "#         positive = segments[pos_idx]\n",
    "#         negatives = [seg for j, seg in enumerate(segments) if j != pos_idx]\n",
    "#         has_all = True\n",
    "#     else:\n",
    "#         # tidak ada segmen yang memuat semua jawaban\n",
    "#         pos_idx = -1\n",
    "#         positive = \"\"                          # atau None, sesuai preferensi\n",
    "#         negatives = segments[:]                # semua dianggap negatif\n",
    "#         has_all = False\n",
    "\n",
    "#     example[\"positive_passage\"] = positive\n",
    "#     example[\"negative_passages\"] = negatives\n",
    "#     example[\"positive_passage_index\"] = pos_idx\n",
    "#     example[\"has_all_answers_in_one_segment\"] = has_all\n",
    "#     return example\n",
    "\n",
    "# # Terapkan ke dataset (sesudah kolom passage_segments_180 & answer_text tersedia)\n",
    "# facqa = facqa.map(add_positive_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a9a3d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3117/3117 [00:00<00:00, 3878.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# normalisasi ringan untuk kesesuaian segmen vs jawaban\n",
    "_PUNCT_RIGHT = r\",\\.\\:\\;\\!\\?\\%\\)\\]\\}\"\n",
    "_PUNCT_LEFT  = r\"\\(\\[\\{\"\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    # hapus spasi sebelum tanda baca kanan: \"Fauzi , Gubernur\" -> \"Fauzi, Gubernur\"\n",
    "    s = re.sub(rf\"\\s+([{_PUNCT_RIGHT}])\", r\"\\1\", s)\n",
    "    # hapus spasi setelah tanda buka kiri: \"( 15/12)\" -> \"(15/12)\"\n",
    "    s = re.sub(rf\"([{_PUNCT_LEFT}])\\s+\", r\"\\1\", s)\n",
    "    # rapikan spasi ganda\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    # trim\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def add_positive_negative(example):\n",
    "    segments = example.get(\"passage_segments_180\", [])  # list[str]\n",
    "    answers  = example.get(\"answer_text\", [])           # list[str]\n",
    "\n",
    "    # bersihkan / normalisasi\n",
    "    seg_norms = [normalize_text(seg).casefold() for seg in segments]\n",
    "    ans_norms = [normalize_text(a).casefold() for a in answers if isinstance(a, str) and a.strip()]\n",
    "\n",
    "    # fungsi cek: segmen memuat SEMUA jawaban\n",
    "    def contains_all(seg_norm: str, ans_list_norm):\n",
    "        return all(a in seg_norm for a in ans_list_norm)\n",
    "\n",
    "    if not segments:\n",
    "        # tidak ada segmen sama sekali\n",
    "        example[\"positive_passage\"] = \"\"\n",
    "        example[\"negative_passages\"] = []\n",
    "        example[\"positive_passage_index\"] = -1\n",
    "        example[\"has_all_answers_in_one_segment\"] = False\n",
    "        return example\n",
    "\n",
    "    # jika tidak ada jawaban (edge case), anggap tidak ada segmen positif\n",
    "    if not ans_norms:\n",
    "        example[\"positive_passage\"] = \"\"\n",
    "        example[\"negative_passages\"] = segments[:]  # semua negatif\n",
    "        example[\"positive_passage_index\"] = -1\n",
    "        example[\"has_all_answers_in_one_segment\"] = False\n",
    "        return example\n",
    "\n",
    "    # cari segmen yang memuat semua jawaban\n",
    "    pos_indices = [i for i, segn in enumerate(seg_norms) if contains_all(segn, ans_norms)]\n",
    "\n",
    "    if pos_indices:\n",
    "        pos_idx = pos_indices[0]                # ambil yang pertama jika ada beberapa\n",
    "        positive = segments[pos_idx]            # pakai teks aslinya (tanpa normalisasi)\n",
    "        negatives = [seg for j, seg in enumerate(segments) if j != pos_idx]\n",
    "        has_all = True\n",
    "    else:\n",
    "        pos_idx = -1\n",
    "        positive = \"\"                           # atau None\n",
    "        negatives = segments[:]                 # semua dianggap negatif\n",
    "        has_all = False\n",
    "\n",
    "    example[\"positive_passage\"] = positive\n",
    "    example[\"negative_passages\"] = negatives\n",
    "    example[\"positive_passage_index\"] = pos_idx\n",
    "    example[\"has_all_answers_in_one_segment\"] = has_all\n",
    "    return example\n",
    "\n",
    "# Terapkan ke dataset (setelah passage_segments_180 & answer_text ada)\n",
    "facqa_positive_and_negative = facqa_initial.map(add_positive_negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bf5c102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banyak baris yang jawabannya tidak berada di segmen pertama: 39\n",
      "Disimpan ke answers_not_in_first_segment.xlsx\n"
     ]
    }
   ],
   "source": [
    "def all_answers_not_in_first(example):\n",
    "    answers = example[\"answer_text\"]  # list of strings\n",
    "    segments = example[\"passage_segments_180\"]\n",
    "\n",
    "    if not segments:  # jaga-jaga kalau list kosong\n",
    "        return False\n",
    "\n",
    "    first_seg = segments[0]\n",
    "    # cek apakah ada jawaban yg muncul di segmen pertama\n",
    "    any_in_first = any(ans in first_seg for ans in answers)\n",
    "\n",
    "    # kalau tidak ada satu pun di segmen pertama, return True\n",
    "    return not any_in_first\n",
    "\n",
    "multi_segments = facqa_positive_and_negative.filter(lambda x: len(x['passage_segments_180'])> 1)\n",
    "# filter baris yang semua jawabannya bukan di segmen pertama\n",
    "not_in_first = multi_segments.filter(all_answers_not_in_first)\n",
    "\n",
    "print(\"banyak baris yang jawabannya tidak berada di segmen pertama:\", len(not_in_first))\n",
    "\n",
    "df = not_in_first.to_pandas()\n",
    "df.to_excel(\"answers_not_in_first_segment.xlsx\", index=False)\n",
    "print(\"Disimpan ke answers_not_in_first_segment.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254366eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3117/3117 [00:00<00:00, 4599.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total B: 3169\n",
      "Total rows: 3117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 3117/3117 [00:00<00:00, 5156.09 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with multi-span answers: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) hitung jumlah 'B' total & distribusi 'B' per baris\n",
    "from collections import Counter\n",
    "def count_B_I_O(ex):\n",
    "    c = Counter(ex[\"seq_label\"])\n",
    "    ex[\"num_B\"] = c.get(\"B\", 0)\n",
    "    ex[\"num_I\"] = c.get(\"I\", 0)\n",
    "    ex[\"num_O\"] = c.get(\"O\", 0)\n",
    "    return ex\n",
    "\n",
    "facqa = facqa.map(count_B_I_O)\n",
    "\n",
    "total_B = int(sum(facqa[\"num_B\"]))\n",
    "print(\"Total B:\", total_B)\n",
    "print(\"Total rows:\", len(facqa))\n",
    "\n",
    "# 2) filter baris yang punya >1 span (num_B > 1)\n",
    "multi_span = facqa.filter(lambda ex: ex[\"num_B\"] > 1)\n",
    "print(\"Rows with multi-span answers:\", len(multi_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "712da5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3117/3117 [06:39<00:00,  7.81 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'passage', 'seq_label', 'id', 'question_text', 'answer_text', 'passage_text', 'passage_segments_180', 'positive_passage', 'negative_passages', 'positive_passage_index', 'has_all_answers_in_one_segment', 'question_text_norm', 'question_text_stem'],\n",
       "    num_rows: 3117\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facqa_positive_and_negative = facqa_positive_and_negative.map(\n",
    "    add_norm_and_stem, \n",
    "    batched=True, \n",
    "    fn_kwargs={\"column\": \"question_text\"}   # sesuaikan dengan nama kolom di tydiqa_gold_all\n",
    ")\n",
    "facqa_positive_and_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43578854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'passage', 'seq_label', 'id', 'question_text', 'answer_text', 'passage_text', 'passage_segments_180', 'positive_passage', 'negative_passages', 'positive_passage_index', 'has_all_answers_in_one_segment', 'question_text_norm', 'question_text_stem'],\n",
       "    num_rows: 3117\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facqa_positive_and_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "601ee2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'id', 'question_text', 'answer_text', 'positive_passage', 'negative_passages', 'question_text_norm', 'question_text_stem'],\n",
       "    num_rows: 3117\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facqa_positive_and_negative = facqa_positive_and_negative.remove_columns([ 'passage', 'seq_label','passage_text', 'passage_segments_180','positive_passage_index', 'has_all_answers_in_one_segment'])\n",
    "facqa_positive_and_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8080b415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah question_stem yang terduplikasi: 73\n",
      "Jumlah baris yang termasuk duplikat  : 189\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# konversi dataset HF ke pandas\n",
    "df = facqa_positive_and_negative.to_pandas()\n",
    "\n",
    "# cari duplikasi berdasarkan kolom question_stem\n",
    "dupes_mask = df.duplicated(subset=[\"question_text_stem\"], keep=False)\n",
    "\n",
    "# subset baris duplikat\n",
    "dupes_df = df[dupes_mask].sort_values(\"question_text_stem\")\n",
    "\n",
    "# hitung jumlah pertanyaan yang duplikat\n",
    "n_dupes = dupes_df[\"question_text_stem\"].nunique()\n",
    "print(f\"Jumlah question_stem yang terduplikasi: {n_dupes}\")\n",
    "print(f\"Jumlah baris yang termasuk duplikat  : {len(dupes_df)}\")\n",
    "\n",
    "# simpan ke Excel\n",
    "# dupes_df.to_excel(\"facqa_question_stem_duplicates.xlsx\", index=False)\n",
    "# print(\"✅ File 'facqa_question_stem_duplicates.xlsx' berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5330b5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3117/3117 [00:00<00:00, 102360.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "facqa.save_to_disk(\"facqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68cd6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "facqa = load_from_disk(\"facqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51112fc0",
   "metadata": {},
   "source": [
    "menghapus baris yang questions_stem nya terduplikasi dan answer nya jelek (pengecekan answer secara manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0437db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before: 3117 | after: 3108 | removed: 9\n"
     ]
    }
   ],
   "source": [
    "# ====== 2) Daftar pasangan dari Excel (sudah dituliskan ulang ke kode) ======\n",
    "EXCEL_PAIRS = [\n",
    "    (\"apa nama badan bagi dari universitas gadjah mada ugm yang libat dalam ada sistem energi alternatif di yogyakarta\", 'PSE UGM'),\n",
    "    (\"apa nama partai hosni mubarak\", 'NDP'),\n",
    "    ('di badan apakah bastian purnama jabat bagai direktur utama', 'BES'),\n",
    "    ('kapan keluar undang nomor 31 tentang ikan', 'tahun '\n",
    "    '2004'),\n",
    "    ('kapan mou indonesia-gam ditandatangani', '15/8'),\n",
    "    ('kepala badan apakah mohamed elbaradei', 'IAEA'),\n",
    "    ('kepala balai apakah yohanes sudarto', 'BKSDA Kalteng'),\n",
    "    ('menandatangani apakah perintah indonesia dan gam di helsinki finlandia tanggal 15 agustus 2005', 'nota kesepahaman atau MOU tentang perdamaian di Aceh'),\n",
    "    ('siapa nama putra presiden hosni mubarak','Jamal')\n",
    "]\n",
    "\n",
    "# Ubah ke set untuk lookup cepat; trimming spasi agar toleran spasi ekstra\n",
    "PAIRS_SET = set((qs.strip(), ans.strip()) for qs, ans in EXCEL_PAIRS)\n",
    "\n",
    "# ====== Filter facqa: drop baris yang match (question_text_stem, salah-satu answer_text) ======\n",
    "def _should_keep_facqa(example):\n",
    "    qstem = (example.get(\"question_text_stem\") or \"\").strip()\n",
    "    answers = example.get(\"answer_text\") or []  # list[str]\n",
    "    for t in answers:\n",
    "        if (qstem, (t or \"\").strip()) in PAIRS_SET:\n",
    "            return False  # baris ini DIHAPUS\n",
    "    return True  # keep\n",
    "\n",
    "before_n = len(facqa_positive_and_negative)\n",
    "facqa_filtered = facqa_positive_and_negative.filter(_should_keep_facqa)\n",
    "after_n = len(facqa_filtered)\n",
    "removed = before_n - after_n\n",
    "print(f\"Rows before: {before_n} | after: {after_n} | removed: {removed}\")\n",
    "\n",
    "# ====== (Opsional) laporan pasangan Excel yang tidak ketemu di dataset ======\n",
    "def _collect_hits(example):\n",
    "    qstem = (example.get(\"question_text_stem\") or \"\").strip()\n",
    "    answers = example.get(\"answer_text\") or []\n",
    "    hits = []\n",
    "    for t in answers:\n",
    "        key = (qstem, (t or \"\").strip())\n",
    "        if key in PAIRS_SET:\n",
    "            hits.append(key)\n",
    "    return {\"_hits\": hits}\n",
    "\n",
    "hits_ds = facqa_positive_and_negative.map(_collect_hits, batched=False)\n",
    "found_pairs = set()\n",
    "for row in hits_ds:\n",
    "    for h in row.get(\"_hits\", []):\n",
    "        # 'h' sudah tuple-like (atau list dua elemen)\n",
    "        found_pairs.add(tuple(h))\n",
    "\n",
    "not_found = PAIRS_SET - found_pairs\n",
    "if not_found:\n",
    "    print(\"\\nPairs from Excel NOT FOUND in facqa:\")\n",
    "    for qs, ans in sorted(not_found):\n",
    "        print(f\"- question_text_stem={qs!r} | answer_text element={ans!r}\")\n",
    "\n",
    "# Commit hasil filter\n",
    "facqa = facqa_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43b5af2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3108/3108 [00:00<00:00, 46293.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "facqa.save_to_disk(\"./generated_data/facqa_midway_through_duplication_removal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ff86b5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'passage_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'passage_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m facqa\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# panjang passage; NaN -> +inf agar tidak terpilih sebagai terpendek\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_passage_len\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassage_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen()\n\u001b[0;32m     10\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_passage_len\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_passage_len\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# (opsional) buat hasil deterministik pada tie: sort by _passage_len lalu ambil first\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'passage_text'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# convert ke pandas\n",
    "df = facqa.to_pandas()\n",
    "\n",
    "# panjang passage; NaN -> +inf agar tidak terpilih sebagai terpendek\n",
    "df[\"_passage_len\"] = df[\"passage_text\"].str.len()\n",
    "df[\"_passage_len\"] = df[\"_passage_len\"].fillna(np.inf)\n",
    "\n",
    "# (opsional) buat hasil deterministik pada tie: sort by _passage_len lalu ambil first\n",
    "df_sorted = df.sort_values(by=[\"question_text_stem\", \"_passage_len\"], kind=\"mergesort\")\n",
    "\n",
    "# ambil indeks dengan passage terpendek per question_text_stem\n",
    "idx_min = df_sorted.groupby(\"question_text_stem\")[\"_passage_len\"].idxmin()\n",
    "\n",
    "# subset baris yang disimpan\n",
    "df_kept = df.loc[idx_min].reset_index(drop=True)\n",
    "\n",
    "print(f\"Before: {len(df)} | After: {len(df_kept)} | Removed: {len(df) - len(df_kept)}\")\n",
    "\n",
    "# buang kolom bantu\n",
    "df_kept = df_kept.drop(columns=[\"_passage_len\"])\n",
    "\n",
    "# convert balik ke HF Dataset (hindari kolom __index_level_0__)\n",
    "facqa_wo_dedupe = Dataset.from_pandas(df_kept, preserve_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0caf723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3001/3001 [00:00<00:00, 3371.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def normalize_fields(batch, answer_col, psg_text):\n",
    "    new_answers = []\n",
    "    new_passages = []\n",
    "\n",
    "    answers_batch  = batch.get(answer_col, [])\n",
    "    passages_batch = batch.get(psg_text, [])\n",
    "\n",
    "    for i in range(len(answers_batch)):\n",
    "        # ===== answers: pastikan list, lalu normalize per elemen =====\n",
    "        ans = answers_batch[i]\n",
    "        if ans is None:\n",
    "            norm_ans = []\n",
    "        elif isinstance(ans, (list, tuple)):\n",
    "            norm_ans = [normalize(a, lowercase=False) for a in ans if a is not None]\n",
    "        else:\n",
    "            # kalau awalnya string tunggal → jadikan list satu elemen\n",
    "            norm_ans = [normalize(ans, lowercase=False)]\n",
    "\n",
    "        new_answers.append(norm_ans)\n",
    "\n",
    "        # ===== passage_text =====\n",
    "        passage = passages_batch[i] if i < len(passages_batch) else None\n",
    "        new_passages.append(normalize(passage, lowercase=False) if passage else passage)\n",
    "\n",
    "    # tulis balik ke kolom yang sama\n",
    "    return {\n",
    "        answer_col: new_answers,\n",
    "        psg_text: new_passages,\n",
    "    }\n",
    "\n",
    "facqa_wo_dedupe = facqa_wo_dedupe.map(\n",
    "    normalize_fields,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    load_from_cache_file=False,\n",
    "    fn_kwargs={\"answer_col\": \"answer_text\", \"psg_text\": \"passage_text\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d04c91b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'passage', 'seq_label', 'id', 'question_text', 'passage_text', 'answer_text', 'question_text_norm', 'question_text_stem'],\n",
       "    num_rows: 3001\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facqa_wo_dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2be43438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing `passage_text`: 100%|██████████| 6/6 [00:00<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Statistik panjang token `context` (Flan-T5) ===\n",
      "       count: 3001\n",
      "        mean: 132.93\n",
      "         std: 44.28\n",
      "         min: 15\n",
      "  p50_median: 127.00\n",
      "         p90: 191.00\n",
      "         p95: 211.00\n",
      "         p99: 266.00\n",
      "         max: 366\n",
      "   > 256:   1.40%\n",
      "   > 512:   0.00%\n",
      "   > 600:   0.00%\n",
      "  > 1024:   0.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = token_lengths_for_column(facqa_wo_dedupe, col=\"passage_text\", batch_size=512)\n",
    "stats = describe_lengths(lengths)\n",
    "# 4) Cetak statistik\n",
    "print(\"=== Statistik panjang token `context` (Flan-T5) ===\")\n",
    "for k in [\"count\",\"mean\",\"std\",\"min\",\"p50_median\",\"p90\",\"p95\",\"p99\",\"max\",\"> 256\",\"> 512\",\"> 600\",\"> 1024\"]:\n",
    "    if k in stats:\n",
    "        if k.startswith(\"> \"):\n",
    "            print(f\"{k:>8}: {stats[k]:6.2f}%\")\n",
    "        elif k in {\"mean\",\"std\",\"p50_median\",\"p90\",\"p95\",\"p99\"}:\n",
    "            print(f\"{k:>12}: {stats[k]:.2f}\")\n",
    "        else:\n",
    "            print(f\"{k:>12}: {stats[k]}\")\n",
    "\n",
    "# 5) Visualisasi histogram\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(lengths, bins=60, edgecolor=\"black\")\n",
    "plt.title(\"Distribusi Panjang Token `context` (Flan-T5)\")\n",
    "plt.xlabel(\"Jumlah token\"); plt.ylabel(\"Frekuensi\")\n",
    "\n",
    "# Garis bantu mean/median/p95\n",
    "mean_v = np.mean(lengths) if lengths.size else 0\n",
    "med_v  = np.median(lengths) if lengths.size else 0\n",
    "p95_v  = np.percentile(lengths, 95) if lengths.size else 0\n",
    "plt.axvline(mean_v, linestyle=\"--\", label=f\"Mean={mean_v:.1f}\")\n",
    "plt.axvline(med_v,  linestyle=\":\",  label=f\"Median={med_v:.1f}\")\n",
    "plt.axvline(p95_v,  linestyle=\"-.\", label=f\"P95={p95_v:.1f}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa1513a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "facqa_wo_dedupe=facqa_wo_dedupe.remove_columns(['question', 'passage', 'seq_label'])\n",
    "facqa_wo_dedupe=facqa_wo_dedupe.rename_column('answer_text', 'answer')\n",
    "\n",
    "# mapping nama lama ke nama baru\n",
    "rename_map = {\n",
    "    \"question_text\": \"question\",\n",
    "    \"question_text_norm\": \"question_norm\",\n",
    "    \"question_text_stem\": \"question_stem\",\n",
    "}\n",
    "\n",
    "# rename kolom\n",
    "facqa_wo_dedupe = facqa_wo_dedupe.rename_columns(rename_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b911c9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3001/3001 [00:00<00:00, 147956.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "facqa_wo_dedupe.save_to_disk(\"facqa_finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d947111",
   "metadata": {},
   "source": [
    "# Isi negative passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d8e61f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "corpus = load_dataset(\"khalidrizki/indonesian-wiki-chunked-180tok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7cb04d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['docid', 'title', 'text'],\n",
      "    num_rows: 300000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ambil hanya split train\n",
    "corpus = corpus[\"train\"]\n",
    "\n",
    "# shuffle dengan seed biar hasil konsisten\n",
    "corpus = corpus.shuffle(seed=42)\n",
    "\n",
    "# pilih 300 ribu baris pertama setelah shuffle\n",
    "corpus = corpus.select(range(300_000))\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d682c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embedding_model = AutoModel.from_pretrained(model_name).to(\"cuda:0\")\n",
    "\n",
    "# Fungsi average pooling\n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "corpus_dict = {row[\"docid\"]: (row[\"title\"], row[\"text\"]) for row in corpus}\n",
    "corpus_docids = list(corpus_dict.keys())\n",
    "corpus_texts = [f\"passage: {corpus_dict[docid][0]} | {corpus_dict[docid][1]}\" for docid in corpus_docids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96874ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Corpus: 100%|██████████| 2344/2344 [08:39<00:00,  4.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import faiss\n",
    "batch_size = 128  # Sesuaikan dengan VRAM yang tersedia\n",
    "corpus_embeddings = []\n",
    "\n",
    "for start_idx in tqdm(range(0, len(corpus_texts), batch_size), desc=\"Encoding Corpus\"):\n",
    "    end_idx = min(start_idx + batch_size, len(corpus_texts))\n",
    "    batch_texts = corpus_texts[start_idx:end_idx]\n",
    "\n",
    "    batch_dict = embedding_tokenizer(batch_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    batch_dict = {k: v.to(\"cuda:0\") for k, v in batch_dict.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**batch_dict)\n",
    "\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)  # Normalisasi untuk cosine similarity\n",
    "    corpus_embeddings.append(embeddings.to(torch.float32).cpu())  # Pastikan float32 dan tetap di CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f90c0121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perkiraan baris maksimal: 503065\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import numpy as np\n",
    "\n",
    "def max_rows_fit(dim, dtype=np.float32, use_fraction=0.6):\n",
    "    \"\"\"Perkirakan maksimum baris embedding float yang aman ditampung di RAM.\"\"\"\n",
    "    avail = psutil.virtual_memory().available  # bytes RAM yang bebas\n",
    "    bytes_per = np.dtype(dtype).itemsize * dim\n",
    "    return int((avail * use_fraction) // bytes_per)\n",
    "\n",
    "# contoh pakai dim dinamis dari satu batch kecil\n",
    "import torch\n",
    "dummy = corpus_embeddings[0] if len(corpus_embeddings) else None  # kalau kamu masih punya 1 batch tensor\n",
    "if dummy is not None:\n",
    "    dim = dummy.shape[1] if dummy.ndim == 2 else embedding_model.config.hidden_size\n",
    "else:\n",
    "    dim = embedding_model.config.hidden_size  # fallback\n",
    "\n",
    "print(\"Perkiraan baris maksimal:\", max_rows_fit(dim, np.float32, use_fraction=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1c14a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Gabungkan semua embedding menjadi satu tensor besar\n",
    "corpus_embeddings = torch.cat(corpus_embeddings, dim=0).numpy().astype(np.float32)  # Konversi ke NumPy\n",
    "\n",
    "# Buat FAISS index untuk pencarian similarity\n",
    "index = faiss.IndexFlatIP(corpus_embeddings.shape[1])  # IP = Inner Product (Cosine Similarity)\n",
    "index.add(corpus_embeddings)  # Tambahkan corpus embeddings ke FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25f2cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan index faiss\n",
    "import os\n",
    "os.makedirs(\"./generated_data\", exist_ok=True)\n",
    "faiss.write_index(index, \"./generated_data/_corpus_faiss.idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6b62f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def add_negative_passages(batch, indices, question_col, dataset_name):\n",
    "    \"\"\"\n",
    "    Tambahkan negative passages ke batch.\n",
    "    - dataset_name in {\"facqa\",\"indoqa\"}  -> ambil top-2 negatif.\n",
    "    - dataset_name == \"tydiqa\"            -> aturan lama (2/1/0 tergantung #positives).\n",
    "    - lainnya (fallback)                  -> aturan lama.\n",
    "    \"\"\"\n",
    "    ds = (dataset_name or \"\").lower()\n",
    "\n",
    "    # 1) Encode semua query dalam batch\n",
    "    batch_queries = [f\"query: {q}\" for q in batch[question_col]]\n",
    "    batch_dict = embedding_tokenizer(\n",
    "        batch_queries, max_length=512, padding=True, truncation=True, return_tensors='pt'\n",
    "    )\n",
    "    batch_dict = {k: v.to(\"cuda:0\") for k, v in batch_dict.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**batch_dict)\n",
    "\n",
    "    query_embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    query_embeddings = F.normalize(query_embeddings, p=2, dim=1).cpu().numpy().astype(np.float32)  # (B, dim)\n",
    "\n",
    "    # 2) FAISS search (ambil kandidat cukup banyak agar tidak habis setelah exclude positives)\n",
    "    #    7 awalnya; kita buat sedikit lebih longgar.\n",
    "    topk = 4\n",
    "    D, I = index.search(query_embeddings, topk)\n",
    "\n",
    "    # 3) Kumpulkan negatif per query\n",
    "    negative_passages_batch = []\n",
    "    for i, idx in enumerate(indices):\n",
    "        \n",
    "        if ds in {\"facqa\", \"indoqa\"}:\n",
    "            max_negatives = 2\n",
    "        else:  # \"tydiqa\" atau fallback\n",
    "            positive_docids = set(p[\"docid\"] for p in batch[\"positive_passages\"][i])\n",
    "            num_positive = len(positive_docids)\n",
    "            if num_positive == 1:\n",
    "                max_negatives = 2\n",
    "            elif num_positive == 2:\n",
    "                max_negatives = 1\n",
    "            else:  # 3+\n",
    "                max_negatives = 0\n",
    "\n",
    "        selected_negative_passages = []\n",
    "        if max_negatives > 0:\n",
    "            for doc_idx in I[i]:\n",
    "                docid = corpus_docids[doc_idx]\n",
    "                if dataset_name=='tydiqa' and docid in positive_docids:\n",
    "                    continue\n",
    "                title, text = corpus_dict[docid]\n",
    "                selected_negative_passages.append(\n",
    "                    {\"docid\": docid, \"title\": title, \"text\": text}\n",
    "                )\n",
    "                if len(selected_negative_passages) == max_negatives:\n",
    "                    break\n",
    "\n",
    "        negative_passages_batch.append(selected_negative_passages)\n",
    "\n",
    "    batch[\"mr_negative_passages\"] = negative_passages_batch\n",
    "\n",
    "    # 4) Bersihkan memori\n",
    "    del batch_dict, outputs, query_embeddings\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13d560",
   "metadata": {},
   "source": [
    "## FacQA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86158f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3001/3001 [01:38<00:00, 30.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "facqa_wo_dedupe = facqa_wo_dedupe.map(\n",
    "    add_negative_passages, \n",
    "    with_indices=True, \n",
    "    batched=True, \n",
    "    batch_size=16, \n",
    "    fn_kwargs=\n",
    "    {\n",
    "        \"question_col\":\"question\", \n",
    "        \"dataset_name\":\"facqa\"\n",
    "    } \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5391ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3001/3001 [00:00<00:00, 141437.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "facqa_wo_dedupe.save_to_disk(\"facqa_with_negs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b5e34",
   "metadata": {},
   "source": [
    "## IndoQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d88fdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'answer', 'passage_text', 'question_norm', 'question_stem'],\n",
       "    num_rows: 4247\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indoqa = indoqa_clean\n",
    "indoqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0591722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4247/4247 [02:23<00:00, 29.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "indoqa = indoqa.map(\n",
    "    add_negative_passages, \n",
    "    with_indices=True, \n",
    "    batched=True, \n",
    "    batch_size=16, \n",
    "    fn_kwargs=\n",
    "    {\n",
    "        \"question_col\":\"question\", \n",
    "        \"dataset_name\":\"indoqa\"\n",
    "    } \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "779921a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/4247 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4247/4247 [00:00<00:00, 139688.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "indoqa.save_to_disk(\"./generated_data/indoqa_with_negs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bbe58c",
   "metadata": {},
   "source": [
    "## TyDi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "813283b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document_title', 'passage_text', 'query', 'question_norm', 'question_stem', 'answer', 'mr_query_id', 'mr_docid', 'mr_negative_passages'],\n",
       "    num_rows: 5912\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "tydi = load_from_disk(\"tydi_finish\")\n",
    "tydi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "581fdcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def passage_miss_answer(example):\n",
    "    passage = example[\"passage_text\"]\n",
    "    answers = example[\"answer\"] or []  # list of string\n",
    "    \n",
    "    # True kalau tidak ada satu pun jawaban yang muncul di passage_text\n",
    "    return not any(ans in passage for ans in answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "129e2381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris yang miss: 3552\n",
      "{'id': '2678620215944634419-0', 'document_title': 'Televisi di Indonesia', 'passage_text': 'Stasiun televisi yang dikelola negara, TVRI mengadakan monopoli televisi di Indonesia sampai tahun 1989, ketika stasiun komersial pertama, RCTI (Rajawali Citra Televisi Indonesia) memulai siarannya sebagai stasiun televisi lokal dan kemudian diberikan lisensi untuk mengudara secara nasional setahun kemudian.', 'query': 'Aapakah stasiun pertama di Indonesia?', 'question_norm': 'aapa stasiun pertama di indonesia?', 'question_stem': 'aapa stasiun pertama di indonesia', 'answer': ['tvri'], 'mr_query_id': '5433', 'mr_docid': '2335198#0', 'mr_negative_passages': [{'docid': '41276#0', 'text': 'TVRI Jogja adalah stasiun televisi regional milik Televisi Republik Indonesia yang merupakan stasiun daerah pertama di Indonesia, didirikan pada tahun 1965. Pertama berdiri di Yogyakarta berlokasi di Jl. Hayam Wuruk, tepatnya saat TVRI Jogja dipimpin oleh Kepala Stasiun yang pertama yakni IR. Dewabrata. Konon, untuk mendirikan menara pemancar, dibangun dari bahan bambu.', 'title': 'TVRI Jogja'}, {'docid': '61110#3', 'text': 'Stasiun ini sering dijadikan sebagai stasiun kereta api percontohan (\"pilot project\") oleh PT KAI untuk segi kualitas pelayanan agar setara dengan bandara. Oleh karena itu, PT KAI menjadikan stasiun ini sebagai stasiun kereta api pertama di Indonesia yang menerapkan sistem \"check-in\" dan \"boarding pass\" sejak Februari 2016 dan sistem pemeriksaan bagasi dengan sinar-X sejak Oktober 2018.', 'title': 'Stasiun Bandung'}, {'docid': '39447#2', 'text': 'Metro Siang adalah program berita utama siang hari yang disiarkan oleh Metro TV. Metro Siang yang mengudara pertama kali siaran perdana di stasiun televisi berita pertama di Indonesia adalah Metro TV pada tanggal 26 November 2000. Metro Siang dirancang khusus untuk memenuhi kebutuhan pemirsa sebagai pengantar makan siang dan istirahat siang pemirsa. Metro Siang yang disiarkan di stasiun televisi berita pertama di Indonesia adalah Metro TV yang ditayangkan setiap hari pukul 11:05-13:00 WIB.', 'title': 'Metro (acara televisi)'}, {'docid': '2335198#25', 'text': 'TV terestrial dimulai dengan pendirian stasiun televisi pertama di Indonesia. Indonesia hanya memiliki satu saluran televisi sampai pembentukan lainnya, RCTI merupakan televisi swasta pertama di Indonesia. Saat ini, stasiun televisi utama nasional free-to-air terrestrial di Indonesia adalah TVRI, RCTI, SCTV, MNCTV, antv, Indosiar, MetroTV, Trans TV, Trans7, tvOne, GTV, Kompas TV, NET., RTV, dan iNews. Televisi terestrial analog di Indonesia saat ini disiarkan menggunakan sistem PAL-B/G dengan suara NICAM stereo. Sejak triwulan pertama 2011 aturan memungkinkan penayangan televisi digital bersamaan dengan dengan televisi analog di beberapa daerah. Indonesia mengadopsi format DVB-T tapi memutuskan untuk mengubah ke DVB-T2 pada tanggal 1 Januari 2012.', 'title': 'Televisi di Indonesia'}, {'docid': '369688#0', 'text': 'Stasiun Maguwo (MGW) (Hanacaraka:, \"Sêtasiyun Maguwa\") adalah stasiun kereta api bandara kelas II yang terletak di Maguwoharjo, Depok, Sleman. Stasiun ini yang terletak pada ketinggian +118 meter ini termasuk dalam Daerah Operasi VI Yogyakarta dan merupakan stasiun aktif yang lokasinya paling timur dan utara di Daerah Istimewa Yogyakarta. Stasiun ini merupakan stasiun kereta api bandara pertama di Indonesia serta memiliki empat jalur dengan jalur 2 dan 3 sebagai sepur lurus.', 'title': 'Stasiun Maguwo'}, {'docid': '2352722#4', 'text': 'Perlu diketahui, bahwasanya ada yang menyatakan bahwa Stasiun Kemijen (maupun Stasiun Semarang Gudang) adalah stasiun kereta api pertama di Indonesia (bahkan hingga saat ini). Padahal stasiun Kemijen yang dimaksud yang dibangun oleh Nederlandsch-Indische Spoorweg Maatschappij (NIS) pada tahun 1864 dan selesai pada tahun 1867, bukanlah stasiun tersebut. Stasiun Kemijen yang terletak di belakang Depo Pertamina adalah stasiun kecil warisan perusahaan kereta api SJS (Samarang-Joana Stoomtram Maatschappij) yang berusia lebih muda dibandingkan NIS. Sementara itu Stasiun Kemijen NIS yang dulu dinamakan Stasiun Samarang dan letaknya tersembunyi di tengah perkampungan.', 'title': 'Stasiun Samarang'}, {'docid': '2335198#9', 'text': 'Pada tanggal 24 Agustus 1989, stasiun televisi kedua di Indonesia, Rajawali Citra Televisi Indonesia atau RCTI, diresmikan. Stasiun televisi ini adalah stasiun televisi swasta pertama di Indonesia. Stasiun televisi ini dimiliki oleh Bambang Trihatmodjo. Tidak seperti TVRI, RCTI diizinkan untuk menyiarkan iklan hingga 15% jam siarannya. Pada tanggal 24 Agustus 1990, stasiun televisi yang ketiga, Surya Citra Televisi, sebelumnya SCTI atau Surabaya Central Televisi Indonesia, diresmikan. Stasiun televisi ini dimiliki oleh \"raja bioskop\" Sudwikatmono.', 'title': 'Televisi di Indonesia'}, {'docid': '354641#15', 'text': 'Sejarah pertelevisian di Indonesia diawali pada tahun 1962 oleh TVRI di Jakarta dengan menggunakan pemancar televisi VHF. Pembangunan pemancar TVRI berjalan dengan cepat terutama setelah diluncurkannya satelit Palapa pada tahun 1975. Pada tahun 1987, yaitu lahirnya stasiun penyiaran televisi swasta pertama di Indonesia, stasiun pemancar TVRI telah mencapai jumlah kurang lebih 200 stasiun pemancar yang keseluruhannya menggunakan frekuensi VHF, dan pemancar TV swasta pertama tersebut diberikan alokasi frekuensi pada pita UHF. Kebijaksanaan penggunaan pita frekuensi VHF untuk TVRI dan UHF untuk swasta pada saat itu dilakukan dengan beberapa pertimbangan yang menguntungkan negara sebagai berikut:', 'title': 'Frekuensi sangat tinggi'}, {'docid': '58004#7', 'text': 'Stasiun ini terdiri dari tiga tingkat. Aula utama, loket, beberapa restoran dan toko, serta mesin ATM terdapat pada tingkat pertama. Tingkat kedua adalah ruang tunggu dengan beberapa restoran cepat saji dan kafetaria, sedangkan peron berada pada tingkat ketiga. Karena stasiun ini termasuk stasiun besar, pengumuman memakai dwibahasa: Indonesia dan Inggris.', 'title': 'Stasiun Gambir'}, {'docid': '693422#2', 'text': 'Sejarah AICPA dimulai ketika \"American Association of Public Accountants\" (AAPA) dibentuk. Pada 1916, American Association of Public Accountants diganti dengan \"Institute of Public Accountants\", yang pada saat itu memiliki anggota 1150 orang. Namanya kemudian berubah menjadi \"American Institute of Accountants\" pada 1917 hingga 1957, ketika akhirnya menjadi \"American Institute of Certified Public Accountants\".', 'title': 'American Institute of Certified Public Accountants'}, {'docid': '72365#2', 'text': 'Pada tahun 1911, Nederlands-Indische Spoorweg Maatschappij mulai menyusun \"masterplan\" baru terhadap sistem perkeretaapian di jalur kereta api segmen Semarang-Solo-Yogyakarta yang sebelumnya diresmikan pada tahun 1873. Hal ini dikarenakan Stasiun Samarang NIS-stasiun pertama di Indonesia-yang pada enam tahun sebelumnya ditutup, sudah tak memungkinkan lagi dioperasikan sebagai stasiun sentral NIS apabila Semarang mengalami air rob.', 'title': 'Stasiun Semarang Tawang'}, {'docid': '1896944#0', 'text': 'Berikut ini adalah 15 kawasan pelabuhan tersibuk di dunia berdasarkan jumlah tonase pemuatan kargo. Kawasan-kawasan dengan pusat keramaian aktivitas di pelabuhan-pelabuhan tertentu menjadi pusat kegiatan ekonomi bagi negara terkait atau kawasan yang lebih luas. Data ini berdasarkan publikasi AAPA World Port Rankings selama tahun 2007 dan di sebagian kawasan pada 2008.', 'title': 'Daftar kawasan pelabuhan tersibuk di dunia'}, {'docid': '2125271#3', 'text': 'Di Indonesia, stasiun televisi yang pertama kali menggunakan NICAM adalah Indosiar, yang kemudian disusul oleh sejumlah stasiun televisi lain. Dan sekarang sistem NICAM telah dipakai oleh mayoritas stasiun televisi swasta di Indonesia, baik nasional maupun lokal.', 'title': 'NICAM'}, {'docid': '436027#1', 'text': 'Stasiun ini merupakan stasiun yang berada pada jalur kereta api pertama di Indonesia antara Stasiun Samarang NIS dengan Stasiun Tanggung. Stasiun ini biasa menjadi tempat perhentian bagi kereta api dari arah timur yang hendak ke barat apabila Stasiun Semarang Tawang tergenang banjir atau penuh jalurnya maupun terjadi kemacetan parah di Jalan Raya Kaligawe.', 'title': 'Stasiun Alastua'}, {'docid': '1841238#22', 'text': 'Setelah berkembang di Eropa dan Amerika pada tahun 1920-an, radio mulai masuk ke Indonesia pada tahun 1925 yang dibawa oleh Belanda saat masih menduduki Indonesia. Bataviase Radio Vereenigning (BRV) merupakan stasiun radio pertama yang resmi mengudara di Indonesia. Stasiun radio ini didirikan dengan tujuan untuk menyampaikan siaran propaganda terkait perusahaan dan perdagangan saat itu. Kemudian diikuti dengan berdirinya stasiun radio yang dipelopori oleh para pemuda Indonesia di Solo pada tahun 1933 dengan nama Solosche Radio Vereenigning (SRV). Hingga akhirnya Belanda menyerah dan kekuasaan Belanda saat itu diambil alih oleh Jepang dan berdampak pada stasiun radio yang ada saat itu. Stasiun radio dibawah penguasaan Jepang saat ini telah membawa dampak pada penyiaran program-program radio, yakni memihak kepentingan militer Jepang. Radio terus berkembang hingga tahun 1960-an banyak radio amatir bermunculan dan menjadi cikal bakal kemunculan radio siaran swasta di Indonesia.', 'title': 'Infrastruktur telekomunikasi'}, {'docid': '768576#0', 'text': 'Top TV Network adalah sebuah stasiun televisi swasta berjaringan di Indonesia. Stasiun televisi ini merupakan stasiun televisi berjaringan pertama yang memfokuskan diri pada wilayah timur dan tengah Indonesia.', 'title': 'Top TV Network'}, {'docid': '1435450#1', 'text': 'Stasiun kereta api ini dibangun pada tahun 1867 oleh Nederlands-Indische Spoorweg Maatschappij (NISM), perusahaan kereta api Hindia Belanda. Banyak yang mengira kalau stasiun ini adalah stasiun pertama di Indonesia, tetapi sesungguhnya Stasiun Samarang NIS adalah stasiun pertama di Indonesia. Dahulu stasiun ini merupakan stasiun kereta api besar dengan mempunyai dipo lokomotif, dipo gerbong, kantor, dan fasilitas kereta api yang lain.', 'title': 'Stasiun Semarang Gudang'}, {'docid': '799#0', 'text': 'Sejak berdirinya TVRI pada 1962, hingga 27 tahun setelah berdirinya TVRI, penduduk Indonesia hanya bisa menyaksikan satu saluran televisi saja. Namun pada tahun 1989, Pemerintah akhirnya mengizinkan RCTI sebagai stasiun televisi swasta pertama di Indonesia, meski hanya penduduk yang mempunyai antena parabola dan dekoder yang dapat menyaksikan RCTI, walaupun pada akhirnya dibuka untuk masyarakat mulai tanggal 21 Maret 1992 di Bandung. Berikut ini adalah daftar stasiun televisi di Indonesia.', 'title': 'Daftar stasiun televisi di Indonesia'}, {'docid': '2335198#17', 'text': 'Penggunaan Bahasa Mandarin dilarang pada tahun 1965-1994 di televisi Indonesia, namun penggunaannya tidak datang sampai tahun kemudian. Pada bulan November 2000, Metro TV menjadi stasiun pertama yang menyiarkan berita dalam Bahasa Mandarin untuk stasiun televisi lokal sejak siaran mulai di Indonesia.', 'title': 'Televisi di Indonesia'}, {'docid': '1470337#3', 'text': 'Ada yang menyatakan bahwa Stasiun Kemijen adalah stasiun kereta api pertama di Indonesia. Padahal stasiun Kemijen yang dimaksud yang dibangun oleh Nederlands-Indische Spoorweg Maatschappij (NIS) pada tahun 1864 yang selesai dan dibuka pada tahun 1867, bukanlah stasiun tersebut. Stasiun Kemijen yang terletak di belakang Depo Pertamina adalah stasiun kecil warisan perusahaan kereta api SJS (Semarang Joana Stoomtram) yang berusia lebih muda dibandingkan NIS. Sedangkan Stasiun Kemijen NIS yang dulu dinamakan Stasiun Samarang dan letaknya tersembunyi di tengah perkampungan.', 'title': 'Stasiun Kemijen'}, {'docid': '1469442#1', 'text': 'Panasonic Awards 2000 yang ditayangkan stasiun televisi bersiaran langsung oleh RCTI sebagai stasiun televisi bersiaran nasional milik swasta secara komersial pertama di Indonesia yang berasal dari kota Jakarta (ibu kota Republik Indonesia) sebelum Panasonic Awards 2001.', 'title': 'Panasonic Awards 2000'}, {'docid': '436857#1', 'text': 'Sejak tanggal 22 Januari 2005 sistem persinyalan di stasiun ini telah diganti dari manual menjadi elektrik. Pergantian sistem persinyalan itu diresmikan oleh Menhub RI Hatta Rajasa. Stasiun ini merupakan stasiun pertama di Indonesia yang menggunakan sinyal elektrik buatan dalam negeri. Sistem \"interlocking\" yang terdapat pada sinyal tersebut dirancang oleh PT Len Industri Indonesia (Persero).', 'title': 'Stasiun Slawi'}, {'docid': '1870202#0', 'text': 'Stasiun Semarang Tanjung Emas (SMH) adalah stasiun kereta api nonaktif yang berada di Tanjung Mas, Semarang Utara, Semarang. Stasiun ini merupakan salah satu stasiun kereta api pertama di Indonesia. Sebelum berhenti beroperasi, stasiun ini menjadi terminal peti kemas selama berapa dekade.', 'title': 'Stasiun Semarang Tanjung Emas'}, {'docid': '64189#2', 'text': 'Meskipun Stasiun Tugu yang diresmikan tahun 1887 ini merupakan salah satu stasiun yang cukup tua, stasiun ini memiliki arsitektur yang unik. Gedung stasiun berada di tengah kedua sisi rel kereta api, sedangkan bangunan menghadap ke jalan poros Kota Yogyakarta. Arsitektur stasiun ini bergaya art deco yang sangat populer pada masa antara Perang Dunia I dan Perang Dunia II. Stasiun ini pernah menjadi tujuan akhir perjalanan kereta luar biasa Presiden Republik Indonesia pertama, Ir. Soekarno, saat memindahkan ibu kota dari Jakarta ke Yogyakarta. Kini stasiun tersebut ditetapkan sebagai cagar budaya oleh Pemerintah Daerah Istimewa Yogyakarta.', 'title': 'Stasiun Yogyakarta'}, {'docid': '5078#3', 'text': 'Stasiun TV ini memiliki konsep agak berbeda dengan stasiun televisi lain, sebab selain mengudara selama 24 jam setiap hari, stasiun TV ini hanya memusatkan acaranya pada siaran warta berita saja. Tetapi dalam perkembangannya, stasiun ini kemudian juga memasukkan unsur hiburan dalam program-programnya, meski tetap dalam koridor news. MetroTV adalah stasiun pertama di Indonesia yang menyiarkan berita dalam bahasa Mandarin: Metro Xin Wen, dan juga satu-satunya stasiun TV di Indonesia yang tidak menayangkan sinetron. MetroTV juga menayangkan siaran internasional berbahasa Inggris pertama di Indonesia Indonesia Now yang dapat disaksikan dari seluruh dunia. Stasiun ini dikenal memiliki presenter berita terbanyak di Indonesia', 'title': 'MetroTV'}, {'docid': '423516#3', 'text': 'Stasiun ini merupakan stasiun kereta api keempat tertua di Indonesia (setelah Samarang NIS, Allas-Toewa, dan Broemboeng) dan sampai saat ini masih beroperasi di Indonesia. Pada tanggal 10 Agustus 1867, jalur kereta api pertama dibuka antara Tanggung-Samarang yang berjarak 25 kilometer oleh Gubernur Jenderal Ludolph Anne Jan Wilt Sloet van de Beele.', 'title': 'Stasiun Tanggung'}, {'docid': '8212#0', 'text': 'Jogja TV adalah sebuah stasiun televisi lokal di Kota Yogyakarta, Indonesia. Jogja TV dimiliki oleh PT Yogyakarta Tugu Televisi dan merupakan stasiun televisi swasta pertama di Yogyakarta. Stasiun televisi ini merupakan anggota jaringan Indonesia Network.', 'title': 'Jogja TV'}, {'docid': '2352722#1', 'text': 'Stasiun ini merupakan salah satu dari empat stasiun kereta api pertama di Indonesia dan dibangun oleh perusahaan kereta api swasta Belanda, Nederlandsch-Indische Spoorweg Maatschappij. Tiga lainnya adalah Allas-Toewa (Alastua), Broemboeng (Brumbung), dan Tangoeng (Tanggung). Stasiun ini juga merupakan stasiun yang beroperasi di jalur kereta api pertama di Indonesia, yang menghubungkan stasiun ini dengan Stasiun Tanggung. Dahulu jalur ini memiliki lebar sepur 1435 mm sebelum akhirnya diubah ke 1067 mm, bahkan NIS pernah membuat lebar sepur ganda di jalur ini.', 'title': 'Stasiun Samarang'}, {'docid': '216382#0', 'text': 'Lembaga Industri Musik Indonesia atau LIMA adalah himpunan perusahaan rekaman dan radio FM di Indonesia, didirikan pada 1975. Stasiun radio FM pertama bergabung adalah Prambors. LIMA menentukan tangga album dan lagu di Indonesia. Pada bulan Mei 1986, LIMA mulai menerbitkan tangga lagu mereka dalam majalah Gadis, dengan lagu #1 pertama \"I Wanna Dance With Somebody (Who Loves Me)\" oleh Whitney Houston dan album #1 pertama \"Koes Plus\" oleh Koes Plus. Saat ini terdapat lebih dari 15 perusahaan rekaman dan 28 stasiun radio FM yang tergabung dalam LIMA, dengan tangga lagu diterbitkan setiap Sabtu.', 'title': 'Lembaga Industri Musik Indonesia'}]}\n"
     ]
    }
   ],
   "source": [
    "tydi_miss = tydi.filter(passage_miss_answer)\n",
    "\n",
    "print(f\"Jumlah baris yang miss: {len(tydi_miss)}\")\n",
    "print(tydi_miss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8505b3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5912/5912 [00:01<00:00, 3251.40 examples/s]\n",
      "Filter: 100%|██████████| 5912/5912 [00:01<00:00, 3311.93 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kosong: 1614\n",
      "Berisi: 4298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# tydi = load_from_disk(\"tydi_finish\")\n",
    "\n",
    "# filter yang kosong\n",
    "tydi_empty = tydiqa_with_mr.filter(lambda x: not x[\"mr_negative_passages\"] or len(x[\"mr_negative_passages\"]) == 0)\n",
    "\n",
    "# filter yang ada isinya\n",
    "tydi_filled = tydiqa_with_mr.filter(lambda x: x[\"mr_negative_passages\"] and len(x[\"mr_negative_passages\"]) > 0)\n",
    "\n",
    "print(\"Kosong:\", len(tydi_empty))\n",
    "print(\"Berisi:\", len(tydi_filled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a24bdbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
       "        num_rows: 4902\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
       "        num_rows: 1224\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
       "        num_rows: 829\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr_tydi = load_dataset(\"castorini/mr-tydi\", \"indonesian\")\n",
    "mr_tydi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea009a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1614/1614 [00:00<00:00, 11634.71 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris yang miss: 20\n",
      "{'id': '3283543765806791008-1', 'document_title': 'Paritas (matematika)', 'passage_text': 'Definisi formal bilangan genap adalah adalah bilangan bulat dalam bentuk n=2k, dimana k adalah bilangan bulat; itu kemudian dapat dibuktikan bahwa bilangan ganjil adalah bilangan bulat dalam bentuk n=2k+1. Penggolongan ini hanya berlaku untuk bilangan bulat, dengan kata lain, bilangan tak bulat seperti 1/2, 4201, atau tak hingga bukan bilangan genap maupun ganjil.', 'question': 'Apakah bilangan genap?', 'question_norm': 'apa bilangan genap?', 'question_stem': 'apa bilang genap', 'answer': ['bilangan bulat dalam bentuk n=2<i data-parsoid=\\'{\"dsr\":[2454.2459'], 'mr_query_id': '316', 'mr_docid': '1881645#1', 'mr_title': 'Paritas (matematika)', 'mr_text': 'Definisi formal bilangan genap adalah adalah bilangan bulat dalam bentuk n=2<i data-parsoid=\\'{\"dsr\":[2454,2459,2,2]}\\'>k, di mana k adalah bilangan bulat;[3] itu kemudian dapat dibuktikan bahwa bilangan ganjil adalah bilangan bulat dalam bentuk n=2<i data-parsoid=\\'{\"dsr\":[3419,3424,2,2]}\\'>k+1. Penggolongan ini hanya berlaku untuk bilangan bulat, dengan kata lain, bilangan tak bulat seperti 1/2, 4.201, atau tak hingga bukan bilangan genap maupun ganjil.', 'mr_negative_passages': [], 'query': 'Apakah bilangan genap?'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# filter baris yang miss\n",
    "tydi_miss = tydi_empty.filter(passage_miss_answer)\n",
    "\n",
    "print(f\"Jumlah baris yang miss: {len(tydi_miss)}\")\n",
    "print(tydi_miss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62de9cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5702/5702 [00:00<00:00, 15507.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 16 baris cocok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 565/565 [00:00<00:00, 9657.60 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: 4 baris cocok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load dataset gold\n",
    "tydiqa_gold = load_dataset(\"khalidalt/tydiqa-goldp\", \"indonesian\", trust_remote_code=True)\n",
    "\n",
    "# ambil semua id dari tydi_miss\n",
    "ids_miss = set(tydi_miss[\"id\"])\n",
    "\n",
    "# filter split train/dev/test dari tydiqa_gold\n",
    "filtered_splits = {}\n",
    "for split_name, ds in tydiqa_gold.items():\n",
    "    ds_filtered = ds.filter(lambda x: x[\"id\"] in ids_miss)\n",
    "    filtered_splits[split_name] = ds_filtered\n",
    "    print(f\"{split_name}: {len(ds_filtered)} baris cocok\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb03f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File berhasil disimpan: tydiqa_gold_matched.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with pd.ExcelWriter(\"tydiqa_gold_matched.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for split_name, ds in filtered_splits.items():\n",
    "        df = ds.to_pandas()\n",
    "        df.to_excel(writer, sheet_name=split_name, index=False)\n",
    "\n",
    "print(\"File berhasil disimpan: tydiqa_gold_matched.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92bec1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'language', 'document_title', 'passage_text', 'question_text', 'answers'],\n",
      "    num_rows: 20\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# kalau ingin gabungkan semua split jadi satu dataset\n",
    "from datasets import concatenate_datasets\n",
    "tydiqa_gold_matched = concatenate_datasets(list(filtered_splits.values()))\n",
    "\n",
    "print(tydiqa_gold_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335be82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 20/20 [00:00<00:00, 623.82 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total baris: 20\n",
      "Baris yang passage_text TIDAK mengandung semua jawaban: 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 0 is out of bounds for size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal baris: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tydiqa_gold_matched)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaris yang passage_text TIDAK mengandung semua jawaban: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_answers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(missing_answers[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\datasets\\arrow_dataset.py:2861\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem(key)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\datasets\\arrow_dataset.py:2845\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2843\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   2844\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2845\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m   2846\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[0;32m   2847\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[0;32m   2848\u001b[0m )\n\u001b[0;32m   2849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\datasets\\formatting\\formatting.py:587\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    586\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[1;32m--> 587\u001b[0m     _check_valid_index_key(key, size)\n\u001b[0;32m    588\u001b[0m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\datasets\\formatting\\formatting.py:527\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[1;34m(key, size)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m size):\n\u001b[1;32m--> 527\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of bounds for size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "\u001b[1;31mIndexError\u001b[0m: Invalid key: 0 is out of bounds for size 0"
     ]
    }
   ],
   "source": [
    "def passage_contains_all_answers(example):\n",
    "    passage = example[\"passage_text\"]\n",
    "    # ambil list jawaban dari kolom answers.text\n",
    "    answers = list(example[\"answers\"][\"text\"])\n",
    "    \n",
    "    # True kalau semua jawaban ada di passage_text\n",
    "    return all(ans in passage for ans in answers)\n",
    "\n",
    "# filter baris yang tidak mengandung semua jawaban\n",
    "missing_answers = tydiqa_gold_matched.filter(lambda x: not passage_contains_all_answers(x))\n",
    "\n",
    "print(f\"Total baris: {len(tydiqa_gold_matched)}\")\n",
    "print(f\"Baris yang passage_text TIDAK mengandung semua jawaban: {len(missing_answers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1c6b6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File berhasil disimpan ke tydi_miss.xlsx dengan 943 baris.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# konversi dataset ke DataFrame\n",
    "df_miss = tydi_miss.to_pandas()\n",
    "\n",
    "# simpan ke excel\n",
    "output_path = \"tydi_miss.xlsx\"\n",
    "df_miss.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"File berhasil disimpan ke {output_path} dengan {len(df_miss)} baris.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c9e7e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensure facqa titles+negs:   0%|          | 0/3001 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 66\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_title\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc_titles,\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmr_negative_passages\u001b[39m\u001b[38;5;124m\"\u001b[39m: neg_rows,\n\u001b[0;32m     63\u001b[0m     }\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Terapkan ke facqa & indoqa (tydi sudah lengkap)\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m facqa  \u001b[38;5;241m=\u001b[39m facqa\u001b[38;5;241m.\u001b[39mmap(_add_and_cast_cols, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, features\u001b[38;5;241m=\u001b[39mNEW_COL_FEATURES, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsure facqa titles+negs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m indoqa \u001b[38;5;241m=\u001b[39m indoqa\u001b[38;5;241m.\u001b[39mmap(_add_and_cast_cols, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, features\u001b[38;5;241m=\u001b[39mNEW_COL_FEATURES, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsure indoqa titles+negs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# cek skema\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\datasets\\arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\datasets\\arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    565\u001b[0m }\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\datasets\\arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3156\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3157\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3158\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\datasets\\arrow_dataset.py:3570\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3568\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(batch\u001b[38;5;241m.\u001b[39mto_arrow())\n\u001b[0;32m   3569\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3570\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_batch(batch)\n\u001b[0;32m   3571\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples_in_batch\n\u001b[0;32m   3572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\datasets\\arrow_writer.py:560\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols:\n\u001b[0;32m    559\u001b[0m     col_values \u001b[38;5;241m=\u001b[39m batch_examples[col]\n\u001b[1;32m--> 560\u001b[0m     col_type \u001b[38;5;241m=\u001b[39m features[col] \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_values, (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)):\n\u001b[0;32m    562\u001b[0m         array \u001b[38;5;241m=\u001b[39m cast_array_to_feature(col_values, col_type) \u001b[38;5;28;01mif\u001b[39;00m col_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m col_values\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "from datasets import Features, Value, Sequence\n",
    "\n",
    "# skema kolom baru\n",
    "NEG_FEATURE = Sequence(feature={\n",
    "    \"docid\": Value(\"string\"),\n",
    "    \"title\": Value(\"string\"),\n",
    "    \"text\": Value(\"string\"),\n",
    "})\n",
    "NEW_COL_FEATURES = Features({\n",
    "    \"document_title\": Value(\"string\"),\n",
    "    \"mr_negative_passages\": NEG_FEATURE,\n",
    "})\n",
    "\n",
    "def _str_or_empty(x):\n",
    "    return \"\" if x is None else str(x)\n",
    "\n",
    "def _norm_one_row(v):\n",
    "    \"\"\"\n",
    "    Normalkan 1 baris mr_negative_passages menjadi list[dict{docid,title,text}] (semua string).\n",
    "    \"\"\"\n",
    "    if v is None:\n",
    "        return []\n",
    "    if isinstance(v, list):\n",
    "        out = []\n",
    "        for d in v:\n",
    "            if isinstance(d, dict):\n",
    "                out.append({\n",
    "                    \"docid\": _str_or_empty(d.get(\"docid\")),\n",
    "                    \"title\": _str_or_empty(d.get(\"title\")),\n",
    "                    \"text\":  _str_or_empty(d.get(\"text\")),\n",
    "                })\n",
    "            elif isinstance(d, (list, tuple)):\n",
    "                # (docid, title, text) atau variasi pendek\n",
    "                did = _str_or_empty(d[0]) if len(d) > 0 else \"\"\n",
    "                tit = _str_or_empty(d[1]) if len(d) > 1 else \"\"\n",
    "                txt = _str_or_empty(d[2]) if len(d) > 2 else \"\"\n",
    "                out.append({\"docid\": did, \"title\": tit, \"text\": txt})\n",
    "            else:\n",
    "                # string/angka/tipe lain -> jadikan text\n",
    "                out.append({\"docid\": \"\", \"title\": \"\", \"text\": _str_or_empty(d)})\n",
    "        return out\n",
    "    # kalau bukan list → bungkus sebagai satu elemen text\n",
    "    return [{\"docid\": \"\", \"title\": \"\", \"text\": _str_or_empty(v)}]\n",
    "\n",
    "def _add_and_cast_cols(batch):\n",
    "    n = len(batch[next(iter(batch))]) if batch else 0\n",
    "\n",
    "    # document_title (string)\n",
    "    if \"document_title\" in batch:\n",
    "        doc_titles = [_str_or_empty(x) for x in batch[\"document_title\"]]\n",
    "    else:\n",
    "        doc_titles = [\"\"] * n\n",
    "\n",
    "    # mr_negative_passages (list[dict])\n",
    "    if \"mr_negative_passages\" in batch:\n",
    "        neg_rows = [_norm_one_row(v) for v in batch[\"mr_negative_passages\"]]\n",
    "    else:\n",
    "        neg_rows = [[] for _ in range(n)]\n",
    "\n",
    "    return {\n",
    "        \"document_title\": doc_titles,\n",
    "        \"mr_negative_passages\": neg_rows,\n",
    "    }\n",
    "\n",
    "# Terapkan ke facqa & indoqa (tydi sudah lengkap)\n",
    "facqa  = facqa.map(_add_and_cast_cols, batched=True, features=NEW_COL_FEATURES, desc=\"Ensure facqa titles+negs\")\n",
    "indoqa = indoqa.map(_add_and_cast_cols, batched=True, features=NEW_COL_FEATURES, desc=\"Ensure indoqa titles+negs\")\n",
    "\n",
    "# cek skema\n",
    "print(facqa.features)\n",
    "print(indoqa.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ee1dd095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File 'all_datasets_question_stem_duplicates.xlsx' berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "# Urutkan duplikat berdasarkan question_stem\n",
    "dupes_sorted = dupes_df.sort_values(by=\"question_stem\").reset_index(drop=True)\n",
    "\n",
    "# Simpan ke Excel\n",
    "dupes_sorted.to_excel(\"all_datasets_question_stem_duplicates.xlsx\", index=False)\n",
    "\n",
    "print(\"✅ File 'all_datasets_question_stem_duplicates.xlsx' berhasil dibuat.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c51086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Removed by id list: 4 rows\n",
      "[Step 2] After dedup by question_stem (prefer 'tydiqa_with_mr'): 13146 rows\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# 1) Drop baris dengan id tertentu\n",
    "tydi_wrong_labels_index = [\n",
    "    \"7087322768323666798-2\",\n",
    "    \"-1483127104798159762-2\",\n",
    "    \"-954542885931683301-58\",\n",
    "    \"-2854454553098103373-6\",\n",
    "]\n",
    "BAD_ID_SET = set(tydi_wrong_labels_index)\n",
    "\n",
    "before_n = len(all_ds)\n",
    "all_ds = all_ds.filter(lambda _id: _id not in BAD_ID_SET, input_columns=[\"id\"])\n",
    "after_bad_drop = len(all_ds)\n",
    "print(f\"[Step 1] Removed by id list: {before_n - after_bad_drop} rows\")\n",
    "\n",
    "# 2) Dedup question_stem dengan preferensi source == \"tydiqa_with_mr\"\n",
    "#    (pakai pandas untuk memudahkan sort berprioritas)\n",
    "df = all_ds.to_pandas()\n",
    "\n",
    "# simpan urutan asli agar deterministik\n",
    "df[\"_orig_idx\"] = np.arange(len(df), dtype=np.int64)\n",
    "\n",
    "# rank: 0 untuk 'tydiqa_with_mr' (prioritas tertinggi), 1 untuk selain itu\n",
    "df[\"_src_rank\"] = (df[\"source\"] != \"tydiqa_with_mr\").astype(int)\n",
    "\n",
    "# sort per group lalu drop_duplicates per question_stem (keep yang rank paling kecil)\n",
    "df_sorted = df.sort_values([\"question_stem\", \"_src_rank\", \"_orig_idx\"])\n",
    "df_dedup  = df_sorted.drop_duplicates(subset=[\"question_stem\"], keep=\"first\")\n",
    "\n",
    "# bersih-bersih kolom bantu\n",
    "df_dedup = df_dedup.drop(columns=[\"_orig_idx\", \"_src_rank\"])\n",
    "\n",
    "# kembali ke HF Dataset\n",
    "all_ds = Dataset.from_pandas(df_dedup, preserve_index=False)\n",
    "\n",
    "print(f\"[Step 2] After dedup by question_stem (prefer 'tydiqa_with_mr'): {len(all_ds)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bdaee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalize answers/passages/titles/negatives: 100%|██████████| 13146/13146 [00:11<00:00, 1146.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import re, unicodedata\n",
    "from itertools import zip_longest\n",
    "\n",
    "def _normalize_negatives(negs):\n",
    "    \"\"\"\n",
    "    Normalisasi kolom mr_negative_passages agar tahan bentuk:\n",
    "    - list of dict: [{'title':..., 'text':...}, ...]\n",
    "    - list of tuple/list: [(title, text), ...] / [['t','x'], ...]\n",
    "    - dict of lists: {'title': [...], 'text': [...]}\n",
    "    - list of string: [text, ...]  (tanpa title)\n",
    "    - string tunggal\n",
    "    - None\n",
    "    Return: list of dict atau None\n",
    "    \"\"\"\n",
    "    if negs is None:\n",
    "        return None\n",
    "\n",
    "    out = []\n",
    "\n",
    "    # dict of lists (HF kadang mem-flatten struct menjadi dict of lists saat batched)\n",
    "    if isinstance(negs, dict) and (\"title\" in negs or \"text\" in negs):\n",
    "        titles = negs.get(\"title\", [])\n",
    "        texts  = negs.get(\"text\", [])\n",
    "        for ti, tx in zip_longest(titles, texts, fillvalue=None):\n",
    "            ti2 = normalize(ti, lowercase=False) if ti is not None else None\n",
    "            tx2 = normalize(tx, lowercase=False) if tx is not None else None\n",
    "            out.append({\"title\": ti2, \"text\": tx2})\n",
    "        return out\n",
    "\n",
    "    # list varian\n",
    "    if isinstance(negs, list):\n",
    "        for d in negs:\n",
    "            if isinstance(d, dict):\n",
    "                ti = d.get(\"title\")\n",
    "                tx = d.get(\"text\")\n",
    "                ti2 = normalize(ti, lowercase=False) if ti is not None else None\n",
    "                tx2 = normalize(tx, lowercase=False) if tx is not None else None\n",
    "                out.append({\"title\": ti2, \"text\": tx2})\n",
    "            elif isinstance(d, (tuple, list)):\n",
    "                # asumsi (title, text) atau (text_only,) dsb.\n",
    "                if len(d) >= 2:\n",
    "                    ti, tx = d[0], d[1]\n",
    "                    ti2 = normalize(ti, lowercase=False) if ti is not None else None\n",
    "                    tx2 = normalize(tx, lowercase=False) if tx is not None else None\n",
    "                    out.append({\"title\": ti2, \"text\": tx2})\n",
    "                elif len(d) == 1:\n",
    "                    tx = d[0]\n",
    "                    tx2 = normalize(tx, lowercase=False) if tx is not None else None\n",
    "                    out.append({\"title\": None, \"text\": tx2})\n",
    "            elif isinstance(d, str):\n",
    "                out.append({\"title\": None, \"text\": normalize(d, lowercase=False)})\n",
    "            else:\n",
    "                # tipe tak dikenal -> lewatkan apa adanya sebagai text\n",
    "                out.append({\"title\": None, \"text\": normalize(str(d), lowercase=False)})\n",
    "        return out\n",
    "\n",
    "    # string tunggal\n",
    "    if isinstance(negs, str):\n",
    "        return [{\"title\": None, \"text\": normalize(negs, lowercase=False)}]\n",
    "\n",
    "    # fallback: jadikan string lalu simpan sebagai text\n",
    "    return [{\"title\": None, \"text\": normalize(str(negs), lowercase=False)}]\n",
    "\n",
    "def normalize_fields(batch):\n",
    "    answers = batch[\"answer\"]\n",
    "    passages = batch.get(\"passage_text\", [])\n",
    "    titles   = batch.get(\"document_title\", [])\n",
    "    negs    = batch.get(\"mr_negative_passages\", [])\n",
    "\n",
    "    new_answers, new_passages, new_titles, new_neg_passages = [], [], [], []\n",
    "\n",
    "    n = len(answers)\n",
    "    for i in range(n):\n",
    "        # answers: list[str]\n",
    "        ans_list = answers[i] or []\n",
    "        normed_ans = [normalize(a, lowercase=False) for a in ans_list if a is not None]\n",
    "        new_answers.append(normed_ans)\n",
    "\n",
    "        # passage_text\n",
    "        passage = passages[i] if i < len(passages) else None\n",
    "        new_passages.append(normalize(passage, lowercase=False) if passage else passage)\n",
    "\n",
    "        # document_title\n",
    "        doc_title = titles[i] if i < len(titles) else None\n",
    "        new_titles.append(normalize(doc_title, lowercase=False) if doc_title else doc_title)\n",
    "\n",
    "        # mr_negative_passages (ragam bentuk)\n",
    "        neg_item = negs[i] if i < len(negs) else None\n",
    "        new_neg_passages.append(_normalize_negatives(neg_item))\n",
    "\n",
    "    return {\n",
    "        \"answer\": new_answers,\n",
    "        \"passage_text\": new_passages,\n",
    "        \"document_title\": new_titles,\n",
    "        \"mr_negative_passages\": new_neg_passages,\n",
    "    }\n",
    "\n",
    "# Terapkan ke dataset\n",
    "all_ds = all_ds.map(\n",
    "    normalize_fields,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Normalize answers/passages/titles/negatives\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d29b36",
   "metadata": {},
   "source": [
    "# Isi negatives dari dev dan test dengan passages dari corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d355357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['docid', 'title', 'text'],\n",
       "        num_rows: 1588236\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = load_dataset(\"khalidrizki/indonesian-wiki-chunked-180tok\", trust_remote_code=True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d4441",
   "metadata": {},
   "source": [
    "ambil negative passages buat yg belum punya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a1bfa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4298/4298 [38:27<00:00,  1.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import gc\n",
    "\n",
    "# ===== Model & tokenizer (Multilingual-E5-Small) =====\n",
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embedding_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = embedding_model.to(device)\n",
    "\n",
    "# ===== Helper: average pooling =====\n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    masked = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return masked.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "# ===== Top-K selector over mr_negative_passages using query_norm =====\n",
    "TOP_K = 4  # ganti ke 2 jika hanya ingin top-2\n",
    "\n",
    "def select_topk_mr_negative_passages(example):\n",
    "    negs = example.get(\"mr_negative_passages\") or []\n",
    "    # Jika kosong atau sudah <= TOP_K, tidak perlu proses\n",
    "    if len(negs) <= TOP_K:\n",
    "        return example\n",
    "\n",
    "    # E5 expects prefixes: 'query: ...' dan 'passage: ...'\n",
    "    query_text = f'query: {example.get(\"query_norm\", \"\")}'\n",
    "\n",
    "    # Rakit teks passage: \"passage: {title} | {text}\"\n",
    "    neg_texts = []\n",
    "    for neg in negs:\n",
    "        title = (neg.get(\"title\") or \"\").strip()\n",
    "        text  = (neg.get(\"text\")  or \"\").strip()\n",
    "        neg_texts.append(f'passage: {title} | {text}')\n",
    "\n",
    "    # Tokenisasi gabungan (query + semua negative passages)\n",
    "    batch_dict = embedding_tokenizer(\n",
    "        [query_text] + neg_texts,\n",
    "        max_length=512,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**batch_dict)\n",
    "\n",
    "    # Embedding + normalisasi (cosine)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    # Pisah query vs passages\n",
    "    query_emb = embeddings[0].unsqueeze(0)\n",
    "    neg_embs  = embeddings[1:]\n",
    "\n",
    "    # Similarity (cosine) → ambil TOP_K indeks terbaik\n",
    "    scores = (query_emb @ neg_embs.T).squeeze(0)\n",
    "    top_idx = torch.argsort(scores, descending=True)[:TOP_K].tolist()\n",
    "\n",
    "    # Pertahankan hanya TOP_K dokumen terbaik\n",
    "    example[\"mr_negative_passages\"] = [negs[i] for i in top_idx]\n",
    "\n",
    "    # Bersih-bersih memori\n",
    "    del batch_dict, outputs, embeddings, scores, query_emb, neg_embs\n",
    "    torch.cuda.empty_cache() if device.startswith(\"cuda\") else None\n",
    "    gc.collect()\n",
    "\n",
    "    return example\n",
    "\n",
    "# ===== Terapkan ke dataset =====\n",
    "# (single-process aman untuk fungsi berat GPU seperti ini)\n",
    "tydiqa_with_mr_neg_filled = tydiqa_with_mr_neg_filled.map(\n",
    "    select_topk_mr_negative_passages,\n",
    "    batched=False,\n",
    "    load_from_cache_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4841dac",
   "metadata": {},
   "source": [
    "Chunking negative passages yang sudah diatur oleh Mr. TyDi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8169f950",
   "metadata": {},
   "source": [
    "ambil top 2 dokumen negatif utk split train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ea5cd",
   "metadata": {},
   "source": [
    "truncate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d656f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
