{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88de1229",
   "metadata": {},
   "source": [
    "# Generate Data Mr. Tydi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32ad7f",
   "metadata": {},
   "source": [
    "## 1. Isi tile dan text dari split dev dan test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95252c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_id': '3',\n",
       " 'query': 'Dimana James Hepburn meninggal?',\n",
       " 'positive_passages': [{'docid': '2386357#15',\n",
       "   'text': 'Dia dipenjarakan di Puri Dragsholm, 75 kilometer Kopenhagen. Dia ditahan dalam apa yang dikatakan sebagai kondisi yang mengerikan. Dia meninggal pada bulan April 1578.[8][10]',\n",
       "   'title': 'James Hepburn'}],\n",
       " 'negative_passages': []}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Unduh dataset multi-lingual retrieval, memiliki 3 split: train, dev, test\n",
    "mr_tydi = load_dataset(\"castorini/mr-tydi\", \"indonesian\")\n",
    "\n",
    "# Load dataset corpus (sumber teks dan title)\n",
    "corpus = load_dataset(\"castorini/mr-tydi-corpus\", \"indonesian\", trust_remote_code=True)\n",
    "\n",
    "# Buat dictionary {docid: (title, text)} untuk pencarian cepat\n",
    "corpus_dict = {row[\"docid\"]: (row[\"title\"], row[\"text\"]) for row in corpus[\"train\"]}\n",
    "\n",
    "# Fungsi untuk melengkapi positive_passages dalam dataset mr_tydi\n",
    "def fill_passage_info(example):\n",
    "    for passage in example[\"positive_passages\"]:\n",
    "        docid = passage[\"docid\"]\n",
    "        if docid in corpus_dict:  # Cek apakah docid ada di corpus\n",
    "            passage[\"title\"], passage[\"text\"] = corpus_dict[docid]\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Terapkan fungsi untuk melengkapi positive_passages di split 'dev' dan 'test'\n",
    "mr_tydi[\"dev\"] = mr_tydi[\"dev\"].map(fill_passage_info)\n",
    "mr_tydi[\"test\"] = mr_tydi[\"test\"].map(fill_passage_info)\n",
    "\n",
    "# Cek hasilnya\n",
    "mr_tydi[\"dev\"][0]\n",
    "# print(mr_tydi[\"test\"][0])  # Contoh setelah pengisian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c94b0e",
   "metadata": {},
   "source": [
    "## 2. Ambil 2 passage untuk negative_passages khusus split dev dan test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de41fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer & model untuk Multilingual-E5-Small\n",
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embedding_model = AutoModel.from_pretrained(model_name).to(\"cuda:0\")\n",
    "\n",
    "# Fungsi average pooling\n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f261f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Corpus: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11480/11480 [1:52:06<00:00,  1.71it/s] \n"
     ]
    }
   ],
   "source": [
    "# Pastikan folder 'generated_data/' ada untuk menyimpan dataset berkala\n",
    "os.makedirs(\"../generated_data/raw\", exist_ok=True)\n",
    "\n",
    "# Buat dictionary {docid: (title, text)} untuk lookup cepat dari corpus\n",
    "corpus_dict = {row[\"docid\"]: (row[\"title\"], row[\"text\"]) for row in corpus[\"train\"]}\n",
    "\n",
    "# Ambil semua dokumen text dari corpus untuk dijadikan embedding\n",
    "corpus_docids = list(corpus_dict.keys())\n",
    "# corpus_texts = [corpus_dict[docid][1] for docid in corpus_docids]  # Ambil teksnya saja\n",
    "corpus_texts = [f\"passage: {corpus_dict[docid][0]} | {corpus_dict[docid][1]}\" for docid in corpus_docids]\n",
    "\n",
    "# Tokenisasi dan embedding seluruh corpus (hanya dilakukan sekali untuk efisiensi)\n",
    "batch_size = 128  # Sesuaikan dengan VRAM yang tersedia\n",
    "corpus_embeddings = []\n",
    "\n",
    "for start_idx in tqdm(range(0, len(corpus_texts), batch_size), desc=\"Encoding Corpus\"):\n",
    "    end_idx = min(start_idx + batch_size, len(corpus_texts))\n",
    "    batch_texts = corpus_texts[start_idx:end_idx]\n",
    "\n",
    "    batch_dict = embedding_tokenizer(batch_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    batch_dict = {k: v.to(\"cuda:0\") for k, v in batch_dict.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**batch_dict)\n",
    "\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)  # Normalisasi untuk cosine similarity\n",
    "    corpus_embeddings.append(embeddings.to(torch.float32).cpu())  # Pastikan float32 dan tetap di CPU\n",
    "\n",
    "# Gabungkan semua embedding menjadi satu tensor besar\n",
    "corpus_embeddings = torch.cat(corpus_embeddings, dim=0).numpy().astype(np.float32)  # Konversi ke NumPy\n",
    "\n",
    "# Buat FAISS index untuk pencarian similarity\n",
    "index = faiss.IndexFlatIP(corpus_embeddings.shape[1])  # IP = Inner Product (Cosine Similarity)\n",
    "index.add(corpus_embeddings)  # Tambahkan corpus embeddings ke FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf82337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan index faiss\n",
    "faiss.write_index(index, \"../generated_data/raw/faiss_index.idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c9c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1224/1224 [01:26<00:00, 14.23 examples/s]\n",
      "Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 384/829 [00:32<00:26, 16.51 examples/s]"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def add_negative_passages(batch, indices):\n",
    "#     batch_queries = batch[\"query\"]  # List of queries\n",
    "    batch_queries = [f\"query: {query}\" for query in batch[\"query\"]]\n",
    "\n",
    "    batch_dict = embedding_tokenizer(batch_queries, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    batch_dict = {k: v.to(\"cuda:0\") for k, v in batch_dict.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**batch_dict)\n",
    "\n",
    "    query_embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    query_embeddings = F.normalize(query_embeddings, p=2, dim=1).cpu().numpy().astype(np.float32)  # (batch, dim)\n",
    "\n",
    "    # FAISS Search untuk semua query dalam batch\n",
    "    D, I = index.search(query_embeddings, 7)  # Ambil 7 kandidat\n",
    "\n",
    "    # Iterasi untuk setiap query dalam batch\n",
    "    negative_passages_batch = []\n",
    "    for i, idx in enumerate(indices):\n",
    "        positive_docids = set(p[\"docid\"] for p in batch[\"positive_passages\"][i])\n",
    "        num_positive = len(positive_docids)\n",
    "\n",
    "        # Tentukan jumlah negatif sesuai aturan:\n",
    "        if num_positive == 1:\n",
    "            max_negatives = 2\n",
    "        elif num_positive == 2:\n",
    "            max_negatives = 1\n",
    "        else:  # 3 atau lebih\n",
    "            max_negatives = 0\n",
    "        selected_negative_passages = []\n",
    "\n",
    "        for doc_idx in I[i]:  # Loop hasil FAISS untuk query ke-i\n",
    "            if max_negatives == 0:\n",
    "                break  # Tidak perlu ambil negatif\n",
    "            docid = corpus_docids[doc_idx]\n",
    "            if docid not in positive_docids:\n",
    "                title, text = corpus_dict[docid]\n",
    "                selected_negative_passages.append({\"docid\": docid, \"title\": title, \"text\": text})\n",
    "            if len(selected_negative_passages) == max_negatives:\n",
    "                break\n",
    "\n",
    "        negative_passages_batch.append(selected_negative_passages)\n",
    "\n",
    "    batch[\"negative_passages\"] = negative_passages_batch\n",
    "\n",
    "    # ðŸ”¥ **BERSIHKAN CACHE GPU & MEMORI SETELAH BATCH SELESAI**\n",
    "    del batch_dict, outputs, query_embeddings\n",
    "    torch.cuda.empty_cache()  # Kosongkan cache GPU\n",
    "    gc.collect()  # Kosongkan cache CPU untuk menghindari memory leak\n",
    "\n",
    "    return batch\n",
    "\n",
    "mr_tydi[\"dev\"] = mr_tydi[\"dev\"].map(add_negative_passages, with_indices=True, batched=True, batch_size=16)\n",
    "mr_tydi[\"test\"] = mr_tydi[\"test\"].map(add_negative_passages, with_indices=True, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77184d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4902/4902 [00:00<00:00, 72832.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1224/1224 [00:00<00:00, 66556.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829/829 [00:00<00:00, 67840.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset berhasil disimpan dengan `save_to_disk`!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mr_tydi.save_to_disk(\"../generated_data/raw/mr_tydi_filled\")\n",
    "print(\"âœ… Dataset berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9c762",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed53ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "mr_tydi = load_from_disk(\"../generated_data/raw/mr_tydi_filled\")\n",
    "tydiqa_gold = load_dataset(\"khalidalt/tydiqa-goldp\", 'indonesian', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f0c0f",
   "metadata": {},
   "source": [
    "1. Membuang kolom yang tidak penting (language, document_title, dan passage_text)\n",
    "2. Mengganti nama kolom (dari yang awalnya id menjadi tydiqa_id, dari yang awalnya question_text ke query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dac584f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tydiqa_id', 'query', 'answers'],\n",
       "        num_rows: 5702\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tydiqa_id', 'query', 'answers'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tydiqa_gold = tydiqa_gold.remove_columns([\"language\", \"document_title\", \"passage_text\"])\n",
    "# language tidak perlu karena redundan (semua entry berbahasa Indonesia)\n",
    "# document_title dan passage_text tidak perlu karena tiap pos_psg dan neg_psg dari mr_tydi sudah menyimpan info tersebut\n",
    "tydiqa_gold = tydiqa_gold.rename_column(\"id\", \"tydiqa_id\")\n",
    "tydiqa_gold = tydiqa_gold.rename_column(\"question_text\", \"query\")\n",
    "tydiqa_gold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daad5ce",
   "metadata": {},
   "source": [
    "Untuk mengambil jawaban, maka kita perlu mengekstrak elemen text dari kolom answers (dan meninggalkan elemen start_byte dan limit_byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0d84312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "def extract_text(example):\n",
    "    example[\"answers\"] = example[\"answers\"][\"text\"]  # Ambil hanya bagian text, start_byte dan limit_byte dibuang saja\n",
    "    return example\n",
    "\n",
    "# Terapkan fungsi untuk membersihkan answers di setiap split\n",
    "tydiqa_gold = DatasetDict({\n",
    "    split: dataset.map(extract_text)\n",
    "    for split, dataset in tydiqa_gold.items()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a6217",
   "metadata": {},
   "source": [
    "1. Membuang karakter whitespace yang berlebihan\n",
    "2. Khusus untuk TyDi QA, mengambil jawaban yang paling pendek (dalam kasus terdapat beberapa jawaban)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c657d921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE\n",
      "['Siapa yang menciptakan Emosikon?'] : ['Nicolas\\nLoufrani']\n",
      "['siapakah ketua Perum LKBN pertama?'] : {'tydiqa_id': '8601389648636013237-1', 'query': 'siapakah ketua Perum LKBN pertama?', 'answers': ['Mr. Soemanang', 'Soemanang danAdam Malik']}\n"
     ]
    }
   ],
   "source": [
    "example1_id = \"-2253919563477221294-3\"\n",
    "example2_id = \"8601389648636013237-1\"\n",
    "print(\"BEFORE\")\n",
    "answer_with_unnecessary_whitespace = tydiqa_gold['train'].filter(lambda x: x['tydiqa_id']==example1_id)\n",
    "print(answer_with_unnecessary_whitespace['query'], \":\", answer_with_unnecessary_whitespace[0]['answers'])\n",
    "\n",
    "instance_with_multiple_answers = tydiqa_gold['validation'].filter(lambda x: x['tydiqa_id']==example2_id)\n",
    "print(instance_with_multiple_answers['query'], \":\", instance_with_multiple_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4b8e22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4902/4902 [00:00<00:00, 8071.08 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1224/1224 [00:00<00:00, 9484.83 examples/s] \n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829/829 [00:00<00:00, 8532.12 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nicolas Loufrani\n",
      "{'tydiqa_id': '8601389648636013237-1', 'query': 'siapakah ketua Perum LKBN pertama?', 'answers': 'Mr. Soemanang'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Fungsi untuk membersihkan teks: hapus newline & whitespace berlebih\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Fungsi untuk membersihkan dan memilih jawaban terpendek\n",
    "def clean_tydiqa(example):\n",
    "    # Bersihkan query\n",
    "    example[\"query\"] = clean_text(example[\"query\"])\n",
    "    \n",
    "    # Bersihkan answers dan pilih jawaban terpendek jika ada lebih dari satu\n",
    "    cleaned_answers = [clean_text(ans) for ans in example[\"answers\"]]\n",
    "    example[\"answers\"] = min(cleaned_answers, key=len) if cleaned_answers else \"\"  # Pilih jawaban terpendek\n",
    "\n",
    "    return example\n",
    "\n",
    "# Fungsi untuk membersihkan query di MR-TyDi\n",
    "def clean_mr_tydi(example):\n",
    "    example[\"query\"] = clean_text(example[\"query\"])\n",
    "    return example\n",
    "\n",
    "# Terapkan pembersihan pada dataset\n",
    "tydiqa_gold_cleaned = DatasetDict({\n",
    "    split: dataset.map(clean_tydiqa)\n",
    "    for split, dataset in tydiqa_gold.items()\n",
    "})\n",
    "\n",
    "mr_tydi_cleaned = DatasetDict({\n",
    "    split: dataset.map(clean_mr_tydi)\n",
    "    for split, dataset in mr_tydi.items()\n",
    "})\n",
    "\n",
    "check_if_answer_is_cleaned = tydiqa_gold_cleaned['train'].filter(lambda x: x['tydiqa_id']==example1_id)\n",
    "print(check_if_answer_is_cleaned[0]['answers'])\n",
    "\n",
    "check_if_answer_more_than_1 = tydiqa_gold_cleaned['validation'].filter(lambda x: x['tydiqa_id']==example2_id)\n",
    "print(check_if_answer_more_than_1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a686e03",
   "metadata": {},
   "source": [
    "# Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "996825dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset berhasil digabungkan berdasarkan `query` dengan struktur mengikuti `mr_tydi_cleaned`.\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets\n",
    "\n",
    "# Gabungkan split train & validation pada tydiqa_gold_cleaned\n",
    "tydiqa_gold_combined = concatenate_datasets([tydiqa_gold_cleaned[\"train\"], tydiqa_gold_cleaned[\"validation\"]])\n",
    "\n",
    "# Buat struktur baru mengikuti split dari mr_tydi_cleaned\n",
    "joined_datasets = {}\n",
    "\n",
    "for split, mr_tydi_split in mr_tydi_cleaned.items():\n",
    "    # Buat dictionary {query: row} dari tydiqa_gold_cleaned untuk lookup cepat\n",
    "    tydiqa_gold_dict = {row[\"query\"]: row for row in tydiqa_gold_combined}\n",
    "    \n",
    "    # Buat daftar baru dengan menggabungkan informasi dari mr_tydi_cleaned dan tydiqa_gold_cleaned\n",
    "    new_split_data = []\n",
    "    \n",
    "    for row in mr_tydi_split:\n",
    "        query = row[\"query\"]\n",
    "        tydiqa_data = tydiqa_gold_dict.get(query, None)  # Ambil data dari tydiqa_gold jika ada\n",
    "        \n",
    "        # Gabungkan data (jika tidak ada di tydiqa_gold, biarkan bagian tersebut kosong)\n",
    "        merged_row = {\n",
    "            **row,  # Data dari mr_tydi_cleaned\n",
    "            \"tydiqa_id\": tydiqa_data[\"tydiqa_id\"] if tydiqa_data else None,\n",
    "            \"answers\": tydiqa_data[\"answers\"] if tydiqa_data else None\n",
    "        }\n",
    "        \n",
    "        new_split_data.append(merged_row)\n",
    "\n",
    "    # Konversi kembali ke Dataset\n",
    "    joined_datasets[split] = mr_tydi_split.from_list(new_split_data)\n",
    "\n",
    "# Simpan hasil sebagai DatasetDict\n",
    "merged_dataset = DatasetDict(joined_datasets)\n",
    "\n",
    "print(\"âœ… Dataset berhasil digabungkan berdasarkan `query` dengan struktur mengikuti `mr_tydi_cleaned`.\")\n",
    "# print(\"âœ… Dataset telah disimpan di 'generated_data/mr_tydi_tydiqa_joined'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f499eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4902/4902 [00:02<00:00, 2162.69 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1224/1224 [00:00<00:00, 14877.13 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829/829 [00:00<00:00, 16170.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Semua row dengan 'answers = None' telah dihapus dari dataset baru `finished_dataset`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Fungsi untuk menghapus rows dengan answers = None\n",
    "def remove_none_answers(dataset):\n",
    "    return dataset.filter(lambda row: row[\"answers\"] is not None)\n",
    "\n",
    "# Buat dataset baru tanpa row yang memiliki answers = None\n",
    "merged_dataset = DatasetDict({\n",
    "    \"train\": remove_none_answers(merged_dataset[\"train\"]),\n",
    "    \"dev\": remove_none_answers(merged_dataset[\"dev\"]),\n",
    "    \"test\": remove_none_answers(merged_dataset[\"test\"])\n",
    "})\n",
    "\n",
    "print(\"âœ… Semua row dengan 'answers = None' telah dihapus dari dataset baru `finished_dataset`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714c2f6",
   "metadata": {},
   "source": [
    "# Cutting Down Negative Passages dari split `Train` agar hanya 2 Passages saja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe211fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer & model untuk Multilingual-E5-Small\n",
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embedding_model = AutoModel.from_pretrained(model_name).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7550abb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [32:12<00:00,  2.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "# Fungsi average pooling\n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "# Fungsi untuk memilih top 4 negative_passages berdasarkan similarity (dari yang awalnya 29-30 negative passages)\n",
    "def select_top2_negative_passages(example):\n",
    "    query_text = f'query: {example[\"query\"]}'\n",
    "    negative_passages = example[\"negative_passages\"]\n",
    "\n",
    "    # Jika sudah <= 2, tidak perlu pemrosesan\n",
    "    if len(negative_passages) <= 2:\n",
    "        return example\n",
    "\n",
    "    # Ambil teks dari negative_passages\n",
    "    neg_texts = [f'passage: {neg[\"title\"]} | {neg['text']}' for neg in negative_passages]\n",
    "\n",
    "    # Tokenisasi dan embedding query serta negative_passages\n",
    "    batch_dict = embedding_tokenizer([query_text] + neg_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    batch_dict = {k: v.to(\"cuda:0\") for k, v in batch_dict.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**batch_dict)\n",
    "\n",
    "    # Hitung embedding dan normalisasi\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)  # Normalisasi untuk cosine similarity\n",
    "\n",
    "    # Hitung similarity scores (query vs negative_passages)\n",
    "    query_embedding = embeddings[0].unsqueeze(0)  # Query ada di indeks pertama\n",
    "    neg_embeddings = embeddings[1:]  # Negative passages setelah query\n",
    "    scores = (query_embedding @ neg_embeddings.T).squeeze(0)  # Cosine similarity\n",
    "\n",
    "    # Ambil indeks top 2 dengan similarity tertinggi\n",
    "    top_indices = torch.argsort(scores, descending=True)[:2]\n",
    "\n",
    "    # Simpan hanya 2 negative_passages terbaik\n",
    "    example[\"negative_passages\"] = [negative_passages[i] for i in top_indices]\n",
    "\n",
    "    # Bersihkan cache GPU setelah query diproses\n",
    "    del batch_dict, outputs, embeddings, scores\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return example\n",
    "\n",
    "# Terapkan fungsi ke split train\n",
    "merged_dataset[\"train\"] = merged_dataset[\"train\"].map(select_top2_negative_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71f9d6",
   "metadata": {},
   "source": [
    "## Hanya mengambil top 3 passages\n",
    "Hal ini bisa dilakukan dengan:\n",
    "1. Jika positive_passages ada 3, maka ambil ketiga-tiganya\n",
    "2. Jika positive_passages ada 2, maka ambil 2 positive_passages dan 1 negative_passages\n",
    "3. Jika positive_passages ada 1, maka ambil 1 postive_passages dan 2 negative_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51579d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [00:00<00:00, 222839.27 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1143/1143 [00:00<00:00, 36016.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [00:00<00:00, 25952.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# post_neg_subset = merged_dataset.select_columns([\"query_id\", \"positive_passages\", \"negative_passages\"]) \n",
    "# post_neg_subset.save_to_disk(\"../generated_data/raw/positive_negative_subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43ced84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [00:00<00:00, 5382.76 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1143/1143 [00:00<00:00, 4348.12 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [00:00<00:00, 4706.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "import random\n",
    "\n",
    "# Fungsi untuk membuat kolom 'top_3_passages'\n",
    "def create_top_3_passages(example):\n",
    "    # Mengambil positive_passages dan negative_passages\n",
    "    positive_passages = example[\"positive_passages\"]\n",
    "    negative_passages = example[\"negative_passages\"]\n",
    "\n",
    "    # Gabungkan 3 passages sesuai dengan aturan yang diinginkan\n",
    "    if len(positive_passages) == 3:\n",
    "        top_3_passages = positive_passages\n",
    "    elif len(positive_passages) == 2:\n",
    "        top_3_passages = positive_passages + [negative_passages[0]]  # Ambil negative pertama\n",
    "    elif len(positive_passages) == 1:\n",
    "        top_3_passages = positive_passages + negative_passages[:2]  # Ambil 2 negative pertama\n",
    "    else:\n",
    "        top_3_passages = []  # Default jika tidak sesuai dengan aturan\n",
    "\n",
    "    example[\"top_3_passages\"] = top_3_passages\n",
    "    return example\n",
    "\n",
    "# Terapkan fungsi ke split train, dev, dan test\n",
    "merged_dataset[\"train\"] = merged_dataset[\"train\"].map(create_top_3_passages)\n",
    "merged_dataset[\"dev\"] = merged_dataset[\"dev\"].map(create_top_3_passages)\n",
    "merged_dataset[\"test\"] = merged_dataset[\"test\"].map(create_top_3_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3937e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename kolom 'answers' menjadi 'answer' di semua split\n",
    "for split in merged_dataset.keys():\n",
    "    merged_dataset[split] = merged_dataset[split].rename_column(\"answers\", \"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03279f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memformat passage agar mengandung string \"Judul:...\\nTeks:...\"\"\n",
    "def format_passages(example, psgs_col='top_3_passages',  title_col='title', text_col='text'):\n",
    "    psgs = example[psgs_col]\n",
    "    formatted_psgs = []\n",
    "    for psg in psgs:\n",
    "        formatted_psgs.append(f\"{psg[title_col]} | {psg[text_col]}\")\n",
    "\n",
    "    example['passages'] = formatted_psgs\n",
    "\n",
    "    return example\n",
    "\n",
    "for split in merged_dataset.keys():\n",
    "    merged_dataset[split] = merged_dataset[split].map(format_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e40e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semua data valid. Melanjutkan penyimpanan dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [00:00<00:00, 182151.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1143/1143 [00:00<00:00, 93957.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [00:00<00:00, 58312.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def check_top_3_passages(example):\n",
    "    top_3_passages = example[\"top_3_passages\"]\n",
    "    \n",
    "    # Jika top_3_passages kosong atau tidak memiliki 3 elemen, beri peringatan\n",
    "    if not top_3_passages or len(top_3_passages) != 3:\n",
    "        print(f\"Peringatan: top_3_passages untuk query_id {example['query_id']} tidak lengkap.\")\n",
    "        return False  # Mengembalikan False untuk menandakan bahwa data ini tidak valid\n",
    "    \n",
    "    return True  # Jika valid, kembalikan True\n",
    "\n",
    "# Terapkan fungsi pengecekan ke split train, dev, dan test\n",
    "train_valid = [check_top_3_passages(example) for example in merged_dataset[\"train\"]]\n",
    "dev_valid = [check_top_3_passages(example) for example in merged_dataset[\"dev\"]]\n",
    "test_valid = [check_top_3_passages(example) for example in merged_dataset[\"test\"]]\n",
    "\n",
    "# Jika ada data yang invalid, beri tahu dan hentikan eksekusi\n",
    "if not all(train_valid) or not all(dev_valid) or not all(test_valid):\n",
    "    print(\"Terdapat data yang tidak valid di salah satu split. Proses dihentikan.\")\n",
    "else:\n",
    "    # Jika semua valid, lanjutkan untuk simpan dataset\n",
    "    print(\"Semua data valid. Melanjutkan penyimpanan dataset.\")\n",
    "    final_dataset = merged_dataset.remove_columns([\"positive_passages\", \"negative_passages, top_3_passages\"])\n",
    "    final_dataset.save_to_disk(\"../generated_data/raw/raw_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f20e4b",
   "metadata": {},
   "source": [
    "# Mengurutkan dan melabeli passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17d706b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "final_dataset = load_dataset('khalidrizki/post-retrieval-research_raw-dataset')\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "126400ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [00:13<00:00, 338.61 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1143/1143 [00:03<00:00, 370.69 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [00:01<00:00, 383.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def truncate_passages(examples):\n",
    "    # Tokenisasi setiap passage dalam kolom 'passages' dan batasi panjangnya menjadi 512 token\n",
    "    truncated_passages = []\n",
    "    for passage in examples['passages']:\n",
    "        # Tokenize each passage and truncate it to 512 tokens\n",
    "        tokenized = t5_tokenizer(passage, padding='max_length', truncation=True, max_length=512, add_special_tokens=False)\n",
    "        \n",
    "        # Decode input_ids menjadi string dan tambahkan ke list truncated_passages\n",
    "        truncated_passages.append(t5_tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))\n",
    "    \n",
    "    examples['trunc_passages'] = truncated_passages\n",
    "    return examples\n",
    "\n",
    "# Terapkan fungsi ke dataset\n",
    "for split in final_dataset.keys():\n",
    "    final_dataset[split] = final_dataset[split].map(truncate_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f40a124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ranked_truncPassages_with_labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [01:20<00:00, 56.71it/s]\n",
      "Processing ranked_truncPassages_with_labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1143/1143 [00:19<00:00, 57.89it/s]\n",
      "Processing ranked_truncPassages_with_labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [00:09<00:00, 57.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import apply_similarity_ranking_to_dataset\n",
    "# Me-rangking passages berdasarkan skor similarity\n",
    "for split in final_dataset.keys():\n",
    "    final_dataset[split] = apply_similarity_ranking_to_dataset(\n",
    "        final_dataset[split], \n",
    "        text_col=\"trunc_passages\",\n",
    "        output_col=\"ranked_truncPassages_with_labels\", \n",
    "        tokenizer=embedding_tokenizer, \n",
    "        model=embedding_model, \n",
    "        device = embedding_model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3854ff",
   "metadata": {},
   "source": [
    "Memindahkan 578 baris dari split dev ke train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c16d374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Tentukan jumlah baris yang ingin dipindahkan\n",
    "num_rows_to_move = 578\n",
    "\n",
    "# Pilih 577 baris acak dari split 'dev'\n",
    "dev_dataset = final_dataset['dev']\n",
    "\n",
    "selected_rows = dev_dataset.select(range(num_rows_to_move))  # Ambil 578 baris pertama setelah shuffle\n",
    "\n",
    "# Hapus 577 baris yang sudah dipilih dari 'dev'\n",
    "remaining_dev = dev_dataset.select(range(num_rows_to_move, len(dev_dataset)))\n",
    "\n",
    "# Konversi ke DataFrame pandas untuk dapat menggunakan concat\n",
    "train_df = final_dataset['train'].to_pandas()\n",
    "selected_rows_df = selected_rows.to_pandas()\n",
    "\n",
    "# Gabungkan keduanya dengan pandas.concat\n",
    "new_train_df = pd.concat([train_df, selected_rows_df], ignore_index=True)\n",
    "\n",
    "# Kembali ke dataset HuggingFace dari DataFrame\n",
    "new_train = Dataset.from_pandas(new_train_df)\n",
    "\n",
    "# Perbarui split train dan dev\n",
    "final_dataset['train'] = new_train\n",
    "final_dataset['dev'] = remaining_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92467d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5120/5120 [00:00<00:00, 5630.19 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [00:00<00:00, 3618.95 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [00:00<00:00, 3861.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels', 'sorted_truncPassages'], 'dev': ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels', 'sorted_truncPassages'], 'test': ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels', 'sorted_truncPassages']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_sorted_passages(row):\n",
    "    # Mengambil ranked_truncPassages_with_labels yang sudah terurut\n",
    "    passages = row['ranked_truncPassages_with_labels']\n",
    "    \n",
    "    # Mengambil teks dari setiap passage\n",
    "    sorted_texts = [passage['text'] for passage in passages]\n",
    "    \n",
    "    return sorted_texts\n",
    "\n",
    "# Menggunakan method map untuk menerapkan fungsi ke setiap row di dataset\n",
    "final_dataset = final_dataset.map(lambda row: {'sorted_truncPassages': extract_sorted_passages(row)}, batched=False)\n",
    "\n",
    "# Memeriksa hasilnya: Pastikan 'sorted_truncPassages' sudah ada di dataset\n",
    "print(final_dataset.column_names)  # Untuk memastikan nama kolom yang tersedia\n",
    "final_dataset = final_dataset.rename_column('answer', 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94bfad",
   "metadata": {},
   "source": [
    "# Push to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a398ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 83.72ba/s]\n",
      "Uploading the dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.94s/it]\n",
      "Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 103.86ba/s]\n",
      "Uploading the dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.89s/it]\n",
      "Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 111.25ba/s]\n",
      "Uploading the dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.48s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/khalidrizki/post-retrieval-research_raw-dataset/commit/2521dd4ed7a0466c17b6e0661c6db86efab7bd61', commit_message='Upload dataset', commit_description='', oid='2521dd4ed7a0466c17b6e0661c6db86efab7bd61', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/khalidrizki/post-retrieval-research_raw-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='khalidrizki/post-retrieval-research_raw-dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset.push_to_hub(\"khalidrizki/postretrieve-raw-dataset-v2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
