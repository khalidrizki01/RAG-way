{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model, Tokenizer, dan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERHASIL MELOAD MODEL DAN DATASET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load ===================================================================================\n",
    "from datasets import load_dataset\n",
    "from utils import load_model_and_tokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\" \n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)\n",
    "dataset = load_dataset(\"khalidrizki/post-retrieval-research_raw-dataset\")\n",
    "print(\"BERHASIL MELOAD MODEL DAN DATASET\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Formatting ============================================================================================================\n",
    "import datasets\n",
    "from functools import partial\n",
    "\n",
    "def format_passages(example, psgs_col):\n",
    "    \"\"\"\n",
    "    Join ketiga passages dengan \\n\\n sebagai penghubung\n",
    "    \"\"\"\n",
    "    \n",
    "    example['formatted_passages'] = \"\\n\\n\".join(example[psgs_col])\n",
    "    return example\n",
    "\n",
    "_format_psgs = partial(\n",
    "    format_passages, \n",
    "    psgs_col = 'passages'\n",
    ")\n",
    "\n",
    "fin_dataset = {}\n",
    "\n",
    "for split in dataset.keys():\n",
    "    fin_dataset[split] = dataset[split].map(_format_psgs)\n",
    "    fin_dataset[split] = fin_dataset[split].remove_columns('passages')\n",
    "\n",
    "fin_dataset = datasets.DatasetDict(fin_dataset)\n",
    "\n",
    "import datasets\n",
    "from functools import partial\n",
    "\n",
    "def format_passages(example, psgs_col):\n",
    "    \"\"\"\n",
    "    Join ketiga passages dengan \\n\\n sebagai penghubung\n",
    "    \"\"\"\n",
    "    \n",
    "    example['formatted_passages'] = \"\\n\\n\".join(example[psgs_col])\n",
    "    return example\n",
    "\n",
    "_format_psgs = partial(\n",
    "    format_passages, \n",
    "    psgs_col = 'passages'\n",
    ")\n",
    "\n",
    "fin_dataset = {}\n",
    "\n",
    "for split in dataset.keys():\n",
    "    fin_dataset[split] = dataset[split].map(_format_psgs)\n",
    "    fin_dataset[split] = fin_dataset[split].remove_columns('passages')\n",
    "\n",
    "fin_dataset = datasets.DatasetDict(fin_dataset)\n",
    "print(\"BERHASIL MEMFORMAT PASSAGES\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate draft summary =============================================================================================\n",
    "from summarize import generate_summary_dataset\n",
    "import time\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Dictionary untuk menyimpan hasil per split\n",
    "processed_splits = {}\n",
    "\n",
    "# Loop untuk setiap split (train, dev, test)\n",
    "for split in fin_dataset.keys():\n",
    "    print(f\"ðŸ”„ Memproses split: {split}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Jalankan rangkuman untuk split tertentu\n",
    "    final_dataset = generate_summary_dataset(\n",
    "        dataset=fin_dataset[split],  # Proses per split\n",
    "        query_col=\"query\",\n",
    "        psgs_col=\"formatted_passages\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=1, \n",
    "        temperature=0, \n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print(f\"âœ… Split {split} selesai dalam {duration:.2f} detik\")\n",
    "\n",
    "    # Simpan hasil per split ke dalam dictionary\n",
    "    processed_splits[split] = final_dataset\n",
    "\n",
    "# Gabungkan kembali hasil per split menjadi DatasetDict\n",
    "dataset_with_draft_summary = DatasetDict(processed_splits)\n",
    "print(\"BERHASIL MEMBUAT DRAFT SUMMARY\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [00:00<00:00, 222776.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1143/1143 [00:00<00:00, 59041.24 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [00:00<00:00, 51965.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Path penyimpanan hasil akhir\n",
    "save_path = \"generated_data/draft_summary_dataset_with_do_sample_False\"\n",
    "\n",
    "# Simpan dataset yang telah digabungkan\n",
    "dataset_with_draft_summary.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Generation with Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, DatasetDict, Dataset\n",
    "from utils import load_model_and_tokenizer\n",
    "\n",
    "# loaded_dataset = load_from_disk(\"./generated_data/draft_summary_dataset\")\n",
    "# model_name = \"Qwen/Qwen3-1.7B\"\n",
    "# model, tokenizer, config = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary for Fine-Tuning ==================================================================================\n",
    "from generate_answer import generate_answers_and_compare_between_with_and_without_summary\n",
    "from datasets import DatasetDict, Dataset\n",
    "import time\n",
    "\n",
    "processed_splits = {}\n",
    "\n",
    "# ðŸ”¹ Mulai proses evaluasi per split\n",
    "for split in dataset_with_draft_summary.keys():\n",
    "    print(f\"ðŸ”„ Memproses split: {split}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    final_dataset = generate_answers_and_compare_between_with_and_without_summary(\n",
    "        dataset=dataset_with_draft_summary[split],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    # ðŸ•’ Waktu eksekusi\n",
    "    print(f\"âœ… Proses selesai dalam {end_time - start_time:.2f} detik\")\n",
    "    processed_splits[split] = Dataset.from_list(final_dataset)\n",
    "\n",
    "fin_dataset = DatasetDict(processed_splits)\n",
    "\n",
    "save_path = \"./generated_data/RECOMP_tuning_w_do_sample_False_and_wo_Judul_nTeks\"\n",
    "fin_dataset.save_to_disk(save_path)\n",
    "\n",
    "print(f\"ðŸ“„ Hasil telah disimpan dalam {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./generated_data/RECOMP-tuning\"\n",
    "ds = fin_dataset.remove_columns(['summary','generated_results'])\n",
    "ds.save_to_disk(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
