{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model, Tokenizer, dan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"../generated_data/raw/fin_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  9.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import load_model_and_tokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\" \n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 19.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from functools import partial\n",
    "\n",
    "from utils import load_model_and_tokenizer\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"../generated_data/raw/fin_dataset\")\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\" \n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)\n",
    "\n",
    "def format_passages(example, psgs_col):\n",
    "    \"\"\"\n",
    "    Join ketiga passages dengan \\n\\n sebagai penghubung\n",
    "    \"\"\"\n",
    "    \n",
    "    example['formatted_passages'] = \"\\n\\n\".join(example[psgs_col])\n",
    "    return example\n",
    "\n",
    "_format_psgs = partial(\n",
    "    format_passages, \n",
    "    psgs_col = 'passages'\n",
    ")\n",
    "\n",
    "processed_dataset = {}\n",
    "\n",
    "for split in dataset.keys():\n",
    "    processed_dataset[split] = dataset[split].map(_format_psgs)\n",
    "\n",
    "processed_dataset = datasets.DatasetDict(processed_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Set print options untuk menampilkan semua elemen tensor\n",
    "# torch.set_printoptions(threshold=torch.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'formatted_passages'],\n",
       "        num_rows: 4542\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'formatted_passages'],\n",
       "        num_rows: 1143\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'formatted_passages'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Summary & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Memproses split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing dataset:   0%|          | 0/4542 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Summarizing dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [5:09:44<00:00,  4.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Split train selesai dalam 18585.36 detik\n",
      "ðŸ”„ Memproses split: dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1143/1143 [1:17:18<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Split dev selesai dalam 4638.98 detik\n",
      "ðŸ”„ Memproses split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [37:41<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Split test selesai dalam 2261.74 detik\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [00:00<00:00, 121556.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1143/1143 [00:00<00:00, 110557.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [00:00<00:00, 52817.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from summarize import generate_summary_dataset\n",
    "import time\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Dictionary untuk menyimpan hasil per split\n",
    "processed_splits = {}\n",
    "\n",
    "# Loop untuk setiap split (train, dev, test)\n",
    "for split in processed_dataset.keys():\n",
    "    print(f\"ðŸ”„ Memproses split: {split}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Jalankan rangkuman untuk split tertentu\n",
    "    processed_split = generate_summary_dataset(\n",
    "        dataset=processed_dataset[split],  # Proses per split\n",
    "        query_col=\"query\",\n",
    "        psgs_col=\"formatted_passages\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print(f\"âœ… Split {split} selesai dalam {duration:.2f} detik\")\n",
    "\n",
    "    # Simpan hasil per split ke dalam dictionary\n",
    "    processed_splits[split] = processed_split\n",
    "\n",
    "# Gabungkan kembali hasil per split menjadi DatasetDict\n",
    "final_dataset = DatasetDict(processed_splits)\n",
    "# Path penyimpanan hasil akhir\n",
    "save_path = \"generated_data/draft_summary_dataset\"\n",
    "\n",
    "# Simpan dataset yang telah digabungkan\n",
    "final_dataset.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Generation with Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from generate_answer import generate_answers_and_evaluate\n",
    "from datasets import load_from_disk, DatasetDict, Dataset\n",
    "from utils import load_model_and_tokenizer\n",
    "import json\n",
    "import time\n",
    "\n",
    "loaded_dataset = load_from_disk(\"./generated_data/draft_summary_dataset\")\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat dataset latihan (tuning) dimana jika:\n",
    "1. baris yang dengan-rangkuman menghasilkan jawaban lebih baik (EM lebih besar atau F1 lebih besar) maka summary disimpan sebagai kolom final_summary\n",
    "2. baris yang tanpa-rangkuman menghasilkan jawaban lebih baik, maka string kosong (\"\") ditambahkan ke final_summary\n",
    "Untuk melihat baris yang memenuhi kondisi 2, bisa mengecek melalui EDA-tydiqa.ipynb (cell-cell terakhir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Memproses split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses:   0%|          | 0/4542 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Generating responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [7:54:47<00:00,  6.27s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Proses selesai dalam 28487.57 detik\n",
      "ðŸ”„ Memproses split: dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1143/1143 [1:59:48<00:00,  6.29s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Proses selesai dalam 7188.78 detik\n",
      "ðŸ”„ Memproses split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [58:07<00:00,  6.17s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Proses selesai dalam 3487.64 detik\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [00:00<00:00, 130544.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1143/1143 [00:00<00:00, 93871.07 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [00:00<00:00, 71981.71 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Hasil telah disimpan dalam ./generated_data/RECOMP_tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processed_splits = {}\n",
    "\n",
    "# ðŸ”¹ Mulai proses evaluasi per split\n",
    "for split in loaded_dataset.keys():\n",
    "    print(f\"ðŸ”„ Memproses split: {split}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    processed_split = generate_answers_and_evaluate(\n",
    "        dataset=loaded_dataset[split],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    # ðŸ•’ Waktu eksekusi\n",
    "    print(f\"âœ… Proses selesai dalam {end_time - start_time:.2f} detik\")\n",
    "    processed_splits[split] = Dataset.from_list(processed_split)\n",
    "\n",
    "processed_dataset = DatasetDict(processed_splits)\n",
    "\n",
    "save_path = \"./generated_data/RECOMP_tuning\"\n",
    "processed_dataset.save_to_disk(save_path)\n",
    "\n",
    "print(f\"ðŸ“„ Hasil telah disimpan dalam {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
