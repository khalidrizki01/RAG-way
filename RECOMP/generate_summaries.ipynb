{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model, Tokenizer, dan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"../generated_data/raw/fin_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import load_model_and_tokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\" \n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 19.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from functools import partial\n",
    "\n",
    "from utils import load_model_and_tokenizer\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"../generated_data/raw/fin_dataset\")\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\" \n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)\n",
    "\n",
    "def format_passages(example, psgs_col):\n",
    "    \"\"\"\n",
    "    Join ketiga passages dengan \\n\\n sebagai penghubung\n",
    "    \"\"\"\n",
    "    \n",
    "    example['formatted_passages'] = \"\\n\\n\".join(example[psgs_col])\n",
    "    return example\n",
    "\n",
    "_format_psgs = partial(\n",
    "    format_passages, \n",
    "    psgs_col = 'passages'\n",
    ")\n",
    "\n",
    "processed_dataset = {}\n",
    "\n",
    "for split in dataset.keys():\n",
    "    processed_dataset[split] = dataset[split].map(_format_psgs)\n",
    "\n",
    "processed_dataset = datasets.DatasetDict(processed_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Set print options untuk menampilkan semua elemen tensor\n",
    "# torch.set_printoptions(threshold=torch.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'formatted_passages'],\n",
       "        num_rows: 4542\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'formatted_passages'],\n",
       "        num_rows: 1143\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'formatted_passages'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Summary & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Memproses split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing dataset:   0%|          | 0/4542 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Summarizing dataset: 100%|██████████| 4542/4542 [5:09:44<00:00,  4.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split train selesai dalam 18585.36 detik\n",
      "🔄 Memproses split: dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing dataset: 100%|██████████| 1143/1143 [1:17:18<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split dev selesai dalam 4638.98 detik\n",
      "🔄 Memproses split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing dataset: 100%|██████████| 565/565 [37:41<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split test selesai dalam 2261.74 detik\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4542/4542 [00:00<00:00, 121556.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1143/1143 [00:00<00:00, 110557.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 565/565 [00:00<00:00, 52817.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from summarize import generate_summary_dataset\n",
    "import time\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Dictionary untuk menyimpan hasil per split\n",
    "processed_splits = {}\n",
    "\n",
    "# Loop untuk setiap split (train, dev, test)\n",
    "for split in processed_dataset.keys():\n",
    "    print(f\"🔄 Memproses split: {split}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Jalankan rangkuman untuk split tertentu\n",
    "    processed_split = generate_summary_dataset(\n",
    "        dataset=processed_dataset[split],  # Proses per split\n",
    "        query_col=\"query\",\n",
    "        psgs_col=\"formatted_passages\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print(f\"✅ Split {split} selesai dalam {duration:.2f} detik\")\n",
    "\n",
    "    # Simpan hasil per split ke dalam dictionary\n",
    "    processed_splits[split] = processed_split\n",
    "\n",
    "# Gabungkan kembali hasil per split menjadi DatasetDict\n",
    "final_dataset = DatasetDict(processed_splits)\n",
    "# Path penyimpanan hasil akhir\n",
    "save_path = \"generated_data/draft_summary_dataset\"\n",
    "\n",
    "# Simpan dataset yang telah digabungkan\n",
    "final_dataset.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Generation with Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 16.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from generate_answer import generate_answers_and_evaluate\n",
    "from datasets import load_from_disk, DatasetDict, Dataset\n",
    "from utils import load_model_and_tokenizer\n",
    "import json\n",
    "import time\n",
    "\n",
    "loaded_dataset = load_from_disk(\"./generated_data/draft_summary_dataset\")\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Memproses split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses:   0%|          | 0/4542 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Generating responses: 100%|██████████| 4542/4542 [7:54:47<00:00,  6.27s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Proses selesai dalam 28487.57 detik\n",
      "🔄 Memproses split: dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 1143/1143 [1:59:48<00:00,  6.29s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Proses selesai dalam 7188.78 detik\n",
      "🔄 Memproses split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 565/565 [58:07<00:00,  6.17s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Proses selesai dalam 3487.64 detik\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4542/4542 [00:00<00:00, 130544.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1143/1143 [00:00<00:00, 93871.07 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 565/565 [00:00<00:00, 71981.71 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Hasil telah disimpan dalam ./generated_data/RECOMP_tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processed_splits = {}\n",
    "\n",
    "# 🔹 Mulai proses evaluasi per split\n",
    "for split in loaded_dataset.keys():\n",
    "    print(f\"🔄 Memproses split: {split}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    processed_split = generate_answers_and_evaluate(\n",
    "        dataset=loaded_dataset[split],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    # 🕒 Waktu eksekusi\n",
    "    print(f\"✅ Proses selesai dalam {end_time - start_time:.2f} detik\")\n",
    "    processed_splits[split] = Dataset.from_list(processed_split)\n",
    "\n",
    "processed_dataset = DatasetDict(processed_splits)\n",
    "\n",
    "save_path = \"./generated_data/RECOMP_tuning\"\n",
    "processed_dataset.save_to_disk(save_path)\n",
    "\n",
    "# # 📂 Simpan hasil berdasarkan ukuran model\n",
    "# output_file = f\"./generated_data/evaluated_summary_result_1B_FOR_TUNING.json\"\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(processed_splits, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"📄 Hasil telah disimpan dalam {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di bawah ga penting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Perbandingan superior_by_having_no_summary ===\n",
      "\n",
      "📂 Split: train\n",
      "🔹 Jumlah superior_by_having_no_summary di 1B: 4\n",
      "🔹 Jumlah superior_by_having_no_summary di 3B: 5\n",
      "\n",
      "📌 Query dari row yang superior_by_having_no_summary di dataset 1B:\n",
      "   - Perusahaan apa yang membuat Accuracy International Arctic Warfare?\n",
      "   - darimanakah taekwondo berasal?\n",
      "   - apakah nama ibukota Argentina?\n",
      "   - berapakah luas Spanyol ?\n",
      "\n",
      "📌 Query dari row yang superior_by_having_no_summary di dataset 3B:\n",
      "   - Perusahaan apa yang membuat Accuracy International Arctic Warfare?\n",
      "   - darimanakah taekwondo berasal?\n",
      "   - Siapa ibu Yesus kristus?\n",
      "   - apakah nama ibukota Argentina?\n",
      "   - Kapan hari kemerdekaan Kamboja\n",
      "\n",
      "============================================================\n",
      "\n",
      "📂 Split: dev\n",
      "🔹 Jumlah superior_by_having_no_summary di 1B: 0\n",
      "🔹 Jumlah superior_by_having_no_summary di 3B: 1\n",
      "\n",
      "📌 Query dari row yang superior_by_having_no_summary di dataset 1B:\n",
      "\n",
      "📌 Query dari row yang superior_by_having_no_summary di dataset 3B:\n",
      "   - Ada berapa bulan dalam tahun Hijriah ?\n",
      "\n",
      "============================================================\n",
      "\n",
      "📂 Split: test\n",
      "🔹 Jumlah superior_by_having_no_summary di 1B: 0\n",
      "🔹 Jumlah superior_by_having_no_summary di 3B: 2\n",
      "\n",
      "📌 Query dari row yang superior_by_having_no_summary di dataset 1B:\n",
      "\n",
      "📌 Query dari row yang superior_by_having_no_summary di dataset 3B:\n",
      "   - Bagaimana sistem pemerintahan Jepang?\n",
      "   - dimanakah kantor pusat Airbus S.A.S?\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 📂 Load kedua dataset JSON\n",
    "with open(\"./generated_data/SAMPEL_evaluated_summary_result_1B_latest.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset_1B = json.load(f)\n",
    "\n",
    "# with open(\"./generated_data/SAMPEL_evaluated_summary_result_3B_latest.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     dataset_3B = json.load(f)\n",
    "\n",
    "# 🔄 Loop melalui setiap split (train, dev, test)\n",
    "superior_counts_1B = {}\n",
    "# superior_counts_3B = {}\n",
    "superior_queries_1B = {}  # Menyimpan query dari row yang superior di 1B\n",
    "# superior_queries_3B = {}  # Menyimpan query dari row yang superior di 3B\n",
    "\n",
    "for split in dataset_1B.keys():\n",
    "    # 🔍 Cek superior_by_having_no_summary untuk masing-masing dataset\n",
    "    superior_1B = []\n",
    "    superior_3B = []\n",
    "\n",
    "    # 🔍 Proses dataset 1B\n",
    "    for idx, row in enumerate(dataset_1B[split]):\n",
    "        w_summary = row[\"generated_results\"][\"w_summary\"]\n",
    "        wo_summary = row[\"generated_results\"][\"wo_summary\"]\n",
    "\n",
    "        # 1️⃣ Cek apakah EM dari wo_summary bernilai 1, jika iya, tambahkan jika >= EM dari w_summary\n",
    "        if wo_summary[\"em\"] == 1 and wo_summary[\"em\"] >= w_summary[\"em\"]:\n",
    "            superior_1B.append(row[\"query\"])\n",
    "        # 2️⃣ Jika kondisi di atas tidak terpenuhi, cek apakah F1 dari w_summary < wo_summary\n",
    "        elif w_summary[\"f1\"] < wo_summary[\"f1\"]:\n",
    "            superior_1B.append(row[\"query\"])\n",
    "\n",
    "    # # 🔍 Proses dataset 3B\n",
    "    # for idx, row in enumerate(dataset_3B[split]):\n",
    "    #     w_summary = row[\"generated_results\"][\"w_summary\"]\n",
    "    #     wo_summary = row[\"generated_results\"][\"wo_summary\"]\n",
    "\n",
    "    #     # 1️⃣ Cek apakah EM dari wo_summary bernilai 1, jika iya, tambahkan jika >= EM dari w_summary\n",
    "    #     if wo_summary[\"em\"] == 1 and wo_summary[\"em\"] >= w_summary[\"em\"]:\n",
    "    #         superior_3B.append(row[\"query\"])\n",
    "    #     # 2️⃣ Jika kondisi di atas tidak terpenuhi, cek apakah F1 dari w_summary < wo_summary\n",
    "    #     elif w_summary[\"f1\"] < wo_summary[\"f1\"]:\n",
    "    #         superior_3B.append(row[\"query\"])\n",
    "\n",
    "    # 🔄 Simpan jumlah row yang superior_by_having_no_summary\n",
    "    superior_counts_1B[split] = len(superior_1B)\n",
    "    # superior_counts_3B[split] = len(superior_3B)\n",
    "\n",
    "    # Simpan query-query yang memenuhi kondisi\n",
    "    superior_queries_1B[split] = superior_1B\n",
    "    # superior_queries_3B[split] = superior_3B\n",
    "\n",
    "# 📢 Output hasil analisis\n",
    "print(\"=== Perbandingan superior_by_having_no_summary ===\")\n",
    "for split in dataset_1B.keys():\n",
    "    print(f\"\\n📂 Split: {split}\")\n",
    "    print(f\"🔹 Jumlah superior_by_having_no_summary di 1B: {superior_counts_1B[split]}\")\n",
    "    # print(f\"🔹 Jumlah superior_by_having_no_summary di 3B: {superior_counts_3B[split]}\")\n",
    "\n",
    "    # Tampilkan query dari dataset 1B\n",
    "    print(f\"\\n📌 Query dari row yang superior_by_having_no_summary di dataset 1B:\")\n",
    "    for query in superior_queries_1B[split]:\n",
    "        print(f\"   - {query}\")\n",
    "\n",
    "    # # Tampilkan query dari dataset 3B\n",
    "    # print(f\"\\n📌 Query dari row yang superior_by_having_no_summary di dataset 3B:\")\n",
    "    # for query in superior_queries_3B[split]:\n",
    "    #     print(f\"   - {query}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (untuk diisi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
