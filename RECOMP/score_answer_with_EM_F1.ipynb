{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12fc44b",
   "metadata": {},
   "source": [
    "# Generate Answer based on T5 Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6ac70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the dataset since khalidrizki/RECOMP-tuning-truncated couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\LENOVO\\.cache\\huggingface\\datasets\\khalidrizki___recomp-tuning-truncated\\default\\0.0.0\\0af0dd211ef16e1ecae2ee0d4b857744a1ad1e05 (last modified on Fri Jun 13 11:24:20 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "dataset = load_dataset(\"khalidrizki/RECOMP-tuning-truncated\")\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78638b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import load_model_and_tokenizer\n",
    "model_name='Qwen/Qwen3-1.7B'\n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d6f97",
   "metadata": {},
   "source": [
    "## Dataset terbaru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef735e3",
   "metadata": {},
   "source": [
    "### Selective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f541b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muat baris-baris teks dari file\n",
    "with open(\"./outputs/FINAL-seeded-truncated-khalidrizki-RECOMP-selective-final-2025-06-13_11-23-00/generated_predictions.txt\", \"r\", encoding=\"utf-8\", errors='replace') as f:\n",
    "    predictions = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Pastikan jumlah prediksi sama dengan jumlah data\n",
    "assert len(predictions) == len(test_data), f\"Jumlah prediksi ({len(predictions)}) tidak sama dengan jumlah data ({len(test_data)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e92c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses with summary only:   0%|          | 0/565 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Generating responses with summary only: 100%|██████████| 565/565 [26:58<00:00,  2.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# Tambahkan kolom ke dataset\n",
    "test_data = test_data.add_column(\"T5_summary\", predictions)\n",
    "\n",
    "from generate_answer import generate_answer_and_do_scoring\n",
    "\n",
    "answer_based_on_t5_summary = generate_answer_and_do_scoring(\n",
    "    test_data, \n",
    "    query_col='query', \n",
    "    filtered_contexts_col=\"T5_summary\", \n",
    "    label_col='answer', \n",
    "    original_passages_col='passages', \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=52\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "258ca2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.3663716814159292\n",
      "F1: 0.2368892156786773\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "result = Dataset.from_list(answer_based_on_t5_summary)\n",
    "em_avg = sum(result['em'])/len(result)\n",
    "f1_avg =  sum(result['f1'])/len(result)\n",
    "print(\"EM:\" ,em_avg)\n",
    "print(\"F1:\", f1_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52b34e",
   "metadata": {},
   "source": [
    "### Unselective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9206c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses with summary only: 100%|██████████| 565/565 [24:46<00:00,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.4230088495575221\n",
      "F1: 0.25677911467294434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Muat baris-baris teks dari file\n",
    "with open(\"./outputs/FINAL-seeded-truncated-khalidrizki-RECOMP-unselective-final-2025-06-13_10-51-32/generated_predictions.txt\", \"r\", encoding=\"utf-8\", errors='replace') as f:\n",
    "    predictions_unselective = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Pastikan jumlah prediksi sama dengan jumlah data\n",
    "assert len(predictions_unselective) == len(test_data), f\"Jumlah prediksi ({len(predictions_unselective)}) tidak sama dengan jumlah data ({len(test_data)})\"\n",
    "\n",
    "# Tambahkan kolom ke dataset\n",
    "test_data = test_data.add_column(\"unselective_summary\", predictions_unselective)\n",
    "\n",
    "from generate_answer import generate_answer_and_do_scoring\n",
    "\n",
    "answer_based_on_unselective_summary = generate_answer_and_do_scoring(\n",
    "    test_data, \n",
    "    query_col='query', \n",
    "    filtered_contexts_col=\"unselective_summary\", \n",
    "    label_col='answer', \n",
    "    original_passages_col='passages', \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=52\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "result_uns = Dataset.from_list(answer_based_on_unselective_summary)\n",
    "em_uns_avg = sum(result_uns['em'])/len(result_uns)\n",
    "f1_uns_avg =  sum(result_uns['f1'])/len(result_uns)\n",
    "print(\"EM:\" ,em_uns_avg)\n",
    "print(\"F1:\", f1_uns_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca6bca5",
   "metadata": {},
   "source": [
    "### RAG Konvensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c9846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses with summary only: 100%|██████████| 565/565 [28:18<00:00,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL RAG BIASA\n",
      "rerata EM: 0.6300884955752213\n",
      "rerata F1: 0.31540144816316523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from generate_answer import generate_answer_and_do_scoring\n",
    "answer_based_on_passages = generate_answer_and_do_scoring(\n",
    "    test_data, \n",
    "    query_col='query', \n",
    "    filtered_contexts_col=\"passages\", \n",
    "    label_col='answer', \n",
    "    original_passages_col='passages', \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=52, \n",
    "    max_source_length = 512\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "normal_RAG = Dataset.from_list(answer_based_on_passages)\n",
    "print(\"HASIL RAG BIASA\")\n",
    "print(\"rerata EM:\", sum(normal_RAG['em'])/len(normal_RAG))\n",
    "print(\"rerata F1:\", sum(normal_RAG['f1'])/len(normal_RAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b9cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Simpan hasil jawaban berdasarkan T5 summary (selective)\n",
    "with open(\"outputs/test_generated_answers/selective_generated_answers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(answer_based_on_t5_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Simpan hasil jawaban berdasarkan unselective summary\n",
    "with open(\"outputs/test_generated_answers/unselective_generated_answers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(answer_based_on_unselective_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Simpan hasil jawaban berdasarkan original passages (normal RAG)\n",
    "with open(\"outputs/test_generated_answers/normal_rag_generated_answer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(answer_based_on_passages, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612a679",
   "metadata": {},
   "source": [
    "## Dataset usang: \n",
    "Pembahasan di bawah adalah hasil eksekusi model pada dataset yang mana query bisa jadi tidak masuk ke prompt. Query bisa jadi tidak masuk kek prompt karena adanya truncation untuk maksimal 512 token, yang mana passages dan instruksi saja bisa mencapai batas tersebut  bahkan sebelum query diselipkan ke dalam prompt. Oleh karena itu, pada dataset terbaru, passages di-truncate thd ukuran `512 - jumlah token dari (instruksi+query)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626b542",
   "metadata": {},
   "source": [
    "Di bawah, aku akan melakukan pengujian terhadap beberapa kasus berbeda:\n",
    "1. Model yg dilatih thd dataset yg telah di-seed dan dataset latihan tsb ada seleksinya `(khalidrizki/T5base-RECOMP-seededDataset-withSelection)`\n",
    "2. Model yg dilatih thd dataset yg telah di-seed dan dataset latihan tsb tiada seleksi\n",
    "3. Model yg dilatih thd dataset yg belum di-seed dan dataset latihan tsb ada seleksi\n",
    "4. Model yg dilatih thd dataset yg belum di-seed dan dataset latihan tsb tiada seleksi `(khalidrizki/T5base-RECOMP-unseedDataset-noSelection)`\n",
    "\n",
    "Perangkuman oleh kompresor sudah dijalankan dengan command `python train_summarizer.py --model_name_or_path khalidrizki/{TERGANTUNG PAKAI MODEL APA} --do_predict --dataset_name khalidrizki/RECOMP-tuning --max_target_length 52 --output_dir ./outputs/ --per_device_eval_batch_size=32 --predict_with_generate --summary_column final_summary --seed 42`\n",
    "\n",
    "Seharusnya: antar model yg sama2 melakukan with/no seleksi (tetapi hanya dibedakan oleh seed dataset saja) akan memiliki skor yang mirip. \n",
    "\n",
    "__Ukuran dataset kurang?__: antar model yg sama-sama un/seeded (tetapi berbeda pada diterapkan atau tidaknya seleksi), model yg dgn seleksi seharusnya lebih baik. Kalau tidak lebih baik, berarti ukuran dataset kurang. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2451df",
   "metadata": {},
   "source": [
    "Idealnya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ddc59e",
   "metadata": {},
   "source": [
    "### T5base-RECOMP-seededDataset-withSelection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93da710",
   "metadata": {},
   "source": [
    "Sebelum menjalankan generasi jawaban dengan rangkuman hasil fine tuning T5, T5 sudah melakukan generasi rangkuman ketika menjalankan train_summarizer.py dengan command `python train_summarizer.py --do_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee605d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Jumlah prediksi (564) tidak sama dengan jumlah data (565)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Pastikan jumlah prediksi sama dengan jumlah data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predictions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_data), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJumlah prediksi (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) tidak sama dengan jumlah data (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Jumlah prediksi (564) tidak sama dengan jumlah data (565)"
     ]
    }
   ],
   "source": [
    "# Muat baris-baris teks dari file\n",
    "with open(\"./outputs/-khalidrizki-T5base-RECOMP-seededDataset-withSelection_on-recompTuning_hub_dataset/generated_predictions.txt\", \"r\", encoding=\"utf-8\", errors='replace') as f:\n",
    "    predictions = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Pastikan jumlah prediksi sama dengan jumlah data\n",
    "assert len(predictions) == len(test_data), f\"Jumlah prediksi ({len(predictions)}) tidak sama dengan jumlah data ({len(test_data)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4223e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumlah prediksi kurang, meng-append string kosong (\"\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses with summary only:   0%|          | 0/565 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Generating responses with summary only: 100%|██████████| 565/565 [30:59<00:00,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "while len(predictions) < len(test_data):\n",
    "    print('jumlah prediksi kurang, meng-append string kosong (\"\")')\n",
    "    predictions.append(\"\")\n",
    "\n",
    "# Pastikan jumlah prediksi sama dengan jumlah data\n",
    "assert len(predictions) == len(test_data), f\"Jumlah prediksi ({len(predictions)}) tidak sama dengan jumlah data ({len(test_data)})\"\n",
    "\n",
    "# Tambahkan kolom ke dataset\n",
    "test_data = test_data.add_column(\"T5_summary\", predictions)\n",
    "\n",
    "from generate_answer import generate_answer_and_do_scoring\n",
    "\n",
    "answer_based_on_t5_summary = generate_answer_and_do_scoring(\n",
    "    test_data, \n",
    "    query_col='query', \n",
    "    filtered_contexts_col=\"T5_summary\", \n",
    "    label_col='answer', \n",
    "    original_passages_col='passages', \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=52\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "854bd1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.10973451327433628\n",
      "F1: 0.08678890872044633\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "result = Dataset.from_list(answer_based_on_t5_summary)\n",
    "em_avg = sum(result['em'])/len(result)\n",
    "f1_avg =  sum(result['f1'])/len(result)\n",
    "print(\"EM:\" ,em_avg)\n",
    "print(\"F1:\", f1_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7500a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11504424778761062"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "em_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da279cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0948697836281932"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c4554",
   "metadata": {},
   "source": [
    "### T5base-RECOMP-unseedDataset-noSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f5e249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses with summary only:   0%|          | 0/565 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating responses with summary only: 100%|██████████| 565/565 [26:12<00:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.4247787610619469\n",
      "F1: 0.2505083083585362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Muat baris-baris teks dari file\n",
    "with open(\"./outputs/-khalidrizki-T5base-RECOMP-unseedDataset-noSelection_on-recompTuning_hub_dataset/generated_predictions.txt\", \"r\", encoding=\"utf-8\", errors='replace') as f:\n",
    "    predictions_unselective = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Pastikan jumlah prediksi sama dengan jumlah data\n",
    "assert len(predictions_unselective) == len(test_data), f\"Jumlah prediksi ({len(predictions_unselective)}) tidak sama dengan jumlah data ({len(test_data)})\"\n",
    "\n",
    "# Tambahkan kolom ke dataset\n",
    "test_data = test_data.add_column(\"unselective_summary\", predictions_unselective)\n",
    "\n",
    "from generate_answer import generate_answer_and_do_scoring\n",
    "\n",
    "answer_based_on_unselective_summary = generate_answer_and_do_scoring(\n",
    "    test_data, \n",
    "    query_col='query', \n",
    "    filtered_contexts_col=\"unselective_summary\", \n",
    "    label_col='answer', \n",
    "    original_passages_col='passages', \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=52\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "result_uns = Dataset.from_list(answer_based_on_unselective_summary)\n",
    "em_uns_avg = sum(result_uns['em'])/len(result_uns)\n",
    "f1_uns_avg =  sum(result_uns['f1'])/len(result_uns)\n",
    "print(\"EM:\" ,em_uns_avg)\n",
    "print(\"F1:\", f1_uns_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed06fa",
   "metadata": {},
   "source": [
    "### RAG konvensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1dff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses with summary only: 100%|██████████| 565/565 [35:01<00:00,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL RAG BIASA\n",
      "rerata EM: 0.32035398230088497\n",
      "rerata F1: 0.1765652985106173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from generate_answer import generate_answer_and_do_scoring\n",
    "answer_based_on_passages = generate_answer_and_do_scoring(\n",
    "    test_data, \n",
    "    query_col='query', \n",
    "    filtered_contexts_col=\"passages\", \n",
    "    label_col='answer', \n",
    "    original_passages_col='passages', \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=52, \n",
    "    max_source_length = 512\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "normal_RAG = Dataset.from_list(answer_based_on_passages)\n",
    "print(\"HASIL RAG BIASA\")\n",
    "print(\"rerata EM:\", sum(normal_RAG['em'])/len(normal_RAG))\n",
    "print(\"rerata F1:\", sum(normal_RAG['f1'])/len(normal_RAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c9ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
