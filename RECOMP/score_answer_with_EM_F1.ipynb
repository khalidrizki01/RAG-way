{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf145be",
   "metadata": {},
   "source": [
    "# Cek Apakah passage masih memiliki \\n\\n\n",
    "\\n\\n adalah separator antara passage, yg mana jika tidak ada berarti model llm hanya menerima input teks berupa passage pertama saja, dan bukan passage kedua & ketika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd1a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset dan tokenizer\n",
    "dataset_name = \"khalidrizki//RECOMP-tuning\"\n",
    "model_name = \"./models/-google-flan-t5-base-2025-06-09_19-36-13\"\n",
    "dataset = load_from_disk(dataset_name)\n",
    "test_data = dataset[\"test\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Inisialisasi\n",
    "decoded_passages = []\n",
    "pipe_counts = []\n",
    "multi_pipe_count = 0\n",
    "single_or_none_pipe_count = 0\n",
    "\n",
    "# Proses setiap passage\n",
    "for passage in tqdm(test_data[\"passages\"]):\n",
    "    # Tokenisasi dan truncation\n",
    "    tokenized = tokenizer(\n",
    "        passage,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    decoded = tokenizer.decode(tokenized[\"input_ids\"][0], skip_special_tokens=True)\n",
    "    decoded_passages.append(decoded)\n",
    "\n",
    "    # Hitung jumlah karakter pipe\n",
    "    pipe_count = decoded.count(\"|\")\n",
    "    pipe_counts.append(pipe_count)\n",
    "\n",
    "    # Klasifikasi\n",
    "    if pipe_count > 1:\n",
    "        multi_pipe_count += 1\n",
    "    else:\n",
    "        single_or_none_pipe_count += 1\n",
    "\n",
    "# Tambahkan kolom ke dataset\n",
    "test_data = test_data.add_column(\"truncated_decoded_passage\", decoded_passages)\n",
    "test_data = test_data.add_column(\"pipe_count\", pipe_counts)\n",
    "\n",
    "# Hitung total dan persentase\n",
    "total = multi_pipe_count + single_or_none_pipe_count\n",
    "percent_multi = 100 * multi_pipe_count / total\n",
    "percent_single_or_none = 100 * single_or_none_pipe_count / total\n",
    "\n",
    "# Cetak statistik\n",
    "print(f\"Total entries: {total}\")\n",
    "print(f\"Pipe > 1: {multi_pipe_count} ({percent_multi:.2f}%)\")\n",
    "print(f\"Pipe ≤ 1: {single_or_none_pipe_count} ({percent_single_or_none:.2f}%)\")\n",
    "\n",
    "# # Contoh hasil\n",
    "# print(\"\\nContoh hasil:\")\n",
    "# for i in range(3):\n",
    "#     print(f\"\\nOriginal passage:\\n{test_data['passages'][i]}\")\n",
    "#     print(f\"\\nTruncated-decoded passage:\\n{test_data['truncated_decoded_passage'][i]}\")\n",
    "#     print(f\"Jumlah karakter '|': {test_data['pipe_count'][i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 990M/990M [12:59<00:00, 1.27MB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/khalidrizki/T5base-RECOMP-seededDataset-withSelection/commit/0ae16a2cd84260598315c767c2130f589dd45333', commit_message='Upload T5ForConditionalGeneration', commit_description='', oid='0ae16a2cd84260598315c767c2130f589dd45333', pr_url=None, repo_url=RepoUrl('https://huggingface.co/khalidrizki/T5base-RECOMP-seededDataset-withSelection', endpoint='https://huggingface.co', repo_type='model', repo_id='khalidrizki/T5base-RECOMP-seededDataset-withSelection'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "seeded_and_with_selection = \"./models/-google-flan-t5-base-2025-06-09_19-36-13\"\n",
    "t5_seeded_with_selection = AutoModelForSeq2SeqLM.from_pretrained(seeded_and_with_selection)\n",
    "t5_sws_tokenizer = AutoTokenizer.from_pretrained(seeded_and_with_selection)\n",
    "t5_seeded_with_selection.push_to_hub('khalidrizki/T5base-RECOMP-seededDataset-withSelection')\n",
    "t5_sws_tokenizer.push_to_hub('khalidrizki/T5base-RECOMP-seededDataset-withSelection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fc44b",
   "metadata": {},
   "source": [
    "# Generate Answer based on T5 Summary (dgn target=final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e929529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the dataset since khalidrizki/RECOMP-tuning couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\LENOVO\\.cache\\huggingface\\datasets\\khalidrizki___recomp-tuning\\default\\0.0.0\\bb47bf610301d16b9d198e477e8c242d9abcb6c7 (last modified on Wed Jun 11 13:09:44 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 13.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"khalidrizki/RECOMP-tuning\")\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "from utils import load_model_and_tokenizer\n",
    "model_name='Qwen/Qwen3-1.7B'\n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626b542",
   "metadata": {},
   "source": [
    "Di bawah, aku akan melakukan pengujian terhadap beberapa kasus berbeda:\n",
    "1. Model yg dilatih thd dataset yg telah di-seed dan dataset latihan tsb ada seleksinya `(khalidrizki/T5base-RECOMP-seededDataset-withSelection)`\n",
    "2. Model yg dilatih thd dataset yg telah di-seed dan dataset latihan tsb tiada seleksi\n",
    "3. Model yg dilatih thd dataset yg belum di-seed dan dataset latihan tsb ada seleksi\n",
    "4. Model yg dilatih thd dataset yg belum di-seed dan dataset latihan tsb tiada seleksi `(khalidrizki/T5base-RECOMP-unseedDataset-noSelection)`\n",
    "\n",
    "Perangkuman oleh kompresor sudah dijalankan dengan command `python train_summarizer.py --model_name_or_path khalidrizki/{TERGANTUNG PAKAI MODEL APA} --do_predict --dataset_name khalidrizki/RECOMP-tuning --max_target_length 52 --output_dir ./outputs/ --per_device_eval_batch_size=32 --predict_with_generate --summary_column final_summary --seed 42`\n",
    "\n",
    "Seharusnya: antar model yg sama2 melakukan with/no seleksi (tetapi hanya dibedakan oleh seed dataset saja) akan memiliki skor yang mirip. \n",
    "\n",
    "__Ukuran dataset kurang?__: antar model yg sama-sama un/seeded (tetapi berbeda pada diterapkan atau tidaknya seleksi), model yg dgn seleksi seharusnya lebih baik. Kalau tidak lebih baik, berarti ukuran dataset kurang. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2451df",
   "metadata": {},
   "source": [
    "Idealnya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ddc59e",
   "metadata": {},
   "source": [
    "## T5base-RECOMP-seededDataset-withSelection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93da710",
   "metadata": {},
   "source": [
    "Sebelum menjalankan generasi jawaban dengan rangkuman hasil fine tuning T5, T5 sudah melakukan generasi rangkuman ketika menjalankan train_summarizer.py dengan command `python train_summarizer.py --do_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee605d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Jumlah prediksi (564) tidak sama dengan jumlah data (565)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Pastikan jumlah prediksi sama dengan jumlah data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predictions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_data), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJumlah prediksi (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) tidak sama dengan jumlah data (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Jumlah prediksi (564) tidak sama dengan jumlah data (565)"
     ]
    }
   ],
   "source": [
    "# Muat baris-baris teks dari file\n",
    "with open(\"./outputs/-khalidrizki-T5base-RECOMP-seededDataset-withSelection_on-recompTuning_hub_dataset/generated_predictions.txt\", \"r\", encoding=\"utf-8\", errors='replace') as f:\n",
    "    predictions = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Pastikan jumlah prediksi sama dengan jumlah data\n",
    "assert len(predictions) == len(test_data), f\"Jumlah prediksi ({len(predictions)}) tidak sama dengan jumlah data ({len(test_data)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4223e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumlah prediksi kurang, meng-append string kosong (\"\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses with summary only:   0%|          | 0/565 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Generating responses with summary only:   1%|▏         | 8/565 [00:34<40:17,  4.34s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m test_data \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39madd_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictions)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenerate_answer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_answer_and_do_scoring\n\u001b[1;32m---> 13\u001b[0m answer_based_on_t5_summary \u001b[38;5;241m=\u001b[39m generate_answer_and_do_scoring(\n\u001b[0;32m     14\u001b[0m     test_data, \n\u001b[0;32m     15\u001b[0m     query_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     16\u001b[0m     summary_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     17\u001b[0m     label_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     18\u001b[0m     passages_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassages\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[0;32m     20\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, \n\u001b[0;32m     21\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m52\u001b[39m\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\Documents\\Skripsi\\post-retrieval-eval\\RECOMP\\generate_answer.py:195\u001b[0m, in \u001b[0;36mgenerate_answer_and_do_scoring\u001b[1;34m(dataset, query_col, summary_col, label_col, passages_col, model, tokenizer, max_new_tokens, max_source_length)\u001b[0m\n\u001b[0;32m    192\u001b[0m passages \u001b[38;5;241m=\u001b[39m dataset[passages_col][i]\n\u001b[0;32m    194\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKonteks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBerdasarkan konteks sebelumnya, jawab pertanyaan berikut. Pertanyaan: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 195\u001b[0m completion \u001b[38;5;241m=\u001b[39m generate_completion(prompt, model, tokenizer, config\u001b[38;5;241m.\u001b[39mdevice_type, max_new_tokens, max_source_length)\n\u001b[0;32m    196\u001b[0m em, f1 \u001b[38;5;241m=\u001b[39m evaluate_em_f1(completion\u001b[38;5;241m.\u001b[39mstrip(), label\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m    198\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassages\u001b[39m\u001b[38;5;124m\"\u001b[39m: passages,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1,\n\u001b[0;32m    206\u001b[0m })\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\Documents\\Skripsi\\post-retrieval-eval\\RECOMP\\generate_answer.py:105\u001b[0m, in \u001b[0;36mgenerate_completion\u001b[1;34m(prompt, model, tokenizer, device_type, max_new_tokens, max_source_length)\u001b[0m\n\u001b[0;32m    102\u001b[0m inputs \u001b[38;5;241m=\u001b[39m prepare_inputs(format_chat_prompt([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}], tokenizer), tokenizer, device_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenize_kwargs)\n\u001b[0;32m    103\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 105\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    106\u001b[0m     input_ids,\n\u001b[0;32m    107\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[0;32m    108\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    109\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m    110\u001b[0m     return_dict_in_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(input_ids[\u001b[38;5;241m0\u001b[39m]):]\n\u001b[0;32m    113\u001b[0m completion \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\utils.py:2460\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[0;32m   2452\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2453\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2454\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2455\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2456\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2457\u001b[0m     )\n\u001b[0;32m   2459\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2460\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2461\u001b[0m         input_ids,\n\u001b[0;32m   2462\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2463\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2464\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2465\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2466\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2467\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2468\u001b[0m     )\n\u001b[0;32m   2470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2471\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2472\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2473\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2474\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2475\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2476\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2477\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\utils.py:3429\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3427\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3428\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3429\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3431\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3432\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3433\u001b[0m     outputs,\n\u001b[0;32m   3434\u001b[0m     model_kwargs,\n\u001b[0;32m   3435\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3436\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:850\u001b[0m, in \u001b[0;36mQwen3ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    845\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    846\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    849\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 850\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    851\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    852\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    853\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    854\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    855\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    856\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    857\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    858\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    859\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    861\u001b[0m )\n\u001b[0;32m    863\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:576\u001b[0m, in \u001b[0;36mQwen3Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    565\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[0;32m    566\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m         position_embeddings,\n\u001b[0;32m    574\u001b[0m     )\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    577\u001b[0m         hidden_states,\n\u001b[0;32m    578\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    579\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    580\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    581\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    582\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    583\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    584\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    585\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    586\u001b[0m     )\n\u001b[0;32m    588\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:305\u001b[0m, in \u001b[0;36mQwen3DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    304\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 305\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m    306\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    308\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:94\u001b[0m, in \u001b[0;36mQwen3MLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 94\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while len(predictions) < len(test_data):\n",
    "    print('jumlah prediksi kurang, meng-append string kosong (\"\")')\n",
    "    predictions.append(\"\")\n",
    "\n",
    "# Pastikan jumlah prediksi sama dengan jumlah data\n",
    "assert len(predictions) == len(test_data), f\"Jumlah prediksi ({len(predictions)}) tidak sama dengan jumlah data ({len(test_data)})\"\n",
    "\n",
    "# Tambahkan kolom ke dataset\n",
    "test_data = test_data.add_column(\"T5_summary\", predictions)\n",
    "\n",
    "from generate_answer import generate_answer_and_do_scoring\n",
    "\n",
    "answer_based_on_t5_summary = generate_answer_and_do_scoring(\n",
    "    test_data, \n",
    "    query_col='query', \n",
    "    summary_col=\"T5_summary\", \n",
    "    label_col='answer', \n",
    "    passages_col='passages', \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=52\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7500a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11504424778761062"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "result = Dataset.from_list(answer_based_on_t5_summary)\n",
    "em_avg = sum(result['em'])/len(result)\n",
    "f1_avg =  sum(result['f1'])/len(result)\n",
    "print(\"EM:\" ,em_avg)\n",
    "print(\"F1:\", f1_avg)\n",
    "em_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da279cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0948697836281932"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c4554",
   "metadata": {},
   "source": [
    "## T5base-RECOMP-unseedDataset-noSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f5e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "name4 = 'khalidrizki/T5base-RECOMP-unseedDataset-noSelection'\n",
    "unseed_no_selection = AutoModelForSeq2SeqLM.from_pretrained(name4)\n",
    "tokenizer_uns = AutoTokenizer.from_pretrained(name4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae39292",
   "metadata": {},
   "source": [
    "# Comparison (tidak diperlukan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94dab6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris sama    : 465\n",
      "Jumlah baris berbeda : 100\n"
     ]
    }
   ],
   "source": [
    "result_1 = \"./outputs/-.-models--google-flan-t5-base-2025-05-28_07-17-31-2025-05-28_10-26-57/generated_predictions.txt\"\n",
    "result_2 = \"./outputs/-khalidrizki-T5base-RECOMP-unseedDataset-noSelection_on-recompTuning_hub_dataset/generated_predictions.txt\"\n",
    "output_diff = \"perbedaan.txt\"\n",
    "\n",
    "# Baca semua baris dari kedua file\n",
    "with open(result_1, \"r\", encoding=\"utf-8\") as f1, open(result_2, \"r\", encoding=\"utf-8\") as f2:\n",
    "    lines1 = [line.rstrip('\\n') for line in f1]\n",
    "    lines2 = [line.rstrip('\\n') for line in f2]\n",
    "\n",
    "# Pastikan jumlah baris sama\n",
    "min_len = min(len(lines1), len(lines2))\n",
    "same_count = 0\n",
    "diff_count = 0\n",
    "\n",
    "with open(output_diff, \"w\", encoding=\"utf-8\") as out:\n",
    "    for i in range(min_len):\n",
    "        if lines1[i] == lines2[i]:\n",
    "            same_count += 1\n",
    "        else:\n",
    "            diff_count += 1\n",
    "            out.write(f\"[Baris {i+1} berbeda]\\n\")\n",
    "            out.write(f\"result_1: {lines1[i]}\\n\")\n",
    "            out.write(f\"result_2: {lines2[i]}\\n\\n\")\n",
    "\n",
    "    if len(lines1) != len(lines2):\n",
    "        out.write(\"[Jumlah baris berbeda antara kedua file]\\n\")\n",
    "        out.write(f\"Total baris result_1: {len(lines1)}\\n\")\n",
    "        out.write(f\"Total baris result_2: {len(lines2)}\\n\")\n",
    "\n",
    "# Cetak ringkasan\n",
    "print(f\"Jumlah baris sama    : {same_count}\")\n",
    "print(f\"Jumlah baris berbeda : {diff_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad80e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('khalidrizki/RECOMP-tuning')\n",
    "test = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc65954",
   "metadata": {},
   "source": [
    "# Percobaan pada dataset lama (tidak di-seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e3df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "result_of_unseed_dataset_with_no_selective_summarization = (\"./outputs/EM_F1_testing/SCORES_google-flan-t5-base-2025-05-28_07-17-31\")\n",
    "fin = load_dataset(result_of_unseed_dataset_with_no_selective_summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c96b284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42123893805309737"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(fin['em'])/len(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63c4c66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(substringmatch_xcode_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c0e3d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(substringmatch_xcode_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea5f642e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25522817089095584"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(fin['f1'])/len(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7904bccc",
   "metadata": {},
   "source": [
    "## Menguji kinerja jika menggunakan RAG biasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42edeb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_answer import generate_answer_and_do_scoring\n",
    "answer_based_on_passages = generate_answer_and_do_scoring(\n",
    "    test_data, \n",
    "    query_col='query', \n",
    "    summary_col=\"passages\", \n",
    "    label_col='answer', \n",
    "    passages_col='passages', \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=52, \n",
    "    max_source_length = 512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e3beeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL RAG BIASA\n",
      "rerata EM: 0.29557522123893804\n",
      "rerata F1: 0.1659491847867348\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "normal_RAG = Dataset.from_list(answer_based_on_passages)\n",
    "print(\"HASIL RAG BIASA\")\n",
    "print(\"rerata EM:\", sum(normal_RAG['em'])/len(normal_RAG))\n",
    "print(\"rerata F1:\", sum(normal_RAG['f1'])/len(normal_RAG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402105f4",
   "metadata": {},
   "source": [
    "__Terbukti bahwa dengan menggunakan RECOMP kinerja RAG menjadi lebih baik, bisa dilihat dari skor EM dan F1 nya yang lebih tinggi__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcc1cfe",
   "metadata": {},
   "source": [
    "# Push Model ke Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13d95914",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model_path = \"./models/-google-flan-t5-base-2025-05-28_07-17-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b58f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_model_path)\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(t5_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_repo = \"khalidrizki/indonesian-T5-RECOMP\"\n",
    "t5_model.push_to_hub(hub_repo)\n",
    "t5_tokenizer.push_to_hub(hub_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c9ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
