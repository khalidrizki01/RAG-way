{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c57bd03260432c9ac9f1126e6797c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from utils import load_model_and_tokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)\n",
    "finished_dataset = load_from_disk(\"./generated_data/mr_tydi_tydiqa_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902f0f831b5b403999323b8c914963f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8da204b38f43de996be5e08d8345e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdc984b4d7c4f13900a6266dfe5652d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query_id': '0', 'query': 'dimanakah Dr. Ernest François Eugène Douwes Dekker meninggal?', 'positive_passages': [{'docid': '7080#33', 'text': 'Ernest Douwes Dekker wafat dini hari tanggal 28 Agustus 1950 (tertulis di batu nisannya; 29 Agustus 1950 versi van der Veur, 2006) dan dimakamkan di TMP Cikutra, Bandung.', 'title': 'Ernest Douwes Dekker'}], 'negative_passages': [{'docid': '20439#25', 'text': 'Eduard Douwes Dekker kemudian pindah ke Ingelheim am Rhein dekat Sungai Rhein sampai akhirnya meninggal 19 Februari 1887.', 'title': 'Eduard Douwes Dekker'}, {'docid': '7080#0', 'text': 'Dr. Ernest François Eugène Douwes Dekker (umumnya dikenal dengan nama Douwes Dekker atau Danudirja Setiabudi; ) adalah seorang pejuang kemerdekaan dan pahlawan nasional Indonesia.', 'title': 'Ernest Douwes Dekker'}, {'docid': '7080#2', 'text': 'Douwes Dekker terlahir di Pasuruan, Jawa Timur, pada tanggal 8 Oktober 1879, sebagaimana yang dia tulis pada riwayat hidup singkat saat mendaftar di Universitas Zurich, September 1913. Ayahnya, Auguste Henri Edoeard Douwes Dekker, adalah seorang agen di bank kelas kakap Nederlandsch Indisch Escomptobank. Auguste ayahnya, memiliki darah Belanda dari ayahnya, Jan (adik Eduard Douwes Dekker) dan dari ibunya, Louise Bousquet. Sementara itu, ibu Douwes Dekker, Louisa Neumann, lahir di Pekalongan, Jawa Tengah, dari pasangan Jerman-Jawa.[1] Dia terlahir sebagai anak ke-3 dari 4 bersaudara, dan keluarganya pun sering berpindah-pindah. Saudaranya yang perempuan dan laki-laki, yakni Adeline (1876) dan Julius (1878) terlahir sewaktu keluarga Dekker berada di Surabaya, dan adik laki-lakinya lahir di Meester Cornelis, Batavia (sekarang Jatinegara, Jakarta Timur pada tahun 1883. Dari situ, keluarga Dekker berpindah lagi ke Pegangsaan, Jakarta Pusat.[1]', 'title': 'Ernest Douwes Dekker'}, {'docid': '7080#17', 'text': 'jmpl|200px|Universitas Zurich, tempat Ernest Douwes Dekker menempuh pendidikan tingginya. Masa di Eropa dimanfaatkan oleh Nes untuk mengambil program doktor di Universitas Zürich, Swiss, dalam bidang ekonomi. Di sini ia tinggal bersama-sama keluarganya. Gelar doktor diperoleh secara agak kontroversial dan dengan nilai \"serendah-rendahnya\", menurut istilah salah satu pengujinya. Karena di Swis ia terlibat konspirasi dengan kaum revolusioner India, ia ditangkap di Hong Kong dan diadili, kemudian ia ditahan di Singapura (1918). Setelah dua tahun dipenjara, ia pulang ke Hindia Belanda 1920.', 'title': 'Ernest Douwes Dekker'}], 'tydiqa_id': '496955121539262633-33', 'answers': '28 Agustus 1950', 'formatted_passages': 'Judul: Ernest Douwes Dekker \\nTeks: Ernest Douwes Dekker wafat dini hari tanggal 28 Agustus 1950 (tertulis di batu nisannya; 29 Agustus 1950 versi van der Veur, 2006) dan dimakamkan di TMP Cikutra, Bandung.\\n\\nJudul: Eduard Douwes Dekker \\nTeks: Eduard Douwes Dekker kemudian pindah ke Ingelheim am Rhein dekat Sungai Rhein sampai akhirnya meninggal 19 Februari 1887.\\n\\nJudul: Ernest Douwes Dekker \\nTeks: Dr. Ernest François Eugène Douwes Dekker (umumnya dikenal dengan nama Douwes Dekker atau Danudirja Setiabudi; ) adalah seorang pejuang kemerdekaan dan pahlawan nasional Indonesia.'}\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "def format_passages(example):\n",
    "    \"\"\"\n",
    "    Menggabungkan positive_passages dan negative_passages, \n",
    "    lalu memformat top 3 passages menjadi string terstruktur.\n",
    "    \"\"\"\n",
    "    # Gabungkan positive_passages dan negative_passages\n",
    "    all_passages = example[\"positive_passages\"] + example[\"negative_passages\"]\n",
    "    \n",
    "    # Format 3 passage pertama\n",
    "    formatted_passages = [\n",
    "        f\"Judul: {p['title']} \\nTeks: {p['text']}\" for p in all_passages[:3]  # Ambil 3 passage pertama\n",
    "    ]\n",
    "    \n",
    "    return {\"formatted_passages\": \"\\n\\n\".join(formatted_passages)}\n",
    "\n",
    "# Terapkan transformasi pada semua split (train, dev, test) dalam sekali `.map()`\n",
    "processed_dataset = {}\n",
    "\n",
    "for split in finished_dataset.keys():  # Loop untuk setiap split: \"train\", \"dev\", \"test\"\n",
    "    processed_dataset[split] = finished_dataset[split].map(format_passages)\n",
    "\n",
    "# Konversi kembali ke DatasetDict\n",
    "processed_dataset = datasets.DatasetDict(processed_dataset)\n",
    "\n",
    "# Cek hasil pada salah satu split\n",
    "print(processed_dataset[\"train\"][0])  # Lihat hasil dari train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 200\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Ambil sampel dari tiap split\n",
    "train_sample = random.sample(list(finished_dataset[\"train\"]), 132)\n",
    "dev_sample = random.sample(list(finished_dataset[\"dev\"]), 40)\n",
    "test_sample = random.sample(list(finished_dataset[\"test\"]), 28)\n",
    "\n",
    "# Gabungkan semua sampel\n",
    "sample_data = train_sample + dev_sample + test_sample\n",
    "\n",
    "print(f\"Total samples: {len(sample_data)}\")  # Harusnya 71 (50+13+8)\n",
    "\n",
    "def combine_passages(example):\n",
    "    example[\"all_passages\"] = example[\"positive_passages\"] + example[\"negative_passages\"]\n",
    "    return example\n",
    "\n",
    "# Terapkan ke semua sampel\n",
    "sample_data = [combine_passages(ex) for ex in sample_data]\n",
    "\n",
    "def format_passages(example):\n",
    "    formatted_passages = [\n",
    "        f\"Judul: {p['title']} \\nTeks: {p['text']}\" for p in example[\"all_passages\"][:3]  # Ambil 5 passage\n",
    "    ]\n",
    "    return \"\\n\\n\".join(formatted_passages)  # Gabungkan semua passages\n",
    "\n",
    "# Tambahkan kolom \"formatted_passages\"\n",
    "for ex in sample_data:\n",
    "    ex[\"formatted_passages\"] = format_passages(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages', 'tydiqa_id', 'answers', 'formatted_passages'],\n",
       "        num_rows: 4542\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages', 'tydiqa_id', 'answers', 'formatted_passages'],\n",
       "        num_rows: 1143\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages', 'tydiqa_id', 'answers', 'formatted_passages'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Memproses split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing dataset: 100%|██████████| 4542/4542 [5:51:08<00:00,  4.64s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split train selesai dalam 21068.54 detik\n",
      "🔄 Memproses split: dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing dataset: 100%|██████████| 1143/1143 [1:26:45<00:00,  4.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split dev selesai dalam 5206.19 detik\n",
      "🔄 Memproses split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing dataset: 100%|██████████| 565/565 [43:37<00:00,  4.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split test selesai dalam 2617.99 detik\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf58dfdee89422783789f98ddbbe80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011d432eb1b74c168e30546751a908c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfa05b54ae54e17887ce03ab8ee6b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DatasetDict telah disimpan di generated_data/draft_summary_dataset\n"
     ]
    }
   ],
   "source": [
    "from summarize import summarize_top_5_combined\n",
    "import time\n",
    "from datasets import DatasetDict\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Dictionary untuk menyimpan hasil per split\n",
    "processed_splits = {}\n",
    "\n",
    "# Loop untuk setiap split (train, dev, test)\n",
    "for split in processed_dataset.keys():\n",
    "    print(f\"🔄 Memproses split: {split}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Jalankan rangkuman untuk split tertentu\n",
    "    processed_split = summarize_top_5_combined(\n",
    "        model_name=model_name,\n",
    "        dataset=processed_dataset[split],  # Proses per split\n",
    "        query_col=\"query\",\n",
    "        docs_col=\"formatted_passages\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print(f\"✅ Split {split} selesai dalam {duration:.2f} detik\")\n",
    "\n",
    "    # Simpan hasil per split ke dalam dictionary\n",
    "    processed_splits[split] = processed_split\n",
    "\n",
    "# Gabungkan kembali hasil per split menjadi DatasetDict\n",
    "final_dataset = DatasetDict(processed_splits)\n",
    "\n",
    "# Path penyimpanan hasil akhir\n",
    "save_path = \"generated_data/draft_summary_dataset\"\n",
    "\n",
    "# Simpan dataset yang telah digabungkan\n",
    "final_dataset.save_to_disk(save_path)\n",
    "\n",
    "print(f\"✅ DatasetDict telah disimpan di {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages', 'tydiqa_id', 'answers', 'formatted_passages', 'summary'],\n",
       "        num_rows: 4542\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages', 'tydiqa_id', 'answers', 'formatted_passages', 'summary'],\n",
       "        num_rows: 1143\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages', 'tydiqa_id', 'answers', 'formatted_passages', 'summary'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dataset = load_from_disk(\"./generated_data/draft_summary_dataset\")\n",
    "loaded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'James Hepburn meninggal di Puri Dragsholm, 75 kilometer dari Kopenhagen.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dataset['dev'][0]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x000002669660EF20> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c128c7f773b4b32a4ba96631ffd69bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/64 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from utils import DeviceType, format_chat_prompt, prepare_inputs\n",
    "# import torch\n",
    "\n",
    "# def summarize_single_row(example, model, tokenizer, query_col, docs_col, max_new_tokens=50, temperature=0.7):\n",
    "#     \"\"\"\n",
    "#     Fungsi ini menerima satu baris dataset, lalu menghasilkan rangkuman teks.\n",
    "    \n",
    "#     Args:\n",
    "#         example: Satu instance dari dataset\n",
    "#         model: Model yang telah dimuat\n",
    "#         tokenizer: Tokenizer yang telah dimuat\n",
    "#         query_col: Nama kolom pertanyaan\n",
    "#         docs_col: Nama kolom dokumen\n",
    "#         max_new_tokens: Jumlah token yang dihasilkan\n",
    "#         temperature: Parameter sampling\n",
    "\n",
    "#     Returns:\n",
    "#         Dict dengan tambahan kolom \"summary\"\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Ambil teks dari dataset\n",
    "#         top_5_text = example[docs_col]\n",
    "#         query = example[query_col]\n",
    "\n",
    "#         # Siapkan prompt untuk rangkuman\n",
    "#         messages = [\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": f'{top_5_text}\\n\\nRingkaslah teks di atas agar dapat menjawab pertanyaan secara mendetail. Jangan berikan pengantar pada hasil. Pertanyaan: \"{query}\"'\n",
    "#             }\n",
    "#         ]\n",
    "\n",
    "#         formatted_prompt = format_chat_prompt(messages, tokenizer)\n",
    "\n",
    "#         # Tokenisasi input\n",
    "#         inputs = prepare_inputs([formatted_prompt], tokenizer, DeviceType.CUDA)\n",
    "#         token_ids = inputs[\"input_ids\"]\n",
    "\n",
    "#         # Gunakan model untuk generate teks\n",
    "#         with torch.no_grad():\n",
    "#             with torch.amp.autocast('cuda:0'):\n",
    "#                 summary_output = model.generate(\n",
    "#                     inputs[\"input_ids\"],\n",
    "#                     max_new_tokens=max_new_tokens,\n",
    "#                     temperature=temperature,\n",
    "#                     top_p=1.0,\n",
    "#                     do_sample=temperature > 0,\n",
    "#                     pad_token_id=tokenizer.pad_token_id,\n",
    "#                     return_dict_in_generate=True\n",
    "#                 )\n",
    "\n",
    "#                 # Ambil token hasil generate\n",
    "#                 generated_ids = summary_output.sequences[0][len(token_ids[0]):]\n",
    "\n",
    "#                 # Decode hasil keluaran\n",
    "#                 summary = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "#         # Kembalikan hasil dengan tambahan kolom summary\n",
    "#         return {\"summary\": summary}\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ Error pada satu instance: {e}\")\n",
    "#         return {\"summary\": \"Error\"}\n",
    "    \n",
    "# print(\"num samples:\", num_samples)    \n",
    "# processed_dataset = sample_dataset.select(range(num_samples)).map(\n",
    "#     lambda x: summarize_single_row(x, model, tokenizer, query_col=\"query\", docs_col=\"formatted_passages\"),\n",
    "#     batched=False  # Harus False karena kita memproses satu per satu\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_12-4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
