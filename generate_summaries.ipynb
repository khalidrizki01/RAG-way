{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model, Tokenizer, dan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c57bd03260432c9ac9f1126e6797c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from utils import load_model_and_tokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)\n",
    "finished_dataset = load_from_disk(\"./generated_data/mr_tydi_tydiqa_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902f0f831b5b403999323b8c914963f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8da204b38f43de996be5e08d8345e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdc984b4d7c4f13900a6266dfe5652d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query_id': '0', 'query': 'dimanakah Dr. Ernest François Eugène Douwes Dekker meninggal?', 'positive_passages': [{'docid': '7080#33', 'text': 'Ernest Douwes Dekker wafat dini hari tanggal 28 Agustus 1950 (tertulis di batu nisannya; 29 Agustus 1950 versi van der Veur, 2006) dan dimakamkan di TMP Cikutra, Bandung.', 'title': 'Ernest Douwes Dekker'}], 'negative_passages': [{'docid': '20439#25', 'text': 'Eduard Douwes Dekker kemudian pindah ke Ingelheim am Rhein dekat Sungai Rhein sampai akhirnya meninggal 19 Februari 1887.', 'title': 'Eduard Douwes Dekker'}, {'docid': '7080#0', 'text': 'Dr. Ernest François Eugène Douwes Dekker (umumnya dikenal dengan nama Douwes Dekker atau Danudirja Setiabudi; ) adalah seorang pejuang kemerdekaan dan pahlawan nasional Indonesia.', 'title': 'Ernest Douwes Dekker'}, {'docid': '7080#2', 'text': 'Douwes Dekker terlahir di Pasuruan, Jawa Timur, pada tanggal 8 Oktober 1879, sebagaimana yang dia tulis pada riwayat hidup singkat saat mendaftar di Universitas Zurich, September 1913. Ayahnya, Auguste Henri Edoeard Douwes Dekker, adalah seorang agen di bank kelas kakap Nederlandsch Indisch Escomptobank. Auguste ayahnya, memiliki darah Belanda dari ayahnya, Jan (adik Eduard Douwes Dekker) dan dari ibunya, Louise Bousquet. Sementara itu, ibu Douwes Dekker, Louisa Neumann, lahir di Pekalongan, Jawa Tengah, dari pasangan Jerman-Jawa.[1] Dia terlahir sebagai anak ke-3 dari 4 bersaudara, dan keluarganya pun sering berpindah-pindah. Saudaranya yang perempuan dan laki-laki, yakni Adeline (1876) dan Julius (1878) terlahir sewaktu keluarga Dekker berada di Surabaya, dan adik laki-lakinya lahir di Meester Cornelis, Batavia (sekarang Jatinegara, Jakarta Timur pada tahun 1883. Dari situ, keluarga Dekker berpindah lagi ke Pegangsaan, Jakarta Pusat.[1]', 'title': 'Ernest Douwes Dekker'}, {'docid': '7080#17', 'text': 'jmpl|200px|Universitas Zurich, tempat Ernest Douwes Dekker menempuh pendidikan tingginya. Masa di Eropa dimanfaatkan oleh Nes untuk mengambil program doktor di Universitas Zürich, Swiss, dalam bidang ekonomi. Di sini ia tinggal bersama-sama keluarganya. Gelar doktor diperoleh secara agak kontroversial dan dengan nilai \"serendah-rendahnya\", menurut istilah salah satu pengujinya. Karena di Swis ia terlibat konspirasi dengan kaum revolusioner India, ia ditangkap di Hong Kong dan diadili, kemudian ia ditahan di Singapura (1918). Setelah dua tahun dipenjara, ia pulang ke Hindia Belanda 1920.', 'title': 'Ernest Douwes Dekker'}], 'tydiqa_id': '496955121539262633-33', 'answers': '28 Agustus 1950', 'formatted_passages': 'Judul: Ernest Douwes Dekker \\nTeks: Ernest Douwes Dekker wafat dini hari tanggal 28 Agustus 1950 (tertulis di batu nisannya; 29 Agustus 1950 versi van der Veur, 2006) dan dimakamkan di TMP Cikutra, Bandung.\\n\\nJudul: Eduard Douwes Dekker \\nTeks: Eduard Douwes Dekker kemudian pindah ke Ingelheim am Rhein dekat Sungai Rhein sampai akhirnya meninggal 19 Februari 1887.\\n\\nJudul: Ernest Douwes Dekker \\nTeks: Dr. Ernest François Eugène Douwes Dekker (umumnya dikenal dengan nama Douwes Dekker atau Danudirja Setiabudi; ) adalah seorang pejuang kemerdekaan dan pahlawan nasional Indonesia.'}\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "def format_passages(example):\n",
    "    \"\"\"\n",
    "    Menggabungkan positive_passages dan negative_passages, \n",
    "    lalu memformat top 3 passages menjadi string terstruktur.\n",
    "    \"\"\"\n",
    "    # Gabungkan positive_passages dan negative_passages\n",
    "    all_passages = example[\"positive_passages\"] + example[\"negative_passages\"]\n",
    "    \n",
    "    # Format 3 passage pertama\n",
    "    formatted_passages = [\n",
    "        f\"Judul: {p['title']} \\nTeks: {p['text']}\" for p in all_passages[:3]  # Ambil 3 passage pertama\n",
    "    ]\n",
    "    \n",
    "    return {\"formatted_passages\": \"\\n\\n\".join(formatted_passages)}\n",
    "\n",
    "# Terapkan transformasi pada semua split (train, dev, test) dalam sekali `.map()`\n",
    "processed_dataset = {}\n",
    "\n",
    "for split in finished_dataset.keys():  # Loop untuk setiap split: \"train\", \"dev\", \"test\"\n",
    "    processed_dataset[split] = finished_dataset[split].map(format_passages)\n",
    "\n",
    "# Konversi kembali ke DatasetDict\n",
    "processed_dataset = datasets.DatasetDict(processed_dataset)\n",
    "\n",
    "# Cek hasil pada salah satu split\n",
    "print(processed_dataset[\"train\"][0])  # Lihat hasil dari train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages', 'tydiqa_id', 'answers', 'formatted_passages'],\n",
       "        num_rows: 4542\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages', 'tydiqa_id', 'answers', 'formatted_passages'],\n",
       "        num_rows: 1143\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages', 'tydiqa_id', 'answers', 'formatted_passages'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buat sampel kecil untuk testing awal (jika perlu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 200\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Ambil sampel dari tiap split\n",
    "train_sample = random.sample(list(finished_dataset[\"train\"]), 132)\n",
    "dev_sample = random.sample(list(finished_dataset[\"dev\"]), 40)\n",
    "test_sample = random.sample(list(finished_dataset[\"test\"]), 28)\n",
    "\n",
    "# Gabungkan semua sampel\n",
    "sample_data = train_sample + dev_sample + test_sample\n",
    "\n",
    "print(f\"Total samples: {len(sample_data)}\")  # Harusnya 71 (50+13+8)\n",
    "\n",
    "def combine_passages(example):\n",
    "    example[\"all_passages\"] = example[\"positive_passages\"] + example[\"negative_passages\"]\n",
    "    return example\n",
    "\n",
    "# Terapkan ke semua sampel\n",
    "sample_data = [combine_passages(ex) for ex in sample_data]\n",
    "\n",
    "def format_passages(example):\n",
    "    formatted_passages = [\n",
    "        f\"Judul: {p['title']} \\nTeks: {p['text']}\" for p in example[\"all_passages\"][:3]  # Ambil 5 passage\n",
    "    ]\n",
    "    return \"\\n\\n\".join(formatted_passages)  # Gabungkan semua passages\n",
    "\n",
    "# Tambahkan kolom \"formatted_passages\"\n",
    "for ex in sample_data:\n",
    "    ex[\"formatted_passages\"] = format_passages(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Summary & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Memproses split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing dataset: 100%|██████████| 4542/4542 [5:51:08<00:00,  4.64s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split train selesai dalam 21068.54 detik\n",
      "🔄 Memproses split: dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing dataset: 100%|██████████| 1143/1143 [1:26:45<00:00,  4.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split dev selesai dalam 5206.19 detik\n",
      "🔄 Memproses split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing dataset: 100%|██████████| 565/565 [43:37<00:00,  4.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split test selesai dalam 2617.99 detik\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf58dfdee89422783789f98ddbbe80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011d432eb1b74c168e30546751a908c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfa05b54ae54e17887ce03ab8ee6b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DatasetDict telah disimpan di generated_data/draft_summary_dataset\n"
     ]
    }
   ],
   "source": [
    "from summarize import summarize_top_5_combined\n",
    "import time\n",
    "from datasets import DatasetDict\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Dictionary untuk menyimpan hasil per split\n",
    "processed_splits = {}\n",
    "\n",
    "# Loop untuk setiap split (train, dev, test)\n",
    "for split in processed_dataset.keys():\n",
    "    print(f\"🔄 Memproses split: {split}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Jalankan rangkuman untuk split tertentu\n",
    "    processed_split = summarize_top_5_combined(\n",
    "        model_name=model_name,\n",
    "        dataset=processed_dataset[split],  # Proses per split\n",
    "        query_col=\"query\",\n",
    "        docs_col=\"formatted_passages\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print(f\"✅ Split {split} selesai dalam {duration:.2f} detik\")\n",
    "\n",
    "    # Simpan hasil per split ke dalam dictionary\n",
    "    processed_splits[split] = processed_split\n",
    "\n",
    "# Gabungkan kembali hasil per split menjadi DatasetDict\n",
    "final_dataset = DatasetDict(processed_splits)\n",
    "\n",
    "# Path penyimpanan hasil akhir\n",
    "save_path = \"generated_data/draft_summary_dataset\"\n",
    "\n",
    "# Simpan dataset yang telah digabungkan\n",
    "final_dataset.save_to_disk(save_path)\n",
    "\n",
    "print(f\"✅ DatasetDict telah disimpan di {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Generation with Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.float16\n",
      "🔄 Memproses split: train dengan model 1B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses:   0%|          | 0/4542 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "Generating responses: 100%|██████████| 4542/4542 [1:55:24<00:00,  1.52s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Proses selesai dalam 6924.12 detik\n",
      "🔄 Memproses split: dev dengan model 1B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 1143/1143 [33:26<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Proses selesai dalam 2006.35 detik\n",
      "🔄 Memproses split: test dengan model 1B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 565/565 [14:10<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Proses selesai dalam 850.10 detik\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e94db0b83204c098b4febd8717f91cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb48373850a14136b430afb7517b8336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdda4640d9ea45efac48377c7a58d713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Hasil telah disimpan dalam ./generated_data/TUNING_final_summary\n"
     ]
    }
   ],
   "source": [
    "from generate_answer import generate_answers_and_evaluate\n",
    "from datasets import load_from_disk, DatasetDict, Dataset\n",
    "from utils import load_model_and_tokenizer\n",
    "import json\n",
    "import time\n",
    "\n",
    "loaded_dataset = load_from_disk(\"./generated_data/draft_summary_dataset\")\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)\n",
    "\n",
    "processed_splits = {}\n",
    "\n",
    "# 🔹 Mulai proses evaluasi per split\n",
    "for split in loaded_dataset.keys():\n",
    "    print(f\"🔄 Memproses split: {split} dengan model 1B\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    processed_split = generate_answers_and_evaluate(\n",
    "        dataset=loaded_dataset[split],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    # 🕒 Waktu eksekusi\n",
    "    print(f\"✅ Proses selesai dalam {end_time - start_time:.2f} detik\")\n",
    "    processed_splits[split] = Dataset.from_list(processed_split)\n",
    "\n",
    "processed_dataset = DatasetDict(processed_splits)\n",
    "\n",
    "save_path = \"./generated_data/TUNING_final_summary\"\n",
    "processed_dataset.save_to_disk(save_path)\n",
    "\n",
    "# # 📂 Simpan hasil berdasarkan ukuran model\n",
    "# output_file = f\"./generated_data/evaluated_summary_result_1B_FOR_TUNING.json\"\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(processed_splits, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"📄 Hasil telah disimpan dalam {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'answer', 'generated_results'],\n",
       "        num_rows: 4542\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'answer', 'generated_results'],\n",
       "        num_rows: 1143\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'answer', 'generated_results'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_unduh = loaded_dataset = load_from_disk(\"./generated_data/TUNING_final_summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Perbandingan superior_by_having_no_summary ===\n",
      "\n",
      "📂 Split: train\n",
      "🔹 Jumlah superior_by_having_no_summary di 1B: 4\n",
      "🔹 Jumlah superior_by_having_no_summary di 3B: 5\n",
      "\n",
      "📌 Query dari row yang superior_by_having_no_summary di dataset 1B:\n",
      "   - Perusahaan apa yang membuat Accuracy International Arctic Warfare?\n",
      "   - darimanakah taekwondo berasal?\n",
      "   - apakah nama ibukota Argentina?\n",
      "   - berapakah luas Spanyol ?\n",
      "\n",
      "📌 Query dari row yang superior_by_having_no_summary di dataset 3B:\n",
      "   - Perusahaan apa yang membuat Accuracy International Arctic Warfare?\n",
      "   - darimanakah taekwondo berasal?\n",
      "   - Siapa ibu Yesus kristus?\n",
      "   - apakah nama ibukota Argentina?\n",
      "   - Kapan hari kemerdekaan Kamboja\n",
      "\n",
      "============================================================\n",
      "\n",
      "📂 Split: dev\n",
      "🔹 Jumlah superior_by_having_no_summary di 1B: 0\n",
      "🔹 Jumlah superior_by_having_no_summary di 3B: 1\n",
      "\n",
      "📌 Query dari row yang superior_by_having_no_summary di dataset 1B:\n",
      "\n",
      "📌 Query dari row yang superior_by_having_no_summary di dataset 3B:\n",
      "   - Ada berapa bulan dalam tahun Hijriah ?\n",
      "\n",
      "============================================================\n",
      "\n",
      "📂 Split: test\n",
      "🔹 Jumlah superior_by_having_no_summary di 1B: 0\n",
      "🔹 Jumlah superior_by_having_no_summary di 3B: 2\n",
      "\n",
      "📌 Query dari row yang superior_by_having_no_summary di dataset 1B:\n",
      "\n",
      "📌 Query dari row yang superior_by_having_no_summary di dataset 3B:\n",
      "   - Bagaimana sistem pemerintahan Jepang?\n",
      "   - dimanakah kantor pusat Airbus S.A.S?\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 📂 Load kedua dataset JSON\n",
    "with open(\"./generated_data/SAMPEL_evaluated_summary_result_1B_latest.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset_1B = json.load(f)\n",
    "\n",
    "with open(\"./generated_data/SAMPEL_evaluated_summary_result_3B_latest.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset_3B = json.load(f)\n",
    "\n",
    "# 🔄 Loop melalui setiap split (train, dev, test)\n",
    "superior_counts_1B = {}\n",
    "superior_counts_3B = {}\n",
    "superior_queries_1B = {}  # Menyimpan query dari row yang superior di 1B\n",
    "superior_queries_3B = {}  # Menyimpan query dari row yang superior di 3B\n",
    "\n",
    "for split in dataset_1B.keys():\n",
    "    # 🔍 Cek superior_by_having_no_summary untuk masing-masing dataset\n",
    "    superior_1B = []\n",
    "    superior_3B = []\n",
    "\n",
    "    # 🔍 Proses dataset 1B\n",
    "    for idx, row in enumerate(dataset_1B[split]):\n",
    "        w_summary = row[\"generated_results\"][\"w_summary\"]\n",
    "        wo_summary = row[\"generated_results\"][\"wo_summary\"]\n",
    "\n",
    "        # 1️⃣ Cek apakah EM dari wo_summary bernilai 1, jika iya, tambahkan jika >= EM dari w_summary\n",
    "        if wo_summary[\"em\"] == 1 and wo_summary[\"em\"] >= w_summary[\"em\"]:\n",
    "            superior_1B.append(row[\"query\"])\n",
    "        # 2️⃣ Jika kondisi di atas tidak terpenuhi, cek apakah F1 dari w_summary < wo_summary\n",
    "        elif w_summary[\"f1\"] < wo_summary[\"f1\"]:\n",
    "            superior_1B.append(row[\"query\"])\n",
    "\n",
    "    # 🔍 Proses dataset 3B\n",
    "    for idx, row in enumerate(dataset_3B[split]):\n",
    "        w_summary = row[\"generated_results\"][\"w_summary\"]\n",
    "        wo_summary = row[\"generated_results\"][\"wo_summary\"]\n",
    "\n",
    "        # 1️⃣ Cek apakah EM dari wo_summary bernilai 1, jika iya, tambahkan jika >= EM dari w_summary\n",
    "        if wo_summary[\"em\"] == 1 and wo_summary[\"em\"] >= w_summary[\"em\"]:\n",
    "            superior_3B.append(row[\"query\"])\n",
    "        # 2️⃣ Jika kondisi di atas tidak terpenuhi, cek apakah F1 dari w_summary < wo_summary\n",
    "        elif w_summary[\"f1\"] < wo_summary[\"f1\"]:\n",
    "            superior_3B.append(row[\"query\"])\n",
    "\n",
    "    # 🔄 Simpan jumlah row yang superior_by_having_no_summary\n",
    "    superior_counts_1B[split] = len(superior_1B)\n",
    "    superior_counts_3B[split] = len(superior_3B)\n",
    "\n",
    "    # Simpan query-query yang memenuhi kondisi\n",
    "    superior_queries_1B[split] = superior_1B\n",
    "    superior_queries_3B[split] = superior_3B\n",
    "\n",
    "# 📢 Output hasil analisis\n",
    "print(\"=== Perbandingan superior_by_having_no_summary ===\")\n",
    "for split in dataset_1B.keys():\n",
    "    print(f\"\\n📂 Split: {split}\")\n",
    "    print(f\"🔹 Jumlah superior_by_having_no_summary di 1B: {superior_counts_1B[split]}\")\n",
    "    print(f\"🔹 Jumlah superior_by_having_no_summary di 3B: {superior_counts_3B[split]}\")\n",
    "\n",
    "    # Tampilkan query dari dataset 1B\n",
    "    print(f\"\\n📌 Query dari row yang superior_by_having_no_summary di dataset 1B:\")\n",
    "    for query in superior_queries_1B[split]:\n",
    "        print(f\"   - {query}\")\n",
    "\n",
    "    # Tampilkan query dari dataset 3B\n",
    "    print(f\"\\n📌 Query dari row yang superior_by_having_no_summary di dataset 3B:\")\n",
    "    for query in superior_queries_3B[split]:\n",
    "        print(f\"   - {query}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (untuk diisi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_12-4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
