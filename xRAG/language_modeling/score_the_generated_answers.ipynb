{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5f1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import XRAG_TOKEN, _concat_messages_qwen\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from model.E5Retriever import E5Retriever\n",
    "from model.xQwen3 import XQwen3ForCausalLM\n",
    "\n",
    "model_name_or_path = \"../output/finetuned/finished_model\"\n",
    "retriever_name_or_path = 'intfloat/multilingual-e5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, \n",
    "    padding_side='left'\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token:\n",
    "    pass\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "if retriever_name_or_path.lower() == 'intfloat/multilingual-e5-small':\n",
    "    retriever = E5Retriever(retriever_name_or_path)\n",
    "    retriever_tokenizer = AutoTokenizer.from_pretrained(retriever_name_or_path)\n",
    "retriever_hidden_size = retriever.get_embed_dim()\n",
    "retriever.eval()\n",
    "retriever = retriever.to(device)\n",
    "\n",
    "def load_xrag_dataset(test_path, test_split, background_col, retriever_name):\n",
    "    # load dataset dokumen retrieval dan ganti nama kolom passages ke background\n",
    "    test_data = load_from_disk(test_path)[test_split]\n",
    "\n",
    "    if retriever_name is not None and retriever_name.lower() == 'intfloat/multilingual-e5-small':\n",
    "        def add_passage_prefix(example):\n",
    "            example[background_col] = [\"passage: \" + x for x in example[background_col]]\n",
    "            return example\n",
    "        \n",
    "        test_data = test_data.map(add_passage_prefix)\n",
    "    return test_data\n",
    "\n",
    "retriever_name_or_path = 'intfloat/multilingual-e5-small'\n",
    "test_data = load_xrag_dataset(\n",
    "    test_path=\"../../generated_data/raw/final_dataset\", \n",
    "    test_split='test',\n",
    "    background_col=\"passages\",\n",
    "    retriever_name= retriever_name_or_path\n",
    ")\n",
    "\n",
    "BACKGROUND_PROMPT_TEMPLATE = \"Konteks: {background}\\n\\n\"\n",
    "\n",
    "retrieval_embed_length = 3\n",
    "\n",
    "def get_start_prompt(use_rag:bool = True):\n",
    "    return {\n",
    "        True: \"Rujuklah dokumen konteks dan jawab pertanyaan: \", \n",
    "        False: \"Jawab pertanyaan: \"\n",
    "    }[use_rag]\n",
    "\n",
    "def format_one_example(row, query_col, background_col, use_rag, retrieval_embed_length):\n",
    "    query = row[query_col]\n",
    "    prompt = f\"Pertanyaan: {query}\".strip()\n",
    "    backgrounds = []\n",
    "\n",
    "    if use_rag:\n",
    "        background_prompt = \"\"\n",
    "\n",
    "        if retrieval_embed_length > 0:\n",
    "            background_prompt += \" \".join([XRAG_TOKEN]*retrieval_embed_length)\n",
    "\n",
    "        else:\n",
    "            background_prompt += \" \"\n",
    "        background_prompt = background_prompt.strip()\n",
    "        prompt = BACKGROUND_PROMPT_TEMPLATE.format_map(dict(background=background_prompt)) + prompt\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def prepare_prompts(test_data, tokenizer, use_rag=True, chat_format='qwen'):\n",
    "    splitter = \"\\n\\n\"\n",
    "\n",
    "    # for idx, row in enumerate(test_data):\n",
    "    prompt_start = get_start_prompt(use_rag=use_rag)\n",
    "    prompt_end = format_one_example(  # , background\n",
    "        test_data, \n",
    "        query_col='query', \n",
    "        background_col= 'passages', \n",
    "        use_rag=use_rag, \n",
    "        retrieval_embed_length=retrieval_embed_length\n",
    "    )\n",
    "    prompt = prompt_start + splitter + prompt_end\n",
    "    if chat_format == 'qwen':\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        prompt = _concat_messages_qwen(messages, tokenizer) + \"<|im_start|>assistant\\nJawaban: \"\n",
    "    tokenized_prompt = tokenizer(prompt, truncation=True, max_length=512, add_special_tokens=False).input_ids\n",
    "    return {\n",
    "        \"prompt\": prompt, \n",
    "        # \"backgrounds\": background, \n",
    "        \"tokenized_prompt\": tokenized_prompt\n",
    "    }\n",
    "\n",
    "from functools import partial\n",
    "max_test_samples = None\n",
    "if max_test_samples is not None:\n",
    "    test_data = test_data.select(range(max_test_samples))\n",
    "\n",
    "prep_prompts = partial(prepare_prompts, tokenizer=tokenizer)\n",
    "\n",
    "formatted_test_data = test_data.map(prep_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e57e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing document embeddings with intfloat/multilingual-e5-small...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                               | 0/283 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "566it [19:10,  2.03s/it]                                    \n"
     ]
    }
   ],
   "source": [
    "from utils import get_retrieval_embeds\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_retrieval_embeds_per_example(example, background_col = 'passages'):\n",
    "    passages = example[background_col]\n",
    "    \n",
    "    tokenized = retriever_tokenizer(\n",
    "        passages,\n",
    "        max_length=180,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = tokenized[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = tokenized[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "    # get embeddings: tensor [num_passages, dim]\n",
    "    embeds = get_retrieval_embeds(\n",
    "        retriever=retriever,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "    ).cpu()\n",
    "\n",
    "    # convert each vector to list[float] for serialization\n",
    "    embed_list = [embed.tolist() for embed in embeds]\n",
    "\n",
    "    return {\"retrieval_embeds\": embed_list}\n",
    "\n",
    "print(f\"Preparing document embeddings with {retriever_name_or_path}...\")\n",
    "\n",
    "compute_retrieval_embeds = partial(\n",
    "    compute_retrieval_embeds_per_example, \n",
    "    background_col = 'passages'\n",
    ")\n",
    "\n",
    "formatted_test_data = formatted_test_data.map(\n",
    "    compute_retrieval_embeds,\n",
    "    batched=False\n",
    ")\n",
    "del retriever.model\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from model.xQwen3 import XQwen3ForCausalLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "MODEL_CLASS = eval(config.architectures[0])\n",
    "model = MODEL_CLASS.from_pretrained(\n",
    "    model_name_or_path, \n",
    "    torch_dtype=torch.bfloat16 \n",
    ").to(\"cuda:0\")\n",
    "model.eval()\n",
    "\n",
    "if retriever is not None:\n",
    "    assert XRAG_TOKEN in tokenizer.get_vocab() \n",
    "    model.set_xrag_token_id(tokenizer.convert_tokens_to_ids(XRAG_TOKEN))\n",
    "\n",
    "from utils import stop_sequences_criteria\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def llm_for_open_generation_dataset(\n",
    "    llm,\n",
    "    llm_tokenizer,\n",
    "    formatted_dataset,\n",
    "    batch_size=2,\n",
    "    enable_progress_bar=True,\n",
    "):\n",
    "    generated_answers = []\n",
    "    total_test_number = len(formatted_dataset)\n",
    "    device = llm.device\n",
    "\n",
    "    progress_bar = tqdm(range(0, total_test_number, batch_size), ncols=60, disable=not enable_progress_bar)\n",
    "\n",
    "    for start_idx in range(0, total_test_number, batch_size):\n",
    "        batch = formatted_dataset.select(range(start_idx, min(start_idx + batch_size, total_test_number)))\n",
    "\n",
    "        prompts = batch[\"prompt\"]\n",
    "        tokenized_prompt = llm_tokenizer(prompts, padding='longest', return_tensors='pt', truncation=True, max_length=512)\n",
    "        input_ids = tokenized_prompt.input_ids.to(device)\n",
    "        attention_mask = tokenized_prompt.attention_mask.to(device)   \n",
    "\n",
    "        stopping_criteria = stop_sequences_criteria(llm_tokenizer, input_ids.shape[1], input_ids.shape[0])\n",
    "        retrieval_kwargs = {}\n",
    "\n",
    "        if \"retrieval_embeds\" in batch.column_names:\n",
    "            embeds_batch = batch[\"retrieval_embeds\"]\n",
    "            # Flatten and convert to tensor\n",
    "            embeds = [torch.tensor(vec, dtype=torch.float32) for sublist in embeds_batch for vec in sublist]\n",
    "            embeds = torch.stack(embeds).to(device)\n",
    "            retrieval_kwargs[\"retrieval_embeds\"] = embeds\n",
    "            stopping_criteria = stop_sequences_criteria(llm_tokenizer, 0, input_ids.shape[0])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_output = llm.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=100,\n",
    "                pad_token_id=llm_tokenizer.pad_token_id,\n",
    "                use_cache=True,\n",
    "                **retrieval_kwargs,\n",
    "            )\n",
    "\n",
    "        # decoded_full_output = llm_tokenizer.batch_decode(generated_output, skip_special_tokens=False)\n",
    "        # print(\"\\n==== OUTPUT LENGKAP (PROMPT + JAWABAN) ====\")\n",
    "        # for i, full in enumerate(decoded_full_output):\n",
    "        #     print(f\"\\n>> Output {start_idx + i}:\\n{full}\")\n",
    "        # print(\"===========================================\\n\")\n",
    "\n",
    "        input_length = 0 if retrieval_kwargs else input_ids.shape[1]\n",
    "        decoded_output = llm_tokenizer.batch_decode(generated_output[:, input_length:], skip_special_tokens=False)\n",
    "        generated_answers.extend([x.strip() for x in decoded_output])\n",
    "\n",
    "        progress_bar.update(batch_size)\n",
    "\n",
    "    return generated_answers\n",
    "\n",
    "eval_batch_size = 2\n",
    "\n",
    "generated_results = llm_for_open_generation_dataset(\n",
    "    llm=model,\n",
    "    llm_tokenizer=tokenizer,\n",
    "    formatted_dataset=formatted_test_data,\n",
    "    batch_size=eval_batch_size,\n",
    "    enable_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import regex\n",
    "\n",
    "class SimpleTokenizer(object):\n",
    "    ALPHA_NUM = r'[\\p{L}\\p{N}\\p{M}]+'\n",
    "    NON_WS = r'[^\\p{Z}\\p{C}]'\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotators: None or empty set (only tokenizes).\n",
    "        \"\"\"\n",
    "        self._regexp = regex.compile(\n",
    "            '(%s)|(%s)' % (self.ALPHA_NUM, self.NON_WS),\n",
    "            flags=regex.IGNORECASE + regex.UNICODE + regex.MULTILINE\n",
    "        )\n",
    "\n",
    "    def tokenize(self, text, uncased=False):\n",
    "        matches = [m for m in self._regexp.finditer(text)]\n",
    "        if uncased:\n",
    "            tokens = [m.group().lower() for m in matches]\n",
    "        else:\n",
    "            tokens = [m.group() for m in matches]\n",
    "        return tokens\n",
    "\n",
    "def _normalize(text):\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "def has_answer(answers, text, tokenizer=SimpleTokenizer()) -> bool:\n",
    "    \"\"\"Check if a document contains an answer string.\"\"\"\n",
    "    text = _normalize(text)\n",
    "    text = tokenizer.tokenize(text, uncased=True)\n",
    "\n",
    "    for answer in answers:\n",
    "        answer = _normalize(answer)\n",
    "        answer = tokenizer.tokenize(answer, uncased=True)\n",
    "        for i in range(0, len(text) - len(answer) + 1):\n",
    "            if answer == text[i: i + len(answer)]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_substring_match_score(outputs,answers):\n",
    "    \"\"\"\n",
    "    outputs: [string1,string2]\n",
    "    answers: [\n",
    "                [string1_1,string1_2],\n",
    "                [string2_1,string2_2]\n",
    "             ]\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    assert len(outputs) == len(answers)\n",
    "    if not isinstance(answers[0],list):\n",
    "        answers = [[x] for x in answers]\n",
    "    substring_match_scores = []\n",
    "    answer_lengths = []\n",
    "    for output,answer in zip(outputs,answers):\n",
    "        if has_answer(answer,output): # EM evaluation\n",
    "            substring_match_scores.append(1.0)\n",
    "        else:\n",
    "            substring_match_scores.append(0.0)\n",
    "        \n",
    "        answer_lengths.append(len(output.split()))\n",
    "\n",
    "    substring_match = round(sum(substring_match_scores)/len(outputs), 4)\n",
    "    lens = round(np.mean(answer_lengths), 4)\n",
    "\n",
    "    return substring_match,substring_match_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2760e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return regex.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def f1(prediction, ground_truths):\n",
    "    return max([f1_score(prediction, gt) for gt in ground_truths])\n",
    "\n",
    "def eval_truthfulqa(outputs,answers):\n",
    "\n",
    "    f1_scores = []\n",
    "    # rl_scores = []\n",
    "    for output,answer in zip(outputs,answers):\n",
    "\n",
    "        f1_scores.append(f1(output, answer))\n",
    "        # rl_scores.append(rl(output, answer))\n",
    "\n",
    "    F1 = round(np.mean(f1_scores), 4)\n",
    "    # RL = round(np.mean(rl_scores), 4)\n",
    "\n",
    "    return F1,  f1_scores #,RL,rl_scores\n",
    "\n",
    "import json\n",
    "\n",
    "# Ambil jawaban referensi dari data\n",
    "answers = [x['answer'] for x in formatted_test_data]\n",
    "\n",
    "# Ganti args dengan parameter eksplisit\n",
    "eval_metrics = 'substring_match'  # atau 'fact_checking_acc', 'truthfulqa_f1_rl'\n",
    "use_rag = True\n",
    "avg_prompt_length = sum(len(x) for x in formatted_test_data[\"prompt\"]) / len(formatted_test_data)\n",
    "model_name_or_path = \"../output/finetuned/finished_model\"\n",
    "retriever_name_or_path = \"intfloat/multilingual-e5-small\"\n",
    "\n",
    "# Evaluasi\n",
    "if eval_metrics == 'substring_match':\n",
    "    score, score_per_sample = get_substring_match_score(generated_results, answers)\n",
    "elif eval_metrics == 'truthfulqa_f1_rl':\n",
    "    f1, rl, f1_scores, rl_scores = eval_truthfulqa(generated_results, answers)\n",
    "    score = f\"{f1}-{rl}\"\n",
    "    score_per_sample = [(f1_score, rl_score) for f1_score, rl_score in zip(f1_scores, rl_scores)]\n",
    "\n",
    "# Tampilkan hasil\n",
    "result_dict = {\n",
    "    \"dataset\": \"final_dataset\",\n",
    "    \"batch_size\": eval_batch_size,\n",
    "    \"include_retrieval\": use_rag,\n",
    "    \"avg_prompt_length\": avg_prompt_length,\n",
    "    \"model\": model_name_or_path,\n",
    "    f\"{eval_metrics}\": score,\n",
    "}\n",
    "\n",
    "if retriever_name_or_path is not None:\n",
    "    result_dict[\"retriever\"] = retriever_name_or_path\n",
    "\n",
    "print(json.dumps(result_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8946f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Direktori tujuan\n",
    "output_dir = \"../output/test_scores/qwen1.7_e5small_batch2_epoch3_2025-06-01-14.30\"\n",
    "\n",
    "# Buat direktori jika belum ada\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Simpan result_dict ke file JSON\n",
    "result_path = os.path.join(output_dir, \"result_dict.json\")\n",
    "with open(result_path, \"w\") as f:\n",
    "    json.dump(result_dict, f, indent=4)\n",
    "\n",
    "# Simpan score_per_sample ke file JSON\n",
    "score_path = os.path.join(output_dir, \"score_per_sample.json\")\n",
    "with open(score_path, \"w\") as f:\n",
    "    json.dump(score_per_sample, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab804cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
