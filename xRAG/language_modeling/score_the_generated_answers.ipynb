{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb5f1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 440.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import XRAG_TOKEN, _concat_messages_qwen\n",
    "from datasets import load_from_disk, load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from model.E5Retriever import E5Retriever\n",
    "from model.xQwen3 import XQwen3ForCausalLM\n",
    "from functools import partial\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "model_name_or_path = \"Qwen/Qwen3-1.7B\"\n",
    "projector_path = \"../output/finetuned/2025-07-19_10-58-09/projector_checkpoints_learningrate0.001/epoch_10/projector.pth\"\n",
    "retriever_name_or_path = 'intfloat/multilingual-e5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, \n",
    "    padding_side='left'\n",
    ")\n",
    "\n",
    "# Menambahkan token xrag ke daftar token tokenizer (karena kita menginjeksi projector ke qwen3)\n",
    "num_added_tokens = 0\n",
    "num_added_tokens += tokenizer.add_tokens([AddedToken(XRAG_TOKEN,lstrip=False,rstrip=False)])\n",
    "xrag_token_id = tokenizer.convert_tokens_to_ids(XRAG_TOKEN)\n",
    "\n",
    "max_test_samples = 50\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "if retriever_name_or_path.lower().startswith('intfloat/multilingual-e5'):\n",
    "    retriever = E5Retriever(retriever_name_or_path)\n",
    "    retriever_tokenizer = AutoTokenizer.from_pretrained(retriever_name_or_path)\n",
    "retriever_hidden_size = retriever.get_embed_dim()\n",
    "retriever.eval()\n",
    "retriever = retriever.to(device)\n",
    "\n",
    "def load_xrag_dataset(test_path, test_split, background_col, retriever_name):\n",
    "    if test_path.startswith('khalidrizki'):\n",
    "        test_data = load_dataset(test_path)[test_split]   \n",
    "    else: \n",
    "        test_data = load_from_disk(test_path)[test_split]\n",
    "\n",
    "    if retriever_name is not None and retriever_name.lower().startswith('intfloat/multilingual-e5'):\n",
    "        def add_passage_prefix(example):\n",
    "            example[background_col] = [\"passage: \" + x for x in example[background_col]]\n",
    "            return example\n",
    "        \n",
    "        test_data = test_data.map(add_passage_prefix)\n",
    "    return test_data\n",
    "\n",
    "retriever_name_or_path = 'intfloat/multilingual-e5-small'\n",
    "test_data = load_xrag_dataset(\n",
    "    test_path=\"khalidrizki/postretrieve-raw-dataset-v2\", \n",
    "    test_split='test',\n",
    "    background_col=\"sorted_truncPassages\",\n",
    "    retriever_name= retriever_name_or_path\n",
    ")\n",
    "\n",
    "def str_format(row, background_col, query_col):\n",
    "    prompt_template = \"Rujuklah latar belakang: {background} Pertanyaan: {query}\"\n",
    "    retrieval_embed_length = len(row[background_col])\n",
    "    background = \" \".join([XRAG_TOKEN]*retrieval_embed_length)\n",
    "    prompt = prompt_template.format_map(dict(background=background, query=row[query_col].strip()))\n",
    "    return prompt\n",
    "\n",
    "def prepare_prompt(row, tokenizer, query_col, background_col):\n",
    "    prompt = str_format(\n",
    "        row, \n",
    "        background_col, \n",
    "        query_col\n",
    "    )\n",
    "\n",
    "    messages = [{'role':'user', 'content':prompt}]\n",
    "    prompt = _concat_messages_qwen(messages, tokenizer, add_generation_prompt=True)\n",
    "    return {'prompt':prompt}\n",
    "\n",
    "if max_test_samples is not None:\n",
    "    test_data = test_data.select(range(max_test_samples))\n",
    "prep_prompt = partial(prepare_prompt, tokenizer=tokenizer, query_col='query', background_col='sorted_truncPassages')\n",
    "formatted_test_data = test_data.map(prep_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f002ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing document embeddings with intfloat/multilingual-e5-small...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:01<00:00, 28.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import get_retrieval_embeds\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_retrieval_embeds_per_example(example, background_col):\n",
    "    passages = example[background_col]\n",
    "    \n",
    "    tokenized = retriever_tokenizer(\n",
    "        passages,\n",
    "        max_length=512,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = tokenized[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = tokenized[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "    # get embeddings: tensor [num_passages, dim]\n",
    "    embeds = get_retrieval_embeds(\n",
    "        retriever=retriever,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "    ).cpu()\n",
    "\n",
    "    # convert each vector to list[float] for serialization\n",
    "    embed_list = [embed.tolist() for embed in embeds]\n",
    "\n",
    "    return {\"retrieval_embeds\": embed_list}\n",
    "\n",
    "print(f\"Preparing document embeddings with {retriever_name_or_path}...\")\n",
    "\n",
    "compute_retrieval_embeds = partial(\n",
    "    compute_retrieval_embeds_per_example, \n",
    "    background_col = 'sorted_truncPassages'\n",
    ")\n",
    "\n",
    "formatted_test_data = formatted_test_data.map(\n",
    "    compute_retrieval_embeds,\n",
    "    batched=False\n",
    ")\n",
    "del retriever.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff22dfc",
   "metadata": {},
   "source": [
    "## Score the generated answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93368c9c",
   "metadata": {},
   "source": [
    "### dgn model lengkap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "050e57e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.02it/s]\n",
      "Some weights of XQwen3ForCausalLM were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['projector.projector.0.bias', 'projector.projector.0.weight', 'projector.projector.2.bias', 'projector.projector.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_80764\\3260820053.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(projector_path), strict=False)\n",
      "  0%|                                | 0/50 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████| 50/50 [01:06<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "from model.xQwen3 import XQwen3ForCausalLM, XQwen3Config\n",
    "\n",
    "# config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "# MODEL_CLASS = eval(config.architectures[0])\n",
    "# model = MODEL_CLASS.from_pretrained(\n",
    "#     model_name_or_path, \n",
    "#     torch_dtype=torch.bfloat16 \n",
    "# ).to(\"cuda:0\")\n",
    "\n",
    "config = XQwen3Config.from_pretrained(model_name_or_path, retriever_hidden_size=384)\n",
    "model = XQwen3ForCausalLM.from_pretrained(  # XLlamaForCausalLM\n",
    "    model_name_or_path,\n",
    "    config=config,\n",
    "    torch_dtype = 'bfloat16'\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "model.load_state_dict(torch.load(projector_path), strict=False)\n",
    "model.eval()\n",
    "\n",
    "if retriever is not None:\n",
    "    assert XRAG_TOKEN in tokenizer.get_vocab() \n",
    "    model.set_xrag_token_id(xrag_token_id)\n",
    "    if num_added_tokens > 0:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "from utils import stop_sequences_criteria\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def llm_for_open_generation_dataset(\n",
    "    llm,\n",
    "    llm_tokenizer,\n",
    "    formatted_dataset,\n",
    "    batch_size=2,\n",
    "    enable_progress_bar=True,\n",
    "):\n",
    "    full_completion = []\n",
    "    # generated_answers = []\n",
    "    \n",
    "    total_test_number = len(formatted_dataset)\n",
    "    device = llm.device\n",
    "\n",
    "    progress_bar = tqdm(range(0, total_test_number, batch_size), ncols=60, disable=not enable_progress_bar)\n",
    "\n",
    "    for start_idx in range(0, total_test_number, batch_size):\n",
    "        batch = formatted_dataset.select(range(start_idx, min(start_idx + batch_size, total_test_number)))\n",
    "\n",
    "        prompts = batch[\"prompt\"]\n",
    "        tokenized_prompt = llm_tokenizer(prompts, padding='longest', return_tensors='pt', truncation=True, max_length=512)\n",
    "\n",
    "        input_ids = tokenized_prompt.input_ids.to(device)\n",
    "        attention_mask = tokenized_prompt.attention_mask.to(device)   \n",
    "\n",
    "        stopping_criteria = stop_sequences_criteria(llm_tokenizer, input_ids.shape[1], input_ids.shape[0])\n",
    "        retrieval_kwargs = {}\n",
    "\n",
    "        if \"retrieval_embeds\" in batch.column_names:\n",
    "            embeds_batch = batch[\"retrieval_embeds\"]\n",
    "            # Flatten and convert to tensor\n",
    "            embeds = [torch.tensor(vec, dtype=torch.float32) for sublist in embeds_batch for vec in sublist]\n",
    "            embeds = torch.stack(embeds).to(device)\n",
    "            retrieval_kwargs[\"retrieval_embeds\"] = embeds\n",
    "            stopping_criteria = stop_sequences_criteria(llm_tokenizer, 0, input_ids.shape[0])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_output = llm.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=52,\n",
    "                pad_token_id=llm_tokenizer.pad_token_id,\n",
    "                **retrieval_kwargs,\n",
    "            )\n",
    "\n",
    "        output = llm_tokenizer.batch_decode(generated_output, skip_special_tokens=True)\n",
    "        full_completion.extend([x.strip() for x in output])\n",
    "\n",
    "        # input_length = input_ids.shape[1] \n",
    "        # answer = llm_tokenizer.batch_decode(generated_output[:, input_length:], skip_special_tokens=True)\n",
    "        # generated_answers.extend([x.strip() for x in answer])\n",
    "\n",
    "        progress_bar.update(batch_size)\n",
    "\n",
    "    return full_completion  #, generated_answers\n",
    "\n",
    "eval_batch_size = 1\n",
    "\n",
    "prompt_and_results = llm_for_open_generation_dataset(\n",
    "    llm=model,\n",
    "    llm_tokenizer=tokenizer,\n",
    "    formatted_dataset=formatted_test_data,\n",
    "    batch_size=eval_batch_size,\n",
    "    enable_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3365b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: Siapakah yang menemuka benua Amerika ?\n",
      "label: orang-orang Viking\n",
      "Jawaban: Amerika adalah benua yang ditemukan oleh Amerika Serikat, yang berarti bahwa benua itu ditemukan oleh orang-orang Amerika.\n",
      "\n",
      "query: Dimanakah letak Donggala ?\n",
      "label: Sulawesi Tengah, Indonesia\n",
      "Kabupaten Donggala adalah sebuah kabupaten di Sulawesi Tengah, Indonesia.\n",
      "\n",
      "query: Siapa bapak Teknik industri?\n",
      "label: Frederick Winslow Taylor\n",
      "1.\n",
      "\n",
      "query: Kapan Penghulu Rasyid meninggal ?\n",
      "label: 15 Desember 1861\n",
      "1998\n",
      "\n",
      "query: seberapa luas kah samudera pasifik?\n",
      "label: 179,7 juta km²\n",
      "16.\n",
      "\n",
      "query: apakah yang dimaksud denga geisha ?\n",
      "label: seniman-penghibur (entertainer) tradisional Jepang\n",
      "seorang wanita yang berpakaian dengan pakaian khas, berpakaian dengan kain khusus, dan berpakaian dengan pakaian yang terkunci, dan berpakaian dengan pakaian yang ter\n",
      "\n",
      "query: Kapan Bank BCA mengeluarkan kartu debit?\n",
      "label: 2000-an\n",
      "1998\n",
      "\n",
      "query: Dimana kantor pusat General Motors?\n",
      "label: Detroit, Michigan, Amerika Serikat\n",
      "Detroit, Michigan, Amerika Serikat\n",
      "\n",
      "query: Berapa luas kota Blitar?\n",
      "label: 32,58 km²\n",
      "1.\n",
      "\n",
      "query: Siapa yang menciptakan serial manga Crows?\n",
      "label: Hiroshi Takahashi\n",
      "Tentu, itu adalah karya dari dua penulis, yaitu Tatsuya Tachibana dan Tatsuya Tachibana.\n",
      "\n",
      "query: siapakah karakter utama serial anime dan manga Eyeshield 21?\n",
      "label: Sena\n",
      "Shinobu Kaito adalah karakter utama dari serial anime dan manga Eyeshield 21.\n",
      "\n",
      "query: Siapakah yang merumuskan naskah proklamasi ?\n",
      "label: Tadashi Maeda, Tomegoro Yoshizumi, S. Nishijima, S. Miyoshi, Mohammad Hatta, Soekarno, dan Achmad Soebardjo\n",
      "Jawaban: Soekarno\n",
      "\n",
      "query: Bagaimanakah sistem pemerintahan di Jepang ?\n",
      "label: monarki konstitusional\n",
      "monarki federal dengan pemerintahan kongres\n",
      "\n",
      "query: kapankah Gerakan Pemuda Ansor didirikan?\n",
      "label: 24 April 1934\n",
      "1950\n",
      "\n",
      "1950\n",
      "\n",
      "query: Apakah yang diceritakan dalam The Years of Rice and Salt?\n",
      "label: dunia tanpa peradaban Kristen dan Eropa\n",
      "Kisah seorang perempuan yang hidup di tengah perubahan zaman, terutama dalam peristiwa perang dunia II, dan kisah seorang pria yang hidup di tengah perubahan zaman, ter\n",
      "\n",
      "query: apakah pendidikan terakhir Budi Susilo Soepandji?\n",
      "label: Doktor\n",
      "Sesuai dengan data yang terdapat di Buku Panduan Pendidikan dan Pelatihan, Budi Susilo Soepandji adalah seorang guru yang memiliki pendidikan terakhir S1 dari Universitas Negeri Surabaya\n",
      "\n",
      "query: dimanakah letak Cekungan Tarim?\n",
      "label: Xinjiang China\n",
      "Cekungan Tarim adalah sebuah cekungan yang terletak di bagian tengah Samudra Pasifik, di antara dua pulau besar, yaitu Pulau Sumatera dan Pulau Borneo.\n",
      "\n",
      "query: terbuat dari apakah Genta ?\n",
      "label: logam\n",
      "kain kain kain kain kain kain kain kain kain kain kain kain kain kain kain kain kain kain kain kain kain kain kain kain kain kain\n",
      "\n",
      "query: Siapakah R.L. Stine?\n",
      "label: penulis dan produser Amerika Serikat\n",
      "R.L\n",
      "\n",
      "query: Siapakah yang menggagas Determinisme biologis?\n",
      "label: Thornstein Veblen\n",
      "Charles Darwin\n",
      "\n",
      "query: Apakah nama lagu kebangsaan Jepang?\n",
      "label: Kimigayo\n",
      "Nihon no Kuni\n",
      "\n",
      "1.\n",
      "\n",
      "query: Dimana Konsili Kartago diadakan?\n",
      "label: kota Kartago di Afrika\n",
      "Konsili Kartago adalah sebuah konsili yang diadakan di Kartago, sebuah kota di wilayah modern Tunisia.\n",
      "\n",
      "query: berapakah luas Bendung Katulampa?\n",
      "label: 5.000 hektare\n",
      "1.\n",
      "\n",
      "query: Apa nama ilmiah tumbuhan kaktus ?\n",
      "label: Cactaceae\n",
      "Cactus adalah nama umum untuk tumbuhan kaktus yang tergolong dalam kelas **Cactaceae**.\n",
      "\n",
      "query: Kapan perahu pertama dibuat ?\n",
      "label: masa Neolitikum\n",
      "1.\n",
      "\n",
      "query: dari manakah tari angin mamiri berasal?\n",
      "label: suku Bugis di Sulawesi Selatan\n",
      "1.\n",
      "\n",
      "query: Kapan Yekaterina I lahir ?\n",
      "label: 15 April 1684\n",
      "1638\n",
      "\n",
      "Kapan Yekaterina I lahir ?\n",
      "\n",
      "query: Kapan Banjir Besar Gun-Yu pertama kali terjadi ?\n",
      "label: 3 SM\n",
      "1931\n",
      "\n",
      "1931 adalah tahun pertama terjadinya Banjir Besar Gun-Yu.\n",
      "\n",
      "query: kapankah presiden soeharto meninggal dunia?\n",
      "label: 27 Januari 2008\n",
      "1.\n",
      "\n",
      "query: Kapan Johann Ludwig Tieck lahir ?\n",
      "label: 31 Mei 1773\n",
      "1770\n",
      "\n",
      "query: Dimana letak Museum Anak Kolong Tangga?\n",
      "label: alan Sriwedani No. 1, Yogyakarta\n",
      "Kota Bandung, Jawa Barat, Indonesia\n",
      "\n",
      "query: apakah yang dimaksud dengan Hiragana?\n",
      "label: cara penulisan bahasa Jepang dan mewakili sebutan sukukata\n",
      "kata-kata yang digunakan dalam bahasa Jepang untuk menulis kata-kata dalam bahasa Jepang, dan juga untuk menulis kata-kata dalam bahasa Jepang dengan menggunakan alat tulis Jepang.\n",
      "\n",
      "query: siapakah pendiri klub Leicester City?\n",
      "label: sekelompok pemuda Wyggeston School\n",
      "William Henry Hargreaves adalah pendiri klub Leicester City.\n",
      "\n",
      "query: Apa lambang dari perusahaan Paramount Pictures?\n",
      "label: gunung dengan lingkaran dari puluhan bintang\n",
      "Pertanyaan: Apa lambang dari perusahaan Paramount Pictures?\n",
      "\n",
      "query: Apa nama pesawat terbang pertama yang dimiliki Indonesia?\n",
      "label: Dakota RI-001 Seulawah\n",
      "Sriwijaya adalah nama pesawat terbang pertama yang dimiliki Indonesia.\n",
      "\n",
      "query: Siapakah raja pertama kerajaan Yordania ?\n",
      "label: Abdullah I\n",
      "Ibn Sa'd\n",
      "\n",
      "query: tahun berapakah Universitas Islam Negeri Sumatera Utara didirikan?\n",
      "label: 1973\n",
      "1973\n",
      "\n",
      "query: kapankah perum PNRI didirikan?\n",
      "label: 1809\n",
      "1.\n",
      "\n",
      "query: Apakah yang dimaksud dengan korvet ?\n",
      "label: jenis kapal perang yang lebih kecil dari fregat dan lebih besar dari kapal patroli pantai\n",
      "korsleting atau korsleting adalah perangkap yang digunakan untuk menghambat aliran air atau udara.\n",
      "\n",
      "query: Di negara manakah Moehammad Joesoef Ronodipoero menjadi duta besar?\n",
      "label: Uruguay, Argentina, dan Chili\n",
      "Indonesia\n",
      "\n",
      "query: Bagaimana Porifera berkembangbiak?\n",
      "label: seksual atau aseksual\n",
      "porifera memperbanyak diri melalui pembelahan sel.\n",
      "\n",
      "query: Dari mana asal kata Bodas ?\n",
      "label: bahasa Sunda\n",
      "Bodas adalah kata yang berasal dari bahasa Indonesia.\n",
      "\n",
      "query: apa nam mata uang thailand ?\n",
      "label: baht\n",
      "Baht Thailan (THB)\n",
      "\n",
      "query: Kapan The Best Damn Thing dirilis?\n",
      "label: 17 April 2007\n",
      "1.\n",
      "\n",
      "query: Sejak kapan Yakuza mulai muncul di Jepang ?\n",
      "label: 1612\n",
      "1989\n",
      "\n",
      "1989\n",
      "\n",
      "query: apakah nama mata uang jepang?\n",
      "label: Yen\n",
      "Yen adalah nama mata uang jepang.\n",
      "\n",
      "query: siapakah pendiri Proton?\n",
      "label: Mahathir Mohammad\n",
      "Konstantin Pobedonnikov adalah pendiri Proton.\n",
      "\n",
      "query: siapakah Grandmaster termuda di indonesia?\n",
      "label: Susanto Megaranto\n",
      "Rujuklah latar belakang: 1.\n",
      "\n",
      "query: apakah kabupaten terluas di Provinsi Jawa Timur?\n",
      "label: Banyuwangi\n",
      "Kabupaten Malang adalah kabupaten terluas di Jawa Timur, dengan luas 10.\n",
      "\n",
      "query: Apakah gereja terbesar di Swedia?\n",
      "label: Gereja Swedia\n",
      "Gereja Lutheran terbesar di Swedia adalah Gereja Lutheran di Stockholm, yang memiliki 1,2 juta anggota.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, pr_and_res in enumerate(prompt_and_results):\n",
    "    print('query:', formatted_test_data['query'][i])\n",
    "    print('label:', formatted_test_data['label'][i])\n",
    "    print(pr_and_res)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unicodedata\n",
    "# import regex\n",
    "\n",
    "# class SimpleTokenizer(object):\n",
    "#     ALPHA_NUM = r'[\\p{L}\\p{N}\\p{M}]+'\n",
    "#     NON_WS = r'[^\\p{Z}\\p{C}]'\n",
    "\n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             annotators: None or empty set (only tokenizes).\n",
    "#         \"\"\"\n",
    "#         self._regexp = regex.compile(\n",
    "#             '(%s)|(%s)' % (self.ALPHA_NUM, self.NON_WS),\n",
    "#             flags=regex.IGNORECASE + regex.UNICODE + regex.MULTILINE\n",
    "#         )\n",
    "\n",
    "#     def tokenize(self, text, uncased=False):\n",
    "#         matches = [m for m in self._regexp.finditer(text)]\n",
    "#         if uncased:\n",
    "#             tokens = [m.group().lower() for m in matches]\n",
    "#         else:\n",
    "#             tokens = [m.group() for m in matches]\n",
    "#         return tokens\n",
    "\n",
    "# def _normalize(text):\n",
    "#     return unicodedata.normalize('NFD', text)\n",
    "\n",
    "# def has_answer(answers, text, tokenizer=SimpleTokenizer()) -> bool:\n",
    "#     \"\"\"Check if a document contains an answer string.\"\"\"\n",
    "#     text = _normalize(text)\n",
    "#     text = tokenizer.tokenize(text, uncased=True)\n",
    "\n",
    "#     for answer in answers:\n",
    "#         answer = _normalize(answer)\n",
    "#         answer = tokenizer.tokenize(answer, uncased=True)\n",
    "#         for i in range(0, len(text) - len(answer) + 1):\n",
    "#             if answer == text[i: i + len(answer)]:\n",
    "#                 return True\n",
    "#     return False\n",
    "\n",
    "# def get_substring_match_score(outputs,answers):\n",
    "#     \"\"\"\n",
    "#     outputs: [string1,string2]\n",
    "#     answers: [\n",
    "#                 [string1_1,string1_2],\n",
    "#                 [string2_1,string2_2]\n",
    "#              ]\n",
    "#     \"\"\"\n",
    "#     import numpy as np\n",
    "#     assert len(outputs) == len(answers)\n",
    "#     if not isinstance(answers[0],list):\n",
    "#         answers = [[x] for x in answers]\n",
    "#     substring_match_scores = []\n",
    "#     answer_lengths = []\n",
    "#     for output,answer in zip(outputs,answers):\n",
    "#         if has_answer(answer,output): # EM evaluation\n",
    "#             substring_match_scores.append(1.0)\n",
    "#         else:\n",
    "#             substring_match_scores.append(0.0)\n",
    "        \n",
    "#         answer_lengths.append(len(output.split()))\n",
    "\n",
    "#     substring_match = round(sum(substring_match_scores)/len(outputs), 4)\n",
    "#     lens = round(np.mean(answer_lengths), 4)\n",
    "\n",
    "#     return substring_match,substring_match_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2760e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"dataset\": \"final_dataset\",\n",
      "    \"batch_size\": 2,\n",
      "    \"include_retrieval\": true,\n",
      "    \"avg_prompt_length\": 186.92920353982302,\n",
      "    \"model\": \"../output/finetuned/finished_model\",\n",
      "    \"substring_match\": 0.0,\n",
      "    \"retriever\": \"intfloat/multilingual-e5-small\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# from collections import Counter\n",
    "# import numpy as np\n",
    "# import string\n",
    "\n",
    "\n",
    "# def normalize_answer(s):\n",
    "#     def remove_articles(text):\n",
    "#         return regex.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "#     def white_space_fix(text):\n",
    "#         return ' '.join(text.split())\n",
    "\n",
    "#     def remove_punc(text):\n",
    "#         exclude = set(string.punctuation)\n",
    "#         return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "#     def lower(text):\n",
    "#         return text.lower()\n",
    "\n",
    "#     return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "# def f1_score(prediction, ground_truth):\n",
    "#     prediction_tokens = normalize_answer(prediction).split()\n",
    "#     ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "#     common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "#     num_same = sum(common.values())\n",
    "#     if num_same == 0:\n",
    "#         return 0\n",
    "#     precision = 1.0 * num_same / len(prediction_tokens)\n",
    "#     recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "#     f1 = (2 * precision * recall) / (precision + recall)\n",
    "#     return f1\n",
    "\n",
    "\n",
    "# def f1(prediction, ground_truths):\n",
    "#     return max([f1_score(prediction, gt) for gt in ground_truths])\n",
    "\n",
    "# def eval_truthfulqa(outputs,answers):\n",
    "\n",
    "#     f1_scores = []\n",
    "#     # rl_scores = []\n",
    "#     for output,answer in zip(outputs,answers):\n",
    "\n",
    "#         f1_scores.append(f1(output, answer))\n",
    "#         # rl_scores.append(rl(output, answer))\n",
    "\n",
    "#     F1 = round(np.mean(f1_scores), 4)\n",
    "#     # RL = round(np.mean(rl_scores), 4)\n",
    "\n",
    "#     return F1,  f1_scores #,RL,rl_scores\n",
    "\n",
    "# import json\n",
    "\n",
    "# # Ambil jawaban referensi dari data\n",
    "# answers = [x['label'] for x in formatted_test_data]\n",
    "\n",
    "# # Ganti args dengan parameter eksplisit\n",
    "# eval_metrics = 'substring_match'  # atau 'fact_checking_acc', 'truthfulqa_f1_rl'\n",
    "# use_rag = True\n",
    "# avg_prompt_length = sum(len(x) for x in formatted_test_data[\"prompt\"]) / len(formatted_test_data)\n",
    "# model_name_or_path = \"../output/finetuned/finished_model\"\n",
    "# retriever_name_or_path = \"intfloat/multilingual-e5-small\"\n",
    "\n",
    "# # Evaluasi\n",
    "# if eval_metrics == 'substring_match':\n",
    "#     score, score_per_sample = get_substring_match_score(generated_results, answers)\n",
    "# elif eval_metrics == 'truthfulqa_f1_rl':\n",
    "#     f1, rl, f1_scores, rl_scores = eval_truthfulqa(generated_results, answers)\n",
    "#     score = f\"{f1}-{rl}\"\n",
    "#     score_per_sample = [(f1_score, rl_score) for f1_score, rl_score in zip(f1_scores, rl_scores)]\n",
    "\n",
    "# # Tampilkan hasil\n",
    "# result_dict = {\n",
    "#     \"dataset\": \"final_dataset\",\n",
    "#     \"batch_size\": eval_batch_size,\n",
    "#     \"include_retrieval\": use_rag,\n",
    "#     \"avg_prompt_length\": avg_prompt_length,\n",
    "#     \"model\": model_name_or_path,\n",
    "#     f\"{eval_metrics}\": score,\n",
    "# }\n",
    "\n",
    "# if retriever_name_or_path is not None:\n",
    "#     result_dict[\"retriever\"] = retriever_name_or_path\n",
    "\n",
    "# print(json.dumps(result_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from datasets import Dataset\n",
    "from metrics import evaluate_substringmatch_f1\n",
    "\n",
    "testing_results = []\n",
    "labels = formatted_test_data['label']\n",
    "for i, row in enumerate(formatted_test_data):\n",
    "    label = labels[i]\n",
    "    generated_result = generated_results[i]\n",
    "    sm, f1 = evaluate_substringmatch_f1(generated_result.strip(), label.strip())\n",
    "\n",
    "    testing_results.append(\n",
    "        {\n",
    "            'query': row['query'],\n",
    "            'passages': row['sorted_truncPassages'],\n",
    "            'full_prompt_and_completion': prompt_and_results[i], \n",
    "            'completion': generated_result, \n",
    "            'label': label, \n",
    "            'em': sm, \n",
    "            'f1': f1\n",
    "        }\n",
    "    )\n",
    "\n",
    "xRAG_results= Dataset.from_list(testing_results)\n",
    "print(\"Hasil dari xRAG\")\n",
    "print(\"rerata substring match:\", sum(xRAG_results['em'])/len(xRAG_results))\n",
    "print(\"rerata F1:\", sum(xRAG_results['f1'])/len(xRAG_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8946f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Direktori tujuan\n",
    "output_dir = \"../output/test_scores/qwen1.7_e5small_batch2_epoch3_2025-06-27-21.40\"\n",
    "\n",
    "# Buat direktori jika belum ada\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Simpan result_dict ke file JSON\n",
    "result_path = os.path.join(output_dir, \"result_dict.json\")\n",
    "with open(result_path, \"w\") as f:\n",
    "    json.dump(result_dict, f, indent=4)\n",
    "\n",
    "# Simpan score_per_sample ke file JSON\n",
    "score_path = os.path.join(output_dir, \"score_per_sample.json\")\n",
    "with open(score_path, \"w\") as f:\n",
    "    json.dump(score_per_sample, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
