{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0e1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import XRAG_TOKEN, load_and_format_dataset, encode_with_chat_format_finetune, encode_with_chat_format_pretrain, add_retriever_embeddings, collator, load_config\n",
    "from utils import get_nll_loss, get_kl_loss, validate_during_pretrain, validate_during_finetune\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, get_scheduler, set_seed\n",
    "from tokenizers import AddedToken\n",
    "from functools import partial\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "import torch\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "sys.path.append('..')\n",
    "from model.E5Retriever import E5Retriever\n",
    "from model.xQwen3 import XQwen3Config, XQwen3ForCausalLM\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf74f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fase latihan: finetune\n",
      "berhasil memuat dataset\n"
     ]
    }
   ],
   "source": [
    "args = load_config(\"../config/finetune.yaml\")\n",
    "\n",
    "print(\"Fase latihan:\", args.task_type)\n",
    "dataset = load_and_format_dataset(args.dataset_path, args.query_col, args.ans_col, args.psg_col, args.task_type, args.max_samples)\n",
    "\n",
    "if 'test' in dataset:\n",
    "    dataset.pop('test')\n",
    "\n",
    "print(\"berhasil memuat dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa9cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_247872\\2606651468.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  projector_ckpt = torch.load(args.projector_path)\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\_utils.py:88: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  untyped_storage = torch.UntypedStorage(self.size(), device=device)\n"
     ]
    }
   ],
   "source": [
    "set_seed(args.seed)\n",
    "projector_ckpt = torch.load(args.projector_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840ed89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memuat retriever dan tokenizernya...\n",
      "memuat tokenizer generatif...\n",
      "memuat model generatif...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.72it/s]\n",
      "Some weights of XQwen3ForCausalLM were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['projector.projector.0.bias', 'projector.projector.0.weight', 'projector.projector.2.bias', 'projector.projector.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_247872\\2694388094.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(args.projector_path), strict=False)\n"
     ]
    }
   ],
   "source": [
    "# Loading model retriever\n",
    "print('memuat retriever dan tokenizernya...')\n",
    "retriever = E5Retriever(args.retriever_name_or_path)\n",
    "retriever_tokenizer = AutoTokenizer.from_pretrained(args.retriever_name_or_path)\n",
    "retriever_hidden_size = retriever.get_embed_dim()\n",
    "retriever.eval()\n",
    "retriever.to('cuda:0')\n",
    "\n",
    "print('memuat tokenizer generatif...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model_name_or_path\n",
    ")\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "print('memuat model generatif...')\n",
    "# TODO: jika finetuning, muat projector terpisah baru gabungkan ke xqwen3\n",
    "config = XQwen3Config.from_pretrained(args.model_name_or_path, retriever_hidden_size=retriever_hidden_size)\n",
    "if args.model_dtype == 'bfloat16':\n",
    "    model_dtype = torch.bfloat16\n",
    "elif args.model_dtype == 'float16':\n",
    "    model_dtype = torch.float16\n",
    "model = XQwen3ForCausalLM.from_pretrained(  # XLlamaForCausalLM\n",
    "    args.model_name_or_path,\n",
    "    config=config,\n",
    "    torch_dtype = model_dtype\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "if args.task_type == 'finetune':\n",
    "    model.load_state_dict(torch.load(args.projector_path), strict=False)\n",
    "\n",
    "# Mengatur pad_token pada tokenizer llama dengan token yang sudah ada dalam Llama\n",
    "if tokenizer.pad_token_id is None:\n",
    "    print('Menambahkan pad token ke tokenizer')\n",
    "\n",
    "    if args.chat_format == 'llama':    \n",
    "        pad_token = \"<|finetune_right_pad_id|>\"\n",
    "        tokenizer.pad_token = pad_token\n",
    "        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "    elif args.chat_format == 'qwen':\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Menambahkan token baru (xrag) ke perbendaharaan tokenizer llama\n",
    "num_added_tokens = 0\n",
    "num_added_tokens += tokenizer.add_tokens([AddedToken(XRAG_TOKEN,lstrip=False,rstrip=False)])\n",
    "xrag_token_id = tokenizer.convert_tokens_to_ids(XRAG_TOKEN)\n",
    "\n",
    "model.set_xrag_token_id(xrag_token_id)\n",
    "if num_added_tokens > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b0bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode chat untuk pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15360/15360 [00:28<00:00, 537.47 examples/s]\n",
      "Map: 100%|██████████| 1695/1695 [00:02<00:00, 613.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if args.task_type == 'finetune':\n",
    "    print('encode chat untuk finetune...')\n",
    "    encode_function = partial(\n",
    "        encode_with_chat_format_finetune,\n",
    "        llm_tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "        use_rag_tuning = args.use_rag_tuning,\n",
    "        use_retriever_embed = not (retriever is None),\n",
    "        chat_format = args.chat_format,\n",
    "    )\n",
    "\n",
    "if args.task_type== 'pretrain':\n",
    "    print('encode chat untuk pretraining')\n",
    "    encode_function = partial(\n",
    "        encode_with_chat_format_pretrain,\n",
    "        llm_tokenizer = tokenizer,\n",
    "        max_seq_length = args.max_seq_length,\n",
    "        retrieval_embed_length=args.retrieval_embed_length,\n",
    "        chat_format = args.chat_format\n",
    "    )\n",
    "\n",
    "lm_datasets = dataset.map(encode_function)\n",
    "lm_datasets.set_format(type=\"pt\")\n",
    "\n",
    "if args.task_type == 'finetune':\n",
    "    print('membuang row yang seluruh labelsnya bernilai -100 (tidak ada porsi assistant sama sekali)...')\n",
    "    for split in lm_datasets.keys():\n",
    "        if \"background\" in lm_datasets[split].column_names:\n",
    "            lm_datasets[split].remove_columns('background')\n",
    "        lm_datasets[split] = lm_datasets[split].filter(lambda example: (example['labels'] != -100).any())\n",
    "        if args.alpha_kl is not None and args.alpha_kl > 0.0:\n",
    "            lm_datasets[split] = lm_datasets[split].filter(\n",
    "                lambda example: \n",
    "                (example['labels']!=-100).sum() == (example['xrag_labels']!=-100).sum()\n",
    "            )\n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "dev_dataset = lm_datasets['dev'] # if args.task_type == 'pretrain' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Menambahkan retriever_embeddings ke dataset sebelum pelatihan\n",
    "print('membuat embeddings untuk dokumen konteks dengan retriever...')\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda example: add_retriever_embeddings(example, retriever, retriever_tokenizer, args.retrieval_context_length, text_col='retriever_input_text')\n",
    ")\n",
    "\n",
    "if dev_dataset is not None:\n",
    "    dev_dataset = dev_dataset.map(\n",
    "        lambda example: add_retriever_embeddings(example, retriever, retriever_tokenizer, args.retrieval_context_length, text_col='retriever_input_text')\n",
    "    )\n",
    "\n",
    "# Hapus objek retriever dan modelnya\n",
    "print('Menghapus retriever...')\n",
    "del retriever.model  \n",
    "del retriever\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "if args.save_embeddings_generated:\n",
    "    ds_with_embeddings = DatasetDict({\n",
    "        'train': train_dataset, \n",
    "        'dev': dev_dataset\n",
    "    })\n",
    "    ds_with_embeddings.save_to_disk(os.path.join(args.processing_steps_output_dir, f\"{args.task_type}-create_embedding_step-learningrate1e-2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5375245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "lm_datasets = load_from_disk('../../generated_data/xRAG-process/finetune-create_embedding_step')\n",
    "train_dataset = lm_datasets['train']\n",
    "dev_dataset = lm_datasets['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dda5e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menginisialisasi dataloader untuk training...\n",
      "Menginisialisasi dataloader untuk validasi...\n",
      "Mengatur agar hanya layer yang menjadi bagian dr projector saja yang diupdate selama training...\n"
     ]
    }
   ],
   "source": [
    "collate_fn = partial(\n",
    "    collator,\n",
    "    llm_tokenizer=tokenizer, \n",
    "    xrag_input_ids_col='xrag_input_ids',\n",
    "    xrag_labels_col = 'xrag_labels', \n",
    "    text_input_ids_col = 'input_ids', \n",
    "    text_labels_col = 'labels', \n",
    "    retriever_embeds_col='retriever_embeddings'\n",
    ")\n",
    "\n",
    "print('Menginisialisasi dataloader untuk training...')\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "if dev_dataset is not None:\n",
    "    print('Menginisialisasi dataloader untuk validasi...')\n",
    "    dev_dataloader = DataLoader(\n",
    "        dev_dataset,\n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=args.per_device_train_batch_size\n",
    "    )\n",
    "\n",
    "if args.update_projector_only:\n",
    "    print('Mengatur agar hanya layer yang menjadi bagian dr projector saja yang diupdate selama training...')\n",
    "    for n,p in model.named_parameters():\n",
    "        if 'projector' not in n:p.requires_grad = False\n",
    "        else:p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],lr=args.learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8723058",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "if args.task_type == 'finetune':\n",
    "    output_dir = os.path.join(args.output_dir, \"finetuned\")\n",
    "    output_dir = os.path.join(output_dir, current_time)\n",
    "    model_output_dir = os.path.join(output_dir, 'finished_model')\n",
    "\n",
    "elif args.task_type == 'pretrain':\n",
    "    output_dir = os.path.join(args.output_dir, 'pretrained')\n",
    "    output_dir = os.path.join(output_dir, current_time)\n",
    "    model_output_dir = os.path.join(output_dir, f\"{args.retriever_name_or_path[-8:]}_{args.chat_format}{args.model_size}_batch{args.per_device_train_batch_size}_{args.num_train_epochs}epoch_lr{args.learning_rate}\")\n",
    "\n",
    "# 2. Simpan loss ke log file di dalam output_dir/loss_logs\n",
    "loss_log_dir = os.path.join(output_dir, \"loss_logs\")\n",
    "os.makedirs(loss_log_dir, exist_ok=True)\n",
    "\n",
    "def append_loss_to_file(filename, value):\n",
    "    filepath = os.path.join(loss_log_dir, filename)\n",
    "    with open(filepath, \"a\") as f:\n",
    "        f.write(f\"{value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346b3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25600 [00:00<?, ?it/s]c:\\Users\\LENOVO\\Documents\\Skripsi\\post-retrieval-eval\\xRAG\\language_modeling\\preprocessing.py:396: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ret['retriever_embeddings'] = torch.stack([torch.tensor(x[retriever_embeds_col]).to('cuda:0') for x in samples])\n",
      "  0%|          | 0/25600 [00:00<?, ?it/s, epoch=1, batch=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 281/25600 [02:38<4:04:03,  1.73it/s, epoch=1, batch=281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 672/25600 [06:14<3:56:47,  1.75it/s, epoch=1, batch=672]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 956/25600 [09:28<3:31:45,  1.94it/s, epoch=1, batch=956] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2560/25600 [25:17<4:05:42,  1.56it/s, epoch=1, batch=2560] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Validating after epoch 1...\n",
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n",
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2561/25600 [28:39<390:11:45, 60.97s/it, epoch=2, batch=1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted sum of KL and NLL loss after epoch 1: 6.094398741166078\n",
      "SELESAI MENYIMPAN PROJECTOR EPOCH 1\n",
      "\n",
      "========================================================================\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 3446/25600 [37:12<3:15:32,  1.89it/s, epoch=2, batch=886] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 3824/25600 [40:48<3:30:59,  1.72it/s, epoch=2, batch=1264]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 4173/25600 [44:11<3:25:42,  1.74it/s, epoch=2, batch=1613] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 1 dan 2 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5120/25600 [53:39<3:10:20,  1.79it/s, epoch=2, batch=2560] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Validating after epoch 2...\n",
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n",
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5121/25600 [56:12<262:54:39, 46.22s/it, epoch=3, batch=1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted sum of KL and NLL loss after epoch 2: 5.681288648409893\n",
      "SELESAI MENYIMPAN PROJECTOR EPOCH 2\n",
      "\n",
      "========================================================================\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5896/25600 [1:03:43<3:29:17,  1.57it/s, epoch=3, batch=776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 6300/25600 [1:07:26<3:08:18,  1.71it/s, epoch=3, batch=1180]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 1 dan 2 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 7301/25600 [1:17:08<2:37:44,  1.93it/s, epoch=3, batch=2181]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 7680/25600 [1:21:20<3:03:33,  1.63it/s, epoch=3, batch=2560]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Validating after epoch 3...\n",
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n",
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 7681/25600 [1:23:16<175:05:03, 35.18s/it, epoch=4, batch=1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted sum of KL and NLL loss after epoch 3: 5.559235589664311\n",
      "SELESAI MENYIMPAN PROJECTOR EPOCH 3\n",
      "\n",
      "========================================================================\n",
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 7798/25600 [1:24:26<3:04:27,  1.61it/s, epoch=4, batch=118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 7937/25600 [1:25:47<2:29:01,  1.98it/s, epoch=4, batch=257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8205/25600 [1:28:24<2:34:41,  1.87it/s, epoch=4, batch=525]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10240/25600 [1:47:36<8:07:50,  1.91s/it, epoch=4, batch=2560]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Validating after epoch 4...\n",
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n",
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10241/25600 [1:53:23<449:27:07, 105.35s/it, epoch=5, batch=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted sum of KL and NLL loss after epoch 4: 5.516756846289752\n",
      "SELESAI MENYIMPAN PROJECTOR EPOCH 4\n",
      "\n",
      "========================================================================\n",
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 10727/25600 [1:58:11<2:30:45,  1.64it/s, epoch=5, batch=487] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 10959/25600 [2:00:22<2:02:40,  1.99it/s, epoch=5, batch=719]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 1 dan 2 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 11984/25600 [2:10:01<2:09:25,  1.75it/s, epoch=5, batch=1744]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 12800/25600 [2:18:26<1:48:13,  1.97it/s, epoch=5, batch=2560] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Validating after epoch 5...\n",
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n",
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 12801/25600 [2:23:30<324:55:36, 91.39s/it, epoch=6, batch=1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted sum of KL and NLL loss after epoch 5: 5.478108436395759\n",
      "SELESAI MENYIMPAN PROJECTOR EPOCH 5\n",
      "\n",
      "========================================================================\n",
      "Starting epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 13461/25600 [2:29:52<2:00:16,  1.68it/s, epoch=6, batch=661]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 13767/25600 [2:32:48<1:56:23,  1.69it/s, epoch=6, batch=967]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 14874/25600 [2:43:07<1:35:13,  1.88it/s, epoch=6, batch=2074]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 1 dan 2 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15360/25600 [2:47:58<1:40:10,  1.70it/s, epoch=6, batch=2560]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Validating after epoch 6...\n",
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n",
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15361/25600 [2:51:28<180:19:00, 63.40s/it, epoch=7, batch=1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted sum of KL and NLL loss after epoch 6: 5.47192469081272\n",
      "SELESAI MENYIMPAN PROJECTOR EPOCH 6\n",
      "\n",
      "========================================================================\n",
      "Starting epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 15953/25600 [2:57:08<1:34:18,  1.70it/s, epoch=7, batch=593]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 1 dan 2 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 16173/25600 [2:59:16<1:25:40,  1.83it/s, epoch=7, batch=813]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17296/25600 [3:10:03<1:11:57,  1.92it/s, epoch=7, batch=1936]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 17920/25600 [3:16:03<1:06:50,  1.92it/s, epoch=7, batch=2560]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Validating after epoch 7...\n",
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n",
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 17921/25600 [3:17:56<73:35:37, 34.50s/it, epoch=8, batch=1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted sum of KL and NLL loss after epoch 7: 5.445761097614842\n",
      "SELESAI MENYIMPAN PROJECTOR EPOCH 7\n",
      "\n",
      "========================================================================\n",
      "Starting epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 18654/25600 [3:24:53<1:03:37,  1.82it/s, epoch=8, batch=734]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 19311/25600 [3:31:05<51:55,  2.02it/s, epoch=8, batch=1391]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 20227/25600 [3:40:11<50:57,  1.76it/s, epoch=8, batch=2307]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20480/25600 [3:42:32<49:18,  1.73it/s, epoch=8, batch=2560]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Validating after epoch 8...\n",
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n",
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20481/25600 [3:44:29<50:19:09, 35.39s/it, epoch=9, batch=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted sum of KL and NLL loss after epoch 8: 5.452062168727915\n",
      "SELESAI MENYIMPAN PROJECTOR EPOCH 8\n",
      "\n",
      "========================================================================\n",
      "Starting epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 20755/25600 [3:47:29<43:18,  1.86it/s, epoch=9, batch=275] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 1 dan 2 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 21924/25600 [3:58:59<35:42,  1.72it/s, epoch=9, batch=1444]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22586/25600 [4:05:12<26:44,  1.88it/s, epoch=9, batch=2106]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 23040/25600 [4:09:25<23:11,  1.84it/s, epoch=9, batch=2560]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Validating after epoch 9...\n",
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n",
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 23041/25600 [4:11:42<29:37:02, 41.67s/it, epoch=10, batch=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted sum of KL and NLL loss after epoch 9: 5.423572769434629\n",
      "SELESAI MENYIMPAN PROJECTOR EPOCH 9\n",
      "\n",
      "========================================================================\n",
      "Starting epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 24014/25600 [4:20:55<14:36,  1.81it/s, epoch=10, batch=974] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 24732/25600 [4:28:05<07:02,  2.06it/s, epoch=10, batch=1692] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 1 dan 2 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 25522/25600 [4:35:31<00:45,  1.73it/s, epoch=10, batch=2482]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25600/25600 [4:36:15<00:00,  1.67it/s, epoch=10, batch=2560]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Validating after epoch 10...\n",
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n",
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n",
      "weighted sum of KL and NLL loss after epoch 10: 5.415387588339223\n",
      "SELESAI MENYIMPAN PROJECTOR EPOCH 10\n",
      "\n",
      "dev loss pertama : {'nll': 2.5380962897526502, 'ppl': 12.65555551297889, 'kl': 1.7781512257067138, 'total_loss': 6.094398741166078}\n",
      "dev loss terakhir: {'nll': 2.2738929991166077, 'ppl': 9.717156155964403, 'kl': 1.5707472946113075, 'total_loss': 5.415387588339223}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 162\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev loss terakhir:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dev_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dev_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     best_epoch_dev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mtensor(dev_losses)\u001b[38;5;241m.\u001b[39margmin()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model terbaik berdasarkan *dev loss* terjadi pada epoch ke-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_epoch_dev\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dengan nilai \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdev_losses[best_epoch_dev\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not infer dtype of dict"
     ]
    }
   ],
   "source": [
    "# Add learning rate scheduler\n",
    "num_training_steps = args.num_train_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_warmup_steps=int(num_training_steps * args.warmup_ratio)  # 3% warmup\n",
    ")\n",
    "\n",
    "# Inisialisasi list untuk menyimpan loss\n",
    "accumulation_steps = args.gradient_accumulation_steps\n",
    "\n",
    "nll_train_losses = []\n",
    "kl_train_losses = []\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "epoch_avg_train_losses = []\n",
    "\n",
    "early_stop_patience = 3\n",
    "no_improve_train = 0\n",
    "no_improve_dev = 0\n",
    "best_train_loss = float(\"inf\")\n",
    "best_dev_loss = float(\"inf\")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(args.num_train_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    print(\"======\"*12)  # Pembatas untuk setiap epoch\n",
    "    print(f\"Starting epoch {epoch+1}\")\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        progress_bar.set_postfix({'epoch': epoch+1, 'batch': batch_idx+1})\n",
    "        progress_bar.update(1)\n",
    "        if batch_idx % accumulation_steps == 0:\n",
    "            optimizer.zero_grad()  # deindent jika ingin cancel batch accumulation  \n",
    "\n",
    "        outputs = model(\n",
    "            input_ids = batch['xrag_input_ids'],\n",
    "            attention_mask = batch['xrag_attention_mask'],\n",
    "            retrieval_embeds = batch['retriever_embeddings']\n",
    "        )\n",
    "        del batch['xrag_input_ids']\n",
    "        del batch['xrag_attention_mask']\n",
    "        del batch['retriever_embeddings']\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        labels = batch['xrag_labels']\n",
    "\n",
    "        nll_loss = get_nll_loss(logits=logits, labels=labels, vocab_size=vocab_size)\n",
    "\n",
    "        loss = args.alpha_nll * nll_loss\n",
    "        nll_train_losses.append(loss.item())\n",
    "        append_loss_to_file(\"nll_train_loss.txt\", nll_train_losses[-1])\n",
    "\n",
    "        if args.alpha_kl is not None and args.alpha_kl > 0.0:\n",
    "            ## forward with retrieval tokens\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                teacher_outputs = model(\n",
    "                    input_ids = batch['input_ids'],\n",
    "                    attention_mask = batch['attention_mask'],\n",
    "                )\n",
    "                del batch['input_ids']\n",
    "                del batch['attention_mask']\n",
    "                torch.cuda.empty_cache()\n",
    "                model.train()\n",
    "\n",
    "            kl_loss = get_kl_loss(\n",
    "                teacher_logits=teacher_outputs.logits,\n",
    "                teacher_labels=batch['labels'],\n",
    "                student_logits=outputs.logits,\n",
    "                student_labels=batch['xrag_labels'],\n",
    "                temperature=args.kl_temperature,\n",
    "            )\n",
    "            kl_loss = args.alpha_kl * kl_loss\n",
    "            kl_train_losses.append(kl_loss.item())\n",
    "\n",
    "            if kl_train_losses:\n",
    "                append_loss_to_file(\"kl_train_loss.txt\", kl_train_losses[-1])\n",
    "\n",
    "            loss += kl_loss\n",
    "            \n",
    "            \n",
    "            del batch['labels']\n",
    "            torch.cuda.empty_cache()\n",
    "        del batch['xrag_labels']\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Simpan loss untuk tiap batch\n",
    "        train_losses.append(loss.item())\n",
    "        append_loss_to_file(\"train_loss.txt\", train_losses[-1])\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameter hanya setelah beberapa batch terakumulasi\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()  \n",
    "            lr_scheduler.step() \n",
    "            optimizer.zero_grad() \n",
    "\n",
    "    epoch_avg_loss = epoch_train_loss / len(train_dataloader)\n",
    "    epoch_avg_train_losses.append(epoch_avg_loss)\n",
    "    append_loss_to_file(\"epoch_avg_train_loss.txt\", epoch_avg_train_losses[-1])\n",
    "\n",
    "    # Setelah setiap epoch selesai, lakukan validasi\n",
    "    if dev_dataset is not None:\n",
    "        print(\"------\"*12)\n",
    "        if args.task_type == 'pretrain':\n",
    "            print(f\"Validating after epoch {epoch+1}...\")\n",
    "            ppl = validate_during_pretrain(model, dev_dataloader, len(tokenizer))\n",
    "            print(f\"Perplexity on dev set after epoch {epoch+1}: {ppl}\")\n",
    "\n",
    "            dev_losses.append(ppl.item())\n",
    "        elif args.task_type == 'finetune':\n",
    "            print(f\"Validating after epoch {epoch+1}...\")\n",
    "            metrics = validate_during_finetune(model, dev_dataloader, vocab_size, args)\n",
    "            print(f\"weighted sum of KL and NLL loss after epoch {epoch+1}: {metrics['total_loss']}\")\n",
    "            dev_losses.append(metrics)\n",
    "\n",
    "        append_loss_to_file(\"dev_loss.txt\", dev_losses[-1])\n",
    "\n",
    "    projector_dir = os.path.join(output_dir, \"projector_checkpoints\", f\"epoch_{epoch+1}\")\n",
    "    os.makedirs(projector_dir, exist_ok=True)\n",
    "    projector_state_dict = {\n",
    "        k: v for k, v in model.state_dict().items()\n",
    "        if k.startswith(\"projector.\")\n",
    "    }\n",
    "    torch.save(projector_state_dict, os.path.join(projector_dir, \"projector.pth\"))\n",
    "\n",
    "    # ======== Cek Early Stopping =========\n",
    "    # 1. Cek train loss\n",
    "    if epoch_avg_loss < best_train_loss:\n",
    "        best_train_loss = epoch_avg_loss\n",
    "        no_improve_train = 0\n",
    "    else:\n",
    "        no_improve_train += 1\n",
    "\n",
    "    # 2. Cek dev loss (jika ada)\n",
    "    if dev_dataset is not None:\n",
    "        current_dev_loss = dev_losses[-1]['total_loss']\n",
    "        if current_dev_loss < best_dev_loss:\n",
    "            best_dev_loss = current_dev_loss\n",
    "            no_improve_dev = 0\n",
    "        else:\n",
    "            no_improve_dev += 1\n",
    "\n",
    "    # 3. Jika salah satu tidak membaik selama 3 epoch\n",
    "    if no_improve_train >= early_stop_patience or no_improve_dev >= early_stop_patience:\n",
    "        print(\"⛔ Pelatihan dihentikan karena tidak ada perbaikan pada loss selama 3 epoch berturut-turut.\")\n",
    "        break\n",
    "    print(f\"SELESAI MENYIMPAN PROJECTOR EPOCH {epoch+1}\")\n",
    "    print()\n",
    "\n",
    "print(\"dev loss pertama :\", dev_losses[0])\n",
    "print(\"dev loss terakhir:\", dev_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5212867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../output\\\\finetuned\\\\2025-07-19_10-58-09\\\\finished_model\\\\tokenizer_config.json',\n",
       " '../output\\\\finetuned\\\\2025-07-19_10-58-09\\\\finished_model\\\\special_tokens_map.json',\n",
       " '../output\\\\finetuned\\\\2025-07-19_10-58-09\\\\finished_model\\\\vocab.json',\n",
       " '../output\\\\finetuned\\\\2025-07-19_10-58-09\\\\finished_model\\\\merges.txt',\n",
       " '../output\\\\finetuned\\\\2025-07-19_10-58-09\\\\finished_model\\\\added_tokens.json',\n",
       " '../output\\\\finetuned\\\\2025-07-19_10-58-09\\\\finished_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "model.save_pretrained(model_output_dir)\n",
    "tokenizer.save_pretrained(model_output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
