{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0e1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import XRAG_TOKEN, load_and_format_dataset, encode_with_chat_format_finetune, encode_with_chat_format_pretrain, add_retriever_embeddings, collator\n",
    "from utils import get_nll_loss, get_kl_loss, validate_during_pretrain, validate_during_finetune\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, get_scheduler, set_seed\n",
    "from tokenizers import AddedToken\n",
    "from functools import partial\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "import torch\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "sys.path.append('..')\n",
    "from model.E5Retriever import E5Retriever\n",
    "from model.xQwen3 import XQwen3Config, XQwen3ForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ae9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5120/5120 [00:01<00:00, 4027.31 examples/s]\n",
      "Map: 100%|██████████| 565/565 [00:00<00:00, 4110.96 examples/s]\n",
      "Map: 100%|██████████| 565/565 [00:00<00:00, 2194.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels', 'sorted_truncPassages'], 'dev': ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels', 'sorted_truncPassages'], 'test': ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels', 'sorted_truncPassages']}\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "dataset = load_dataset('khalidrizki/postretrieve-raw-dataset')\n",
    "# Fungsi untuk mengekstrak teks dari ranked_truncPassages_with_labels\n",
    "def extract_sorted_passages(row):\n",
    "    # Mengambil ranked_truncPassages_with_labels yang sudah terurut\n",
    "    passages = row['ranked_truncPassages_with_labels']\n",
    "    \n",
    "    # Mengambil teks dari setiap passage\n",
    "    sorted_texts = [passage['text'] for passage in passages]\n",
    "    \n",
    "    return sorted_texts\n",
    "\n",
    "# Menggunakan method map untuk menerapkan fungsi ke setiap row di dataset\n",
    "dataset = dataset.map(lambda row: {'sorted_truncPassages': extract_sorted_passages(row)}, batched=False)\n",
    "\n",
    "# Memeriksa hasilnya: Pastikan 'sorted_truncPassages' sudah ada di dataset\n",
    "print(dataset.column_names)  # Untuk memastikan nama kolom yang tersedia\n",
    "dataset = dataset.rename_column('answer', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7412121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf74f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fase latihan: finetune\n",
      "berhasil memuat dataset\n"
     ]
    }
   ],
   "source": [
    "class Args: \n",
    "    # dataset config\n",
    "    dataset_path = \"khalidrizki/postretrieve-research-raw-v2\"\n",
    "    query_col = 'query'\n",
    "    ans_col = 'label'\n",
    "    psg_col = 'sorted_truncPassages'\n",
    "    max_samples = None\n",
    "\n",
    "    # model config\n",
    "    model_dtype = 'bfloat16'\n",
    "\n",
    "    # trainer config\n",
    "    seed = 42\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_accumulation_steps = 8\n",
    "    lr_scheduler_type = \"linear\"\n",
    "    warmup_ratio = 0.03\n",
    "    alpha_nll = 1.0\n",
    "\n",
    "    # retriever config\n",
    "    retriever_name_or_path='intfloat/multilingual-e5-small'\n",
    "    retrieval_context_length= 512  # 180\n",
    "\n",
    "    # prompting config\n",
    "    chat_format=\"qwen\"\n",
    "\n",
    "    # Output naming\n",
    "    model_size = \"1.7B\"\n",
    "    output_dir='../output'\n",
    "    \n",
    "    # Config unique to xRAG\n",
    "    update_projector_only = True\n",
    "    save_embeddings_generated = False\n",
    "    processing_steps_output_dir = '../../generated_data/xRAG-process'\n",
    "    \n",
    "    # pretrain\n",
    "    task_type='pretrain'\n",
    "    model_name_or_path = \"Qwen/Qwen3-1.7B\"\n",
    "    num_train_epochs = 10\n",
    "    learning_rate=6.0e-3\n",
    "    alpha_kl = None\n",
    "    kl_temperature=0.0\n",
    "    max_seq_length = 600  # 336\n",
    "    retrieval_embed_length=1\n",
    "\n",
    "    # #  finetune\n",
    "    # task_type=\"finetune\"\n",
    "    # model_name_or_path=\"khalidrizki/xRAG-Qwen3-pretrained\"\n",
    "    # num_train_epochs = 3\n",
    "    # learning_rate = 2.0e-5\n",
    "    # alpha_kl= 2.0\n",
    "    # kl_temperature= 1.0 \n",
    "    # max_seq_length = 1620 # 1024\n",
    "    # retrieval_embed_length=3\n",
    "    # use_rag_tuning = True\n",
    "\n",
    "args = Args()\n",
    "print(\"Fase latihan:\", args.task_type)\n",
    "dataset = load_and_format_dataset(args.dataset_path, args.query_col, args.ans_col, args.psg_col, args.task_type, args.max_samples)\n",
    "\n",
    "if 'test' in dataset:\n",
    "    dataset.pop('test')\n",
    "\n",
    "print(\"berhasil memuat dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840ed89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memuat retriever dan tokenizernya...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memuat tokenizer generatif...\n",
      "memuat model generatif...\n"
     ]
    }
   ],
   "source": [
    "set_seed(args.seed)\n",
    "\n",
    "# Loading model retriever\n",
    "print('memuat retriever dan tokenizernya...')\n",
    "retriever = E5Retriever(args.retriever_name_or_path)\n",
    "retriever_tokenizer = AutoTokenizer.from_pretrained(args.retriever_name_or_path)\n",
    "retriever_hidden_size = retriever.get_embed_dim()\n",
    "retriever.eval()\n",
    "retriever.to('cuda:0')\n",
    "\n",
    "print('memuat tokenizer generatif...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model_name_or_path\n",
    ")\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "print('memuat model generatif...')\n",
    "config = XQwen3Config.from_pretrained(args.model_name_or_path, retriever_hidden_size=retriever_hidden_size)\n",
    "if args.model_dtype == 'bfloat16':\n",
    "    model_dtype = torch.bfloat16\n",
    "elif args.model_dtype == 'float16':\n",
    "    model_dtype = torch.float16\n",
    "model = XQwen3ForCausalLM.from_pretrained(  # XLlamaForCausalLM\n",
    "    args.model_name_or_path,\n",
    "    config=config,\n",
    "    torch_dtype = model_dtype\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "# Mengatur pad_token pada tokenizer llama dengan token yang sudah ada dalam Llama\n",
    "if tokenizer.pad_token_id is None:\n",
    "    print('Menambahkan pad token ke tokenizer')\n",
    "\n",
    "    if args.chat_format == 'llama':    \n",
    "        pad_token = \"<|finetune_right_pad_id|>\"\n",
    "        tokenizer.pad_token = pad_token\n",
    "        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "    elif args.chat_format == 'qwen':\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Menambahkan token baru (xrag) ke perbendaharaan tokenizer llama\n",
    "num_added_tokens = 0\n",
    "num_added_tokens += tokenizer.add_tokens([AddedToken(XRAG_TOKEN,lstrip=False,rstrip=False)])\n",
    "xrag_token_id = tokenizer.convert_tokens_to_ids(XRAG_TOKEN)\n",
    "model.set_xrag_token_id(xrag_token_id)\n",
    "if num_added_tokens > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b0bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode chat untuk finetune...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00,  7.63 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 45.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "membuang row yang seluruh labelsnya bernilai -100 (tidak ada porsi assistant sama sekali)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2/2 [00:00<00:00, 80.33 examples/s]\n",
      "Filter: 100%|██████████| 2/2 [00:00<00:00, 126.80 examples/s]\n",
      "Filter: 100%|██████████| 2/2 [00:00<00:00, 278.78 examples/s]\n",
      "Filter: 100%|██████████| 2/2 [00:00<00:00, 266.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if args.task_type == 'finetune':\n",
    "    print('encode chat untuk finetune...')\n",
    "    encode_function = partial(\n",
    "        encode_with_chat_format_finetune,\n",
    "        llm_tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "        use_rag_tuning = args.use_rag_tuning,\n",
    "        use_retriever_embed = not (retriever is None),\n",
    "        chat_format = args.chat_format,\n",
    "    )\n",
    "\n",
    "if args.task_type== 'pretrain':\n",
    "    print('encode chat untuk pretraining')\n",
    "    encode_function = partial(\n",
    "        encode_with_chat_format_pretrain,\n",
    "        llm_tokenizer = tokenizer,\n",
    "        max_seq_length = args.max_seq_length,\n",
    "        retrieval_embed_length=args.retrieval_embed_length,\n",
    "        chat_format = args.chat_format\n",
    "    )\n",
    "\n",
    "lm_datasets = dataset.map(encode_function)\n",
    "lm_datasets.set_format(type=\"pt\")\n",
    "\n",
    "if args.task_type == 'finetune':\n",
    "    print('membuang row yang seluruh labelsnya bernilai -100 (tidak ada porsi assistant sama sekali)...')\n",
    "    for split in lm_datasets.keys():\n",
    "        if \"background\" in lm_datasets[split].column_names:\n",
    "            lm_datasets[split].remove_columns('background')\n",
    "        lm_datasets[split] = lm_datasets[split].filter(lambda example: (example['labels'] != -100).any())\n",
    "        if args.alpha_kl is not None and args.alpha_kl > 0.0:\n",
    "            lm_datasets[split] = lm_datasets[split].filter(\n",
    "                lambda example: \n",
    "                (example['labels']!=-100).sum() == (example['xrag_labels']!=-100).sum()\n",
    "            )\n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "dev_dataset = lm_datasets['dev'] # if args.task_type == 'pretrain' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed42d534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "membuat embeddings untuk dokumen konteks dengan retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:01<00:00,  1.87 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00,  5.73 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menghapus retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Menambahkan retriever_embeddings ke dataset sebelum pelatihan\n",
    "print('membuat embeddings untuk dokumen konteks dengan retriever...')\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda example: add_retriever_embeddings(example, retriever, retriever_tokenizer, args.retrieval_context_length, text_col='retriever_input_text')\n",
    ")\n",
    "\n",
    "if dev_dataset is not None:\n",
    "    dev_dataset = dev_dataset.map(\n",
    "        lambda example: add_retriever_embeddings(example, retriever, retriever_tokenizer, args.retrieval_context_length, text_col='retriever_input_text')\n",
    "    )\n",
    "\n",
    "# Hapus objek retriever dan modelnya\n",
    "print('Menghapus retriever...')\n",
    "del retriever.model  # Menghapus model dari memori\n",
    "del retriever  # Menghapus objek retriever itu sendiri\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "if args.save_embeddings_generated:\n",
    "    ds_with_embeddings = DatasetDict({\n",
    "        'train': train_dataset, \n",
    "        'dev': dev_dataset\n",
    "    })\n",
    "    ds_with_embeddings.save_to_disk(os.path.join(args.processing_steps_output_dir, f\"{args.task_type}-create_embedding_step\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dda5e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menginisialisasi dataloader untuk training...\n",
      "Menginisialisasi dataloader untuk validasi...\n",
      "Mengatur agar hanya layer yang menjadi bagian dr projector saja yang diupdate selama training...\n"
     ]
    }
   ],
   "source": [
    "collate_fn = partial(\n",
    "    collator,\n",
    "    llm_tokenizer=tokenizer, \n",
    "    xrag_input_ids_col='xrag_input_ids',\n",
    "    xrag_labels_col = 'xrag_labels', \n",
    "    text_input_ids_col = 'input_ids', \n",
    "    text_labels_col = 'labels', \n",
    "    retriever_embeds_col='retriever_embeddings'\n",
    ")\n",
    "\n",
    "print('Menginisialisasi dataloader untuk training...')\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "if dev_dataset is not None:\n",
    "    print('Menginisialisasi dataloader untuk validasi...')\n",
    "    dev_dataloader = DataLoader(\n",
    "        dev_dataset,\n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=args.per_device_train_batch_size\n",
    "    )\n",
    "\n",
    "if args.update_projector_only:\n",
    "    print('Mengatur agar hanya layer yang menjadi bagian dr projector saja yang diupdate selama training...')\n",
    "    for n,p in model.named_parameters():\n",
    "        if 'projector' not in n:p.requires_grad = False\n",
    "        else:p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],lr=args.learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0346b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add learning rate scheduler\n",
    "num_training_steps = args.num_train_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_warmup_steps=int(num_training_steps * args.warmup_ratio)  # 3% warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2818a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi list untuk menyimpan loss\n",
    "accumulation_steps = args.gradient_accumulation_steps\n",
    "\n",
    "nll_train_losses = []\n",
    "kl_train_losses = []\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "epoch_avg_train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e3d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\LENOVO\\Documents\\Skripsi\\post-retrieval-eval\\xRAG\\language_modeling\\preprocessing.py:416: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ret['retriever_embeddings'] = torch.stack([torch.tensor(x[retriever_embeds_col]).to('cuda:0') for x in samples])\n",
      "  0%|          | 0/3 [00:00<?, ?it/s, epoch=1, batch=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Starting epoch 1\n",
      "------------------------------------------------------------------------\n",
      "Validating after epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [01:05<00:32, 32.58s/it, epoch=2, batch=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted sum of KL and NLL loss after epoch 1: 16.582947731018066\n",
      "========================================================================\n",
      "Starting epoch 2\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(args.num_train_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    print(\"======\"*12)  # Pembatas untuk setiap epoch\n",
    "    print(f\"Starting epoch {epoch+1}\")\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        progress_bar.set_postfix({'epoch': epoch+1, 'batch': batch_idx+1})\n",
    "        progress_bar.update(1)\n",
    "        if batch_idx % accumulation_steps == 0:\n",
    "            optimizer.zero_grad()  # deindent jika ingin cancel batch accumulation  \n",
    "\n",
    "        outputs = model(\n",
    "            input_ids = batch['xrag_input_ids'],\n",
    "            attention_mask = batch['xrag_attention_mask'],\n",
    "            retrieval_embeds = batch['retriever_embeddings']\n",
    "        )\n",
    "        del batch['xrag_input_ids']\n",
    "        del batch['xrag_attention_mask']\n",
    "        del batch['retriever_embeddings']\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        labels = batch['xrag_labels']\n",
    "\n",
    "        nll_loss = get_nll_loss(logits=logits, labels=labels, vocab_size=vocab_size)\n",
    "\n",
    "        loss = args.alpha_nll * nll_loss\n",
    "        nll_train_losses.append(loss.item())\n",
    "\n",
    "        if args.alpha_kl is not None and args.alpha_kl > 0.0:\n",
    "            ## forward with retrieval tokens\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                teacher_outputs = model(\n",
    "                    input_ids = batch['input_ids'],\n",
    "                    attention_mask = batch['attention_mask'],\n",
    "                )\n",
    "                del batch['input_ids']\n",
    "                del batch['attention_mask']\n",
    "                torch.cuda.empty_cache()\n",
    "                model.train()\n",
    "\n",
    "            kl_loss = get_kl_loss(\n",
    "                teacher_logits=teacher_outputs.logits,\n",
    "                teacher_labels=batch['labels'],\n",
    "                student_logits=outputs.logits,\n",
    "                student_labels=batch['xrag_labels'],\n",
    "                temperature=args.kl_temperature,\n",
    "            )\n",
    "            kl_loss = args.alpha_kl * kl_loss\n",
    "            kl_train_losses.append(kl_loss.item())\n",
    "            loss += kl_loss\n",
    "            \n",
    "            del batch['labels']\n",
    "            torch.cuda.empty_cache()\n",
    "        del batch['xrag_labels']\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Simpan loss untuk tiap batch\n",
    "        train_losses.append(loss.item())\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameter hanya setelah beberapa batch terakumulasi\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()  # deindent jika ingin cancel batch acc\n",
    "            lr_scheduler.step()  # deindent jika ingin cancel batch acc\n",
    "        # Jika menggunakan lebih banyak mini-batch sebelum update, pastikan gradien dihitung selama beberapa langkah\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:  # HAPUS JIKA INGIN CANCEL BATCH ACC\n",
    "            optimizer.zero_grad()  # Reset gradien untuk batch berikutnya  # --\"-- \n",
    "\n",
    "    epoch_avg_loss = epoch_train_loss / len(train_dataloader)\n",
    "    epoch_avg_train_losses.append(epoch_avg_loss)\n",
    "\n",
    "    # Setelah setiap epoch selesai, lakukan validasi\n",
    "    if dev_dataset is not None:\n",
    "        print(\"------\"*12)\n",
    "        if args.task_type == 'pretrain':\n",
    "            print(f\"Validating after epoch {epoch+1}...\")\n",
    "            ppl = validate_during_pretrain(model, dev_dataloader, len(tokenizer))\n",
    "            print(f\"Perplexity on dev set after epoch {epoch+1}: {ppl}\")\n",
    "\n",
    "            dev_losses.append(ppl.item())\n",
    "        if args.task_type == 'finetune':\n",
    "            print(f\"Validating after epoch {epoch+1}...\")\n",
    "            metrics = validate_during_finetune(model, dev_dataloader, vocab_size, args)\n",
    "            print(f\"weighted sum of KL and NLL loss after epoch {epoch+1}: {metrics['total_loss']}\")\n",
    "            dev_losses.append(metrics)\n",
    "\n",
    "print(\"dev loss pertama :\", dev_losses[0])\n",
    "print(\"dev loss terakhir:\", dev_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44927ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 6.00 GB\n",
      "Allocated GPU Memory: 4.28 GB\n",
      "Max Reserved GPU Memory: 7.27 GB\n",
      "Reserved GPU Memory: 6.22 GB\n",
      "Free GPU Memory: 1.94 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# GPU yang digunakan\n",
    "def check_gpu():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    # Total memori GPU\n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory / (1024 ** 3)  # Dalam GB\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} GB\")\n",
    "\n",
    "    # Memori yang sudah dialokasikan oleh PyTorch\n",
    "    allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 3)  # Dalam GB\n",
    "    print(f\"Allocated GPU Memory: {allocated_memory:.2f} GB\")\n",
    "\n",
    "    max_reserved_memory = torch.cuda.max_memory_reserved(device) / (1024 ** 3)  # Dalam GB\n",
    "    print(f\"Max Reserved GPU Memory: {max_reserved_memory:.2f} GB\")\n",
    "\n",
    "    # Memori GPU yang dicadangkan oleh PyTorch\n",
    "    reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 3)  # Dalam GB\n",
    "    print(f\"Reserved GPU Memory: {reserved_memory:.2f} GB\")\n",
    "\n",
    "    # Memori GPU yang tersedia\n",
    "    free_memory = reserved_memory - allocated_memory\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} GB\")\n",
    "\n",
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15ec3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "if args.task_type == 'finetune':\n",
    "    output_dir = os.path.join(args.output_dir, \"finetuned\")\n",
    "    output_dir = os.path.join(output_dir, current_time)\n",
    "    model_output_dir = os.path.join(output_dir, 'finished_model')\n",
    "\n",
    "elif args.task_type == 'pretrain':\n",
    "    output_dir = os.path.join(args.output_dir, 'pretrained')\n",
    "    output_dir = os.path.join(output_dir, current_time)\n",
    "    model_output_dir = os.path.join(output_dir, f\"{args.retriever_name_or_path[-8:]}_{args.chat_format}{args.model_size}_batch{args.per_device_train_batch_size}_{args.num_train_epochs}epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5212867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../output\\\\finetuned\\\\2025-06-05_14-58-31\\\\finished_model\\\\tokenizer_config.json',\n",
       " '../output\\\\finetuned\\\\2025-06-05_14-58-31\\\\finished_model\\\\special_tokens_map.json',\n",
       " '../output\\\\finetuned\\\\2025-06-05_14-58-31\\\\finished_model\\\\vocab.json',\n",
       " '../output\\\\finetuned\\\\2025-06-05_14-58-31\\\\finished_model\\\\merges.txt',\n",
       " '../output\\\\finetuned\\\\2025-06-05_14-58-31\\\\finished_model\\\\added_tokens.json',\n",
       " '../output\\\\finetuned\\\\2025-06-05_14-58-31\\\\finished_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "model.save_pretrained(model_output_dir)\n",
    "tokenizer.save_pretrained(model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, \"train_loss.csv\"), mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['step', 'train_loss'])\n",
    "    for step, loss in enumerate(train_losses):\n",
    "        writer.writerow([step, loss])\n",
    "\n",
    "with open(os.path.join(output_dir, \"train_loss_per_epoch.csv\"), mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['epoch', 'avg_train_loss'])\n",
    "    for epoch, loss in enumerate(epoch_avg_train_losses):\n",
    "        writer.writerow([epoch, loss])\n",
    "\n",
    "\n",
    "with open(os.path.join(output_dir, \"nll_loss.csv\"), mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['step', 'nll_loss'])\n",
    "    for step, loss in enumerate(nll_train_losses):\n",
    "        writer.writerow([step, loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if args.task_type == 'pretrain':\n",
    "with open(os.path.join(output_dir, \"dev_loss.csv\"), mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['epoch', 'dev_loss'])\n",
    "    for epoch, loss in enumerate(dev_losses):\n",
    "        writer.writerow([epoch, loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a45a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.task_type == 'finetune':\n",
    "    with open(os.path.join(output_dir, \"kl_loss.csv\"), mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['step', 'kl_loss'])\n",
    "        for step, loss in enumerate(kl_train_losses):\n",
    "            writer.writerow([step, loss])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
