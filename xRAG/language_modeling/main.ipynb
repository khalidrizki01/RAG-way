{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaea1670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import XRAG_TOKEN, load_and_format_dataset, encode_with_chat_format_finetune, encode_with_chat_format_pretrain, add_retriever_embeddings, collator\n",
    "from utils import get_nll_loss, get_kl_loss, validate_during_pretrain\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, get_scheduler\n",
    "from tokenizers import AddedToken\n",
    "from functools import partial\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "import torch\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "sys.path.append('..')\n",
    "from model.E5Retriever import E5Retriever\n",
    "from model.xLlama import XLlamaConfig, XLlamaForCausalLM\n",
    "from model.xQwen3 import XQwen3Config, XQwen3ForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b0f2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8b7d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune\n",
      "berhasil memuat dataset\n"
     ]
    }
   ],
   "source": [
    "class Args: \n",
    "    retrieval_context_length= 512  # 180\n",
    "    overwrite_cache =False\n",
    "    max_samples = None\n",
    "    chat_format=\"qwen\"  # \"llama\"\n",
    "    model_size = \"1,7B\"\n",
    "    retriever_name_or_path='intfloat/multilingual-e5-small'\n",
    "    dataset_path = \"../../generated_data/raw/final_dataset\"\n",
    "    query_col = 'query'\n",
    "    ans_col = 'answer'\n",
    "    psg_col = 'passages'\n",
    "    lr_scheduler_type = \"linear\"\n",
    "    warmup_ratio = 0.03\n",
    "    num_train_epochs = 3\n",
    "    alpha_nll = 1.0\n",
    "    update_projector_only = True\n",
    "    per_device_train_batch_size = 2\n",
    "    max_train_steps = None\n",
    "    checkpointing_steps = None\n",
    "    output_dir='../output'\n",
    "    lang=\"indonesian\"\n",
    "\n",
    "    # # pretrain\n",
    "    # max_seq_length = 600  # 336\n",
    "    # model_name_or_path = \"Qwen/Qwen3-1.7B\"\n",
    "    # task_type='pretrain'\n",
    "    # learning_rate=6.0e-3\n",
    "    # alpha_kl = None\n",
    "    # kl_temperature=0.0\n",
    "    # retrieval_embed_length=1\n",
    "\n",
    "    #  finetune\n",
    "    learning_rate = 2.0e-5\n",
    "    max_seq_length = 1620 # 1024\n",
    "    use_rag_tuning = True\n",
    "    alpha_kl= 2.0\n",
    "    kl_temperature= 1.0 \n",
    "    model_name_or_path=\"../output/pretrained/e5-small_qwen1,7B_batch2_Nonerows\"\n",
    "    \n",
    "    task_type=\"finetune\"\n",
    "    retrieval_embed_length=3\n",
    "\n",
    "args = Args()\n",
    "print(args.task_type)\n",
    "dataset = load_and_format_dataset(args.dataset_path, args.query_col, args.ans_col, args.psg_col, args.task_type, args.max_samples, include_psg_len=False)\n",
    "\n",
    "if 'test' in dataset:\n",
    "    dataset.pop('test')\n",
    "if args.task_type == 'finetune':\n",
    "    if 'dev' in dataset:\n",
    "        dataset.pop('dev')\n",
    "\n",
    "print(\"berhasil memuat dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db828a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memuat retriever dan tokenizernya...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memuat tokenizer generatif...\n",
      "memuat model generatif...\n"
     ]
    }
   ],
   "source": [
    "# Loading model retriever\n",
    "print('memuat retriever dan tokenizernya...')\n",
    "retriever = E5Retriever(args.retriever_name_or_path)\n",
    "retriever_tokenizer = AutoTokenizer.from_pretrained(args.retriever_name_or_path)\n",
    "retriever_hidden_size = retriever.get_embed_dim()\n",
    "retriever.eval()\n",
    "retriever.to('cuda:0')\n",
    "\n",
    "print('memuat tokenizer generatif...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model_name_or_path\n",
    ")\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "print('memuat model generatif...')\n",
    "# config = XLlamaConfig.from_pretrained(args.model_name_or_path, retriever_hidden_size=retriever_hidden_size)\n",
    "config = XQwen3Config.from_pretrained(args.model_name_or_path, retriever_hidden_size=retriever_hidden_size)\n",
    "model = XQwen3ForCausalLM.from_pretrained(  # XLlamaForCausalLM\n",
    "    args.model_name_or_path,\n",
    "    config=config,\n",
    "    torch_dtype = torch.bfloat16\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "# Mengatur pad_token pada tokenizer llama dengan token yang sudah ada dalam Llama\n",
    "if tokenizer.pad_token_id is None:\n",
    "    print('Menambahkan pad token ke tokenizer')\n",
    "\n",
    "    if args.chat_format == 'llama':    \n",
    "        pad_token = \"<|finetune_right_pad_id|>\"\n",
    "        tokenizer.pad_token = pad_token\n",
    "        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "    elif args.chat_format == 'qwen':\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Menambahkan token baru (xrag) ke perbendaharaan tokenizer llama\n",
    "num_added_tokens = 0\n",
    "num_added_tokens += tokenizer.add_tokens([AddedToken(XRAG_TOKEN,lstrip=False,rstrip=False)])\n",
    "xrag_token_id = tokenizer.convert_tokens_to_ids(XRAG_TOKEN)\n",
    "model.set_xrag_token_id(xrag_token_id)\n",
    "if num_added_tokens > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2daa4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode chat untuk finetune...\n"
     ]
    }
   ],
   "source": [
    "if args.task_type == 'finetune':\n",
    "    print('encode chat untuk finetune...')\n",
    "    encode_function = partial(\n",
    "        encode_with_chat_format_finetune,\n",
    "        llm_tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "        retrieval_context_length=args.retrieval_context_length,\n",
    "        lang=args.lang, \n",
    "        use_rag_tuning = args.use_rag_tuning,\n",
    "        use_retriever_embed = not (retriever is None),\n",
    "        retriever_tokenizer = retriever_tokenizer,\n",
    "        chat_format = args.chat_format,\n",
    "    )\n",
    "\n",
    "if args.task_type== 'pretrain':\n",
    "    print('encode chat untuk pretraining')\n",
    "    encode_function = partial(\n",
    "        encode_with_chat_format_pretrain,\n",
    "        llm_tokenizer = tokenizer,\n",
    "        retriever_tokenizer = retriever_tokenizer, \n",
    "        max_seq_length = args.max_seq_length,\n",
    "        retrieval_context_length=args.retrieval_context_length,\n",
    "        retrieval_embed_length=args.retrieval_embed_length,\n",
    "        chat_format = args.chat_format\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5889feed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4542/4542 [00:37<00:00, 120.61 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "membuang row yang seluruh labelsnya bernilai -100 (tidak ada porsi assistant sama sekali)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 4542/4542 [00:00<00:00, 4554.73 examples/s]\n",
      "Filter: 100%|██████████| 4540/4540 [00:01<00:00, 3208.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = dataset.map(encode_function)\n",
    "lm_datasets.set_format(type=\"pt\")\n",
    "\n",
    "if args.task_type == 'finetune':\n",
    "    print('membuang row yang seluruh labelsnya bernilai -100 (tidak ada porsi assistant sama sekali)...')\n",
    "    lm_datasets['train'] = lm_datasets['train'].filter(lambda example: (example['labels'] != -100).any())\n",
    "    if args.alpha_kl is not None and args.alpha_kl > 0.0:\n",
    "        lm_datasets['train'] = lm_datasets['train'].filter(\n",
    "            lambda example: \n",
    "            (example['labels']!=-100).sum() == (example['xrag_labels']!=-100).sum()\n",
    "        )\n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "dev_dataset = lm_datasets['dev'] if args.task_type == 'pretrain' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd95515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "membuat embeddings untuk dokumen konteks dengan retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4540/4540 [01:38<00:00, 45.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menghapus retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Menambahkan retriever_embeddings ke dataset sebelum pelatihan\n",
    "print('membuat embeddings untuk dokumen konteks dengan retriever...')\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda example: add_retriever_embeddings(example, retriever, retriever_tokenizer, args.retrieval_context_length, text_col='retriever_input_text')\n",
    ")\n",
    "\n",
    "if dev_dataset is not None:\n",
    "    dev_dataset = dev_dataset.map(\n",
    "        lambda example: add_retriever_embeddings(example, retriever, retriever_tokenizer, args.retrieval_context_length, text_col='retriever_input_text')\n",
    "    )\n",
    "\n",
    "# Hapus objek retriever dan modelnya\n",
    "print('Menghapus retriever...')\n",
    "del retriever.model  # Menghapus model dari memori\n",
    "del retriever  # Menghapus objek retriever itu sendiri\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0990f681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menginisialisasi dataloader untuk training...\n",
      "Mengatur agar hanya layer yang menjadi bagian dr projector saja yang diupdate selama training...\n"
     ]
    }
   ],
   "source": [
    "collate_fn = partial(\n",
    "    collator,\n",
    "    llm_tokenizer=tokenizer, \n",
    "    xrag_input_ids_col='xrag_input_ids',\n",
    "    xrag_labels_col = 'xrag_labels', \n",
    "    text_input_ids_col = 'input_ids', \n",
    "    text_labels_col = 'labels', \n",
    "    retriever_embeds_col='retriever_embeddings'\n",
    ")\n",
    "\n",
    "print('Menginisialisasi dataloader untuk training...')\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "\n",
    "if dev_dataset is not None:\n",
    "    print('Menginisialisasi dataloader untuk validasi...')\n",
    "    dev_dataloader = DataLoader(\n",
    "        dev_dataset,\n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=args.per_device_train_batch_size\n",
    "    )\n",
    "\n",
    "\n",
    "if args.update_projector_only:\n",
    "    print('Mengatur agar hanya layer yang menjadi bagian dr projector saja yang diupdate selama training...')\n",
    "    for n,p in model.named_parameters():\n",
    "        if 'projector' not in n:p.requires_grad = False\n",
    "        else:p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],lr=args.learning_rate) \n",
    "\n",
    "# Add learning rate scheduler\n",
    "num_training_steps = args.num_train_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_warmup_steps=int(num_training_steps * args.warmup_ratio)  # 3% warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab7ae564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6810 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Inisialisasi list untuk menyimpan loss\n",
    "accumulation_steps = 4\n",
    "\n",
    "nll_train_losses = []\n",
    "kl_train_losses = []\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6da7c1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\Documents\\Skripsi\\post-retrieval-eval\\xRAG\\language_modeling\\preprocessing.py:456: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ret['retriever_embeddings'] = torch.stack([torch.tensor(x[retriever_embeds_col]).to('cuda:0') for x in samples])\n",
      "  0%|          | 1/6810 [00:03<6:16:20,  3.32s/it, epoch=1, batch=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1088/6810 [10:53<57:59,  1.64it/s, epoch=1, batch=1088]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 1 dan 2 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1202/6810 [11:59<52:12,  1.79it/s, epoch=1, batch=1202]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1874/6810 [18:27<41:26,  1.99it/s, epoch=1, batch=1874]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2271/6810 [22:26<38:42,  1.95it/s, epoch=2, batch=1]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 2505/6810 [24:53<47:17,  1.52it/s, epoch=2, batch=235]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 3159/6810 [31:13<39:42,  1.53it/s, epoch=2, batch=889]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 4312/6810 [42:40<22:53,  1.82it/s, epoch=2, batch=2042]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4541/6810 [45:03<19:16,  1.96it/s, epoch=3, batch=1]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 5435/6810 [54:31<12:27,  1.84it/s, epoch=3, batch=895]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 0 dan 1 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 5576/6810 [55:51<12:56,  1.59it/s, epoch=3, batch=1036]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 4 dan 5 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 6464/6810 [1:04:30<02:56,  1.96it/s, epoch=3, batch=1924]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token xRAG pada posisi 3 dan 4 memiliki nilai yang sama setelah disematkan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6810/6810 [1:08:12<00:00,  1.69it/s, epoch=3, batch=2270]"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.num_train_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    print(\"======\"*12)  # Pembatas untuk setiap epoch\n",
    "    print(f\"Starting epoch {epoch+1}\")\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        progress_bar.set_postfix({'epoch': epoch+1, 'batch': batch_idx+1})\n",
    "        progress_bar.update(1)\n",
    "        if batch_idx % accumulation_steps == 0:\n",
    "            optimizer.zero_grad()  # deindent jika ingin cancel batch accumulation  \n",
    "\n",
    "        outputs = model(\n",
    "            input_ids = batch['xrag_input_ids'],\n",
    "            attention_mask = batch['xrag_attention_mask'],\n",
    "            retrieval_embeds = batch['retriever_embeddings']\n",
    "        )\n",
    "        del batch['xrag_input_ids']\n",
    "        del batch['xrag_attention_mask']\n",
    "        del batch['retriever_embeddings']\n",
    "        torch.cuda.empty_cache()\n",
    "        logits = outputs.logits\n",
    "        labels = batch['xrag_labels']\n",
    "\n",
    "        nll_loss = get_nll_loss(logits=logits, labels=labels, vocab_size=vocab_size)\n",
    "\n",
    "        loss = args.alpha_nll * nll_loss\n",
    "        nll_train_losses.append(loss.item())\n",
    "\n",
    "        if args.alpha_kl is not None and args.alpha_kl > 0.0:\n",
    "            ## forward with retrieval tokens\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                teacher_outputs = model(\n",
    "                    input_ids = batch['input_ids'],\n",
    "                    attention_mask = batch['attention_mask'],\n",
    "                )\n",
    "                del batch['input_ids']\n",
    "                del batch['attention_mask']\n",
    "                torch.cuda.empty_cache()\n",
    "                model.train()\n",
    "\n",
    "            kl_loss = get_kl_loss(\n",
    "                teacher_logits=teacher_outputs.logits,\n",
    "                teacher_labels=batch['labels'],\n",
    "                student_logits=outputs.logits,\n",
    "                student_labels=batch['xrag_labels'],\n",
    "                temperature=args.kl_temperature,\n",
    "            )\n",
    "            kl_loss = args.alpha_kl * kl_loss\n",
    "            kl_train_losses.append(kl_loss.item())\n",
    "            loss += kl_loss\n",
    "            \n",
    "            del batch['labels']\n",
    "            torch.cuda.empty_cache()\n",
    "        del batch['xrag_labels']\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Simpan loss untuk tiap batch\n",
    "        train_losses.append(loss.item())\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameter hanya setelah beberapa batch terakumulasi\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()  # deindent jika ingin cancel batch acc\n",
    "            lr_scheduler.step()  # deindent jika ingin cancel batch acc\n",
    "        # Jika menggunakan lebih banyak mini-batch sebelum update, pastikan gradien dihitung selama beberapa langkah\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:  # HAPUS JIKA INGIN CANCEL BATCH ACC\n",
    "            optimizer.zero_grad()  # Reset gradien untuk batch berikutnya  # --\"-- \n",
    "\n",
    "    # # Setelah setiap epoch selesai, lakukan validasi\n",
    "    # if dev_dataloader is not None:\n",
    "    #     print(\"------\"*12)\n",
    "    #     if args.task_type == 'pretrain':\n",
    "    #         print(f\"Validating after epoch {epoch+1}...\")\n",
    "    #         ppl = validate_during_pretrain(model, dev_dataloader, len(tokenizer))\n",
    "    #         print(f\"Perplexity on dev set after epoch {epoch+1}: {ppl}\")\n",
    "\n",
    "    #         dev_losses.append(ppl.item())\n",
    "\n",
    "# Hitung rata-rata dari dev_losses\n",
    "# average_dev_loss = sum(dev_losses) / len(dev_losses) if dev_losses else 0.0\n",
    "# print(f\"Average Perplexity on dev set: {average_dev_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ea17e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 6.00 GB\n",
      "Allocated GPU Memory: 3.84 GB\n",
      "Max Reserved GPU Memory: 6.73 GB\n",
      "Reserved GPU Memory: 5.10 GB\n",
      "Free GPU Memory: 1.25 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# GPU yang digunakan\n",
    "def check_gpu():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    # Total memori GPU\n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory / (1024 ** 3)  # Dalam GB\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} GB\")\n",
    "\n",
    "    # Memori yang sudah dialokasikan oleh PyTorch\n",
    "    allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 3)  # Dalam GB\n",
    "    print(f\"Allocated GPU Memory: {allocated_memory:.2f} GB\")\n",
    "\n",
    "    max_reserved_memory = torch.cuda.max_memory_reserved(device) / (1024 ** 3)  # Dalam GB\n",
    "    print(f\"Max Reserved GPU Memory: {max_reserved_memory:.2f} GB\")\n",
    "\n",
    "    # Memori GPU yang dicadangkan oleh PyTorch\n",
    "    reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 3)  # Dalam GB\n",
    "    print(f\"Reserved GPU Memory: {reserved_memory:.2f} GB\")\n",
    "\n",
    "    # Memori GPU yang tersedia\n",
    "    free_memory = reserved_memory - allocated_memory\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} GB\")\n",
    "\n",
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1342a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lakukan validasi setelah setiap epoch\n",
    "# dev_losses = []\n",
    "\n",
    "# if dev_dataloader is not None:\n",
    "#     if args.task_type == 'pretrain':\n",
    "#         print(f\"Validating...\")\n",
    "#         ppl = validate_during_pretrain(model, dev_dataloader, len(tokenizer))\n",
    "#         print(f\"Perplexity on dev set: {ppl}\")\n",
    "\n",
    "#         dev_losses.append(ppl.item())\n",
    "\n",
    "# # Hitung rata-rata dari dev_losses\n",
    "# average_dev_loss = sum(dev_losses) / len(dev_losses) if dev_losses else 0.0\n",
    "# print(f\"Average Perplexity on dev set: {average_dev_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "136a6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.task_type == 'finetune':\n",
    "    output_dir = os.path.join(args.output_dir, \"finetuned\")\n",
    "    model_output_dir = os.path.join(output_dir, 'finished_model')\n",
    "\n",
    "elif args.task_type == 'pretrain':\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    output_dir = os.path.join(args.output_dir, current_time)\n",
    "    output_dir = os.path.join(args.output_dir, 'pretrained')\n",
    "    model_output_dir = os.path.join(output_dir, f\"{args.retriever_name_or_path[-8:]}_{args.chat_format}{args.model_size}_batch{args.per_device_train_batch_size}_{args.max_samples}rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14d461c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../output\\\\finetuned\\\\finished_model\\\\tokenizer_config.json',\n",
       " '../output\\\\finetuned\\\\finished_model\\\\special_tokens_map.json',\n",
       " '../output\\\\finetuned\\\\finished_model\\\\vocab.json',\n",
       " '../output\\\\finetuned\\\\finished_model\\\\merges.txt',\n",
       " '../output\\\\finetuned\\\\finished_model\\\\added_tokens.json',\n",
       " '../output\\\\finetuned\\\\finished_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "model.save_pretrained(model_output_dir)\n",
    "tokenizer.save_pretrained(model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbb79c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, \"train_loss.csv\"), mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['step', 'train_loss'])\n",
    "    for step, loss in enumerate(train_losses):\n",
    "        writer.writerow([step, loss])\n",
    "\n",
    "with open(os.path.join(output_dir, \"nll_loss.csv\"), mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['step', 'nll_loss'])\n",
    "    for step, loss in enumerate(nll_train_losses):\n",
    "        writer.writerow([step, loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d617454",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.task_type == 'pretrain':\n",
    "    with open(os.path.join(output_dir, \"dev_loss.csv\"), mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['epoch', 'dev_loss'])\n",
    "        for epoch, loss in enumerate(dev_losses):\n",
    "            writer.writerow([epoch, loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "626a9bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.task_type == 'finetune':\n",
    "    with open(os.path.join(output_dir, \"kl_loss.csv\"), mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['step', 'kl_loss'])\n",
    "        for step, loss in enumerate(kl_train_losses):\n",
    "            writer.writerow([step, loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e468f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
