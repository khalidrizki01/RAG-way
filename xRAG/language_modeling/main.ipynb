{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaea1670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import XRAG_TOKEN, load_and_format_dataset, encode_with_chat_format_finetune, encode_with_chat_format_pretrain, add_retriever_embeddings, collator\n",
    "from utils import get_nll_loss, get_kl_loss, validate_during_pretrain\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, get_scheduler\n",
    "from tokenizers import AddedToken\n",
    "from functools import partial\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "from model.E5Retriever import E5Retriever\n",
    "from model.xLlama import XLlamaConfig, XLlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b7d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune\n"
     ]
    }
   ],
   "source": [
    "class Args: \n",
    "    retrieval_context_length= 512  # 180\n",
    "    overwrite_cache =False\n",
    "    max_train_samples = 2000000\n",
    "    chat_format=\"llama\"\n",
    "    retriever_name_or_path='intfloat/multilingual-e5-small'\n",
    "    workdir = \".\"\n",
    "    lr_scheduler_type = \"linear\"\n",
    "    warmup_ratio = 0.03\n",
    "    weight_decay = 0.0\n",
    "    num_train_epochs = 1\n",
    "    use_flash_attn = True\n",
    "    alpha_nll = 1.0\n",
    "    seed = 980406\n",
    "    update_projector_only = True\n",
    "    per_device_train_batch_size = 4\n",
    "    max_train_steps = None\n",
    "    checkpointing_steps = None\n",
    "    output_dir='../output'\n",
    "    lang=\"english\"\n",
    "\n",
    "    # # pretrain\n",
    "    # max_seq_length = 600  # 336\n",
    "    # model_name_or_path = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "    # task_type='pretrain'\n",
    "    # learning_rate=6.0e-3\n",
    "    # alpha_kl = None\n",
    "    # kl_temperature=0.0\n",
    "    # retrieval_embed_length=1\n",
    "\n",
    "    #  finetune\n",
    "    learning_rate = 2.0e-5\n",
    "    max_seq_length = 1600 # 1024\n",
    "    use_rag_tuning = True\n",
    "    alpha_kl= 2.0\n",
    "    kl_temperature= 1.0 \n",
    "    model_name_or_path=\"../output/2025-05-05_22-50-00/e5small-llama1Binstruct-batch4\"\n",
    "    task_type=\"finetune\"\n",
    "    retrieval_embed_length=3\n",
    "\n",
    "args = Args()\n",
    "print(args.task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e8a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../generated_data/TUNING_final_summary\"\n",
    "query_col = 'query'\n",
    "ans_col = 'answer'\n",
    "psg_col = 'passages'\n",
    "\n",
    "max_rows = 500\n",
    "\n",
    "dataset = load_and_format_dataset(dataset_path, query_col, ans_col, psg_col, args.task_type, max_rows, include_psg_len=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db828a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memuat retriever dan tokenizernya...\n",
      "memuat tokenizer generatif...\n",
      "memuat model generatif...\n"
     ]
    }
   ],
   "source": [
    "# Loading model retriever\n",
    "print('memuat retriever dan tokenizernya...')\n",
    "retriever = E5Retriever(args.retriever_name_or_path)\n",
    "retriever_tokenizer = AutoTokenizer.from_pretrained(args.retriever_name_or_path)\n",
    "retriever_hidden_size = retriever.get_embed_dim()\n",
    "retriever.eval()\n",
    "retriever.to('cuda:0')\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "print('memuat tokenizer generatif...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model_name_or_path\n",
    ")\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "print('memuat model generatif...')\n",
    "config = XLlamaConfig.from_pretrained(args.model_name_or_path, retriever_hidden_size=retriever_hidden_size)\n",
    "model = XLlamaForCausalLM.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    config=config,\n",
    "    torch_dtype = torch.bfloat16\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "# Mengatur pad_token pada tokenizer llama dengan token yang sudah ada dalam Llama\n",
    "pad_token = \"<|finetune_right_pad_id|>\"\n",
    "tokenizer.pad_token = pad_token\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "\n",
    "# Menambahkan token baru (xrag) ke perbendaharaan tokenizer llama\n",
    "num_added_tokens = 0\n",
    "num_added_tokens += tokenizer.add_tokens([AddedToken(XRAG_TOKEN,lstrip=False,rstrip=False)])\n",
    "xrag_token_id = tokenizer.convert_tokens_to_ids(XRAG_TOKEN)\n",
    "model.set_xrag_token_id(xrag_token_id)\n",
    "if num_added_tokens > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2daa4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode chat untuk finetune...\n"
     ]
    }
   ],
   "source": [
    "if args.task_type == 'finetune':\n",
    "    print('encode chat untuk finetune...')\n",
    "    encode_function = partial(\n",
    "        encode_with_chat_format_finetune,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "        lang=args.lang, \n",
    "        retrieval_context_length=args.retrieval_context_length,\n",
    "        use_rag_tuning = args.use_rag_tuning,\n",
    "        use_retriever_embed = not (retriever is None),\n",
    "        retriever_tokenizer = retriever_tokenizer,\n",
    "        chat_format = args.chat_format,\n",
    "    )\n",
    "\n",
    "if args.task_type== 'pretrain':\n",
    "    print('encode chat untuk pretraining')\n",
    "    encode_function = partial(\n",
    "        encode_with_chat_format_pretrain,\n",
    "        tokenizer = tokenizer,\n",
    "        retriever_tokenizer = retriever_tokenizer, \n",
    "        max_seq_length = args.max_seq_length,\n",
    "        retrieval_context_length=args.retrieval_context_length,\n",
    "        retrieval_embed_length=args.retrieval_embed_length,\n",
    "        chat_format = args.chat_format\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5889feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = dataset.map(encode_function)\n",
    "lm_datasets.set_format(type=\"pt\")\n",
    "\n",
    "if args.task_type == 'finetune':\n",
    "    print('membuang row yang seluruh labelsnya bernilai -100 (tidak ada porsi assistant sama sekali)...')\n",
    "    lm_datasets['train'] = lm_datasets['train'].filter(lambda example: (example['labels'] != -100).any())\n",
    "    if args.alpha_kl is not None and args.alpha_kl > 0.0:\n",
    "        lm_datasets['train'] = lm_datasets['train'].filter(\n",
    "            lambda example: \n",
    "            (example['labels']!=-100).sum() == (example['xrag_labels']!=-100).sum()\n",
    "        )\n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "dev_dataset = lm_datasets['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd95515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "membuat embeddings untuk dokumen konteks dengan retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 484/484 [00:11<00:00, 43.95 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:10<00:00, 47.96 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menghapus retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Menambahkan retriever_embeddings ke dataset sebelum pelatihan\n",
    "print('membuat embeddings untuk dokumen konteks dengan retriever...')\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda example: add_retriever_embeddings(example, retriever, retriever_tokenizer, args.retrieval_context_length, text_col='retriever_input_text')\n",
    ")\n",
    "\n",
    "dev_dataset = dev_dataset.map(\n",
    "    lambda example: add_retriever_embeddings(example, retriever, retriever_tokenizer, args.retrieval_context_length, text_col='retriever_input_text')\n",
    ")\n",
    "\n",
    "# Hapus objek retriever dan modelnya\n",
    "print('Menghapus retriever...')\n",
    "del retriever.model  # Menghapus model dari memori\n",
    "del retriever  # Menghapus objek retriever itu sendiri\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00771cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['passage: Judul: John F. Kennedy Teks: John Fitzgerald Kennedy lahir di 83 Beals Street, Brookline, Massachusetts, pada tanggal 29 Mei 1917[10] dari pasangan pebisnis/politikus Joseph Patrick \"Joe\" Kennedy, Sr. (1888–1969) dan filantropis Rose Elizabeth Fitzgerald (1890–1995). Joe adalah putra sulung pebisnis/politikus Patrick Joseph \"P. J.\" Kennedy (1858–1929) dan Mary Augusta Hickey (1857–1923). Rose adalah putri sulung Wali Kota Boston John Francis \"Honey Fitz\" Fitzgerald (1863–1950) dan Mary Josephine \"Josie\" Hannon (1865–1964). Keempat kakek-neneknya adalah anak-anak imigran Irlandia.[1]',\n",
       " 'passage: Judul: Keluarga Kennedy Teks: Anak pertama dari Joseph P. Kennedy, Sr. adalah Joseph Patrick \"Joe\" Kennedy, Jr. yang diharapkan ayahnya untuk terjun di dunia politik dan menjadi presiden. Setelah Joe, Jr. tewas dalam Perang Dunia II, harapan menjadi presiden dialihkan ke putra kedua John Fitzgerald \"Jack\" Kennedy. Segera setelah terpilih sebagai presiden pada November 1960, Jack mengajak adik laki-lakinya, Robert Francis \"Bobby\" Kennedy dan Edward Moore \"Ted\" Kennedy untuk menempati jabatan-jabatan penting di pemerintah federal. Mereka menerima publisitas yang intens, sering kali menonjolkan penampilan mereka yang tampak muda, daya pikat, pendidikan, dan masa depan mereka di politik. Selama 64 tahun, terhitung dari tahun 1947 ketika Jack dilantik sebagai anggota Kongres hingga tahun 2011 saat Patrick Joseph Kennedy II pensiun dari Kongres, anggota klan Kennedy selalu',\n",
       " 'passage: menempati jabatan politik di Washington, D.C. (meski ada kekosongan kurang dari sebulan antara pengunduran diri Jack dari Senat dan pelantikannya sebagai presiden). Seperempat dari sejarah negara Amerika Serikat tidak terlepas dari aktifnya anggota keluarga Kennedy dalam politik. Kekosongan terjadi setelah Patrick pensiun dari Kongres pada tahun 2011 hingga disumpahnya Joseph P. Kennedy III sebagai anggota Kongres pada tahun 2013.',\n",
       " 'passage: Judul: John F. Kennedy, Jr. Teks: John F. Kennedy, Jr. () sering disebut sebagai JFK Jr. atau John-John, adalah sosialita Amerika Serikat, jurnalis, pengacara, dan penerbit majalah. Dia adalah anak laki-laki pertama dan tertua dari Presiden Amerika Serikat John F. Kennedy dan Ibu Negara Amerika Serikat Jacqueline Lee \"Jackie\" Bouvier. Dia adalah keponakan laki-laki dari senator Robert F. Kennedy dan Edward Kennedy. Dia meninggal karena kecelakaan pesawat bersama istrinya Carolyn Jeanne Bessette dan kakak istrinya Lauren pada 16 Juli 1999.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['retriever_input_text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e074c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 484/484 [00:00<00:00, 1068.30 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1, tensor(3): 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Fungsi untuk menghitung panjang dari setiap list di kolom 'background'\n",
    "def count_lengths(example):\n",
    "    return {'background_length': len(example['background'])}\n",
    "\n",
    "# Terapkan fungsi map ke dataset\n",
    "lengths_dataset = train_dataset.map(count_lengths)\n",
    "\n",
    "# Ambil data dari kolom 'background_length' yang baru\n",
    "lengths = lengths_dataset['background_length']\n",
    "\n",
    "# Menghitung jumlah elemen unik (berapa banyak baris dengan panjang list tertentu)\n",
    "length_counts = Counter(lengths)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(length_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0990f681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menginisialisasi dataloader untuk training...\n",
      "Menginisialisasi dataloader untuk validasi...\n",
      "Mengatur agar hanya layer yang menjadi bagian dr projector saja yang diupdate selama training...\n"
     ]
    }
   ],
   "source": [
    "collate_fn = partial(\n",
    "    collator,\n",
    "    llm_tokenizer=tokenizer, \n",
    "    llm_tokenizer_pad_token = \"<|finetune_right_pad_id|>\", \n",
    "    xrag_input_ids_col='xrag_input_ids',\n",
    "    xrag_labels_col = 'xrag_labels', \n",
    "    text_input_ids_col = 'input_ids', \n",
    "    text_labels_col = 'labels', \n",
    "    retriever_embeds_col='retriever_embeddings'\n",
    ")\n",
    "\n",
    "print('Menginisialisasi dataloader untuk training...')\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "print('Menginisialisasi dataloader untuk validasi...')\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset,\n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "\n",
    "if args.update_projector_only:\n",
    "    print('Mengatur agar hanya layer yang menjadi bagian dr projector saja yang diupdate selama training...')\n",
    "    for n,p in model.named_parameters():\n",
    "        if 'projector' not in n:p.requires_grad = False\n",
    "        else:p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],lr=args.learning_rate) \n",
    "\n",
    "# Add learning rate scheduler\n",
    "num_training_steps = args.num_train_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_warmup_steps=int(num_training_steps * args.warmup_ratio)  # 3% warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3437c1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/121 [00:00<?, ?it/s]c:\\Users\\LENOVO\\Documents\\Skripsi\\RAG-way\\xRAG\\language_modeling\\preprocessing.py:439: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ret['retriever_embeddings'] = torch.stack([torch.tensor(x[retriever_embeds_col]).to('cuda:0') for x in samples])\n",
      "  0%|          | 0/121 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 384] at entry 0 and [4, 384] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     10\u001b[0m epoch_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader):\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m     16\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxrag_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     17\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxrag_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     18\u001b[0m         retrieval_embeds \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretriever_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     19\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\Documents\\Skripsi\\RAG-way\\xRAG\\language_modeling\\preprocessing.py:439\u001b[0m, in \u001b[0;36mcollator\u001b[1;34m(samples, llm_tokenizer, llm_tokenizer_pad_token, xrag_input_ids_col, xrag_labels_col, text_input_ids_col, text_labels_col, retriever_embeds_col)\u001b[0m\n\u001b[0;32m    427\u001b[0m xrag_input_ids, xrag_attention_mask, xrag_labels \u001b[38;5;241m=\u001b[39m padding(\n\u001b[0;32m    428\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m[x[xrag_input_ids_col] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m samples],\n\u001b[0;32m    429\u001b[0m     labels\u001b[38;5;241m=\u001b[39m[x[xrag_labels_col] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m samples] \u001b[38;5;28;01mif\u001b[39;00m xrag_labels_col \u001b[38;5;129;01min\u001b[39;00m samples[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    430\u001b[0m     padding_side\u001b[38;5;241m=\u001b[39mllm_tokenizer\u001b[38;5;241m.\u001b[39mpadding_side\n\u001b[0;32m    431\u001b[0m )\n\u001b[0;32m    433\u001b[0m ret \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxrag_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: xrag_input_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m),  \u001b[38;5;66;03m# token hasil embedding llm_tokenizer \u001b[39;00m\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxrag_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: xrag_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m),  \u001b[38;5;66;03m# memberi masking agar pad tidak dipelajari selama training\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxrag_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m: xrag_labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m),  \u001b[38;5;66;03m# token hasil embedding llm_tokenizer, dimana instruksi sistem sudah disensor (dengan -100)\u001b[39;00m\n\u001b[0;32m    437\u001b[0m }\n\u001b[1;32m--> 439\u001b[0m ret[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretriever_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mtensor(x[retriever_embeds_col])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m samples])\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_input_ids_col \u001b[38;5;129;01min\u001b[39;00m samples[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    442\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m [x[text_input_ids_col] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m samples]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 384] at entry 0 and [4, 384] at entry 2"
     ]
    }
   ],
   "source": [
    "# Inisialisasi list untuk menyimpan loss\n",
    "nll_train_losses = []\n",
    "kl_train_losses = []\n",
    "train_losses = []\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(args.num_train_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids = batch['xrag_input_ids'],\n",
    "            attention_mask = batch['xrag_attention_mask'],\n",
    "            retrieval_embeds = batch['retriever_embeddings']\n",
    "        )\n",
    "        del batch['xrag_input_ids']\n",
    "        del batch['xrag_attention_mask']\n",
    "        del batch['retriever_embeddings']\n",
    "        torch.cuda.empty_cache()\n",
    "        logits = outputs.logits\n",
    "        labels = batch['xrag_labels']\n",
    "\n",
    "        nll_loss = get_nll_loss(logits=logits, labels=labels, vocab_size=vocab_size)\n",
    "\n",
    "        loss = args.alpha_nll * nll_loss\n",
    "        nll_train_losses.append(loss.item())\n",
    "\n",
    "        if args.alpha_kl is not None and args.alpha_kl > 0.0:\n",
    "                    \n",
    "            ## forward with retrieval tokens\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                teacher_outputs = model(\n",
    "                    input_ids = batch['input_ids'],\n",
    "                    attention_mask = batch['attention_mask'],\n",
    "                )\n",
    "                del batch['input_ids']\n",
    "                del batch['attention_mask']\n",
    "                torch.cuda.empty_cache()\n",
    "                model.train()\n",
    "\n",
    "            kl_loss = get_kl_loss(\n",
    "                teacher_logits=teacher_outputs.logits,\n",
    "                teacher_labels=batch['labels'],\n",
    "                student_logits=outputs.logits,\n",
    "                student_labels=batch['xrag_labels'],\n",
    "                temperature=args.kl_temperature,\n",
    "            )\n",
    "            kl_loss = args.alpha_kl * kl_loss\n",
    "            kl_train_losses.append(kl_loss.item())\n",
    "            loss += kl_loss\n",
    "            \n",
    "            del batch['labels']\n",
    "            torch.cuda.empty_cache()\n",
    "        del batch['xrag_labels']\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Simpan loss untuk tiap batch\n",
    "        train_losses.append(loss.item())\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc0be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating after epoch 1...\n",
      "NLL Loss: 2.417423963546753\n",
      "NLL Loss: 1.8294185400009155\n",
      "NLL Loss: 2.1833391189575195\n",
      "NLL Loss: 2.4166274070739746\n",
      "NLL Loss: 2.331878662109375\n",
      "NLL Loss: 1.9211673736572266\n",
      "NLL Loss: 2.3154242038726807\n",
      "NLL Loss: 2.2908835411071777\n",
      "NLL Loss: 2.3752048015594482\n",
      "NLL Loss: 1.6519795656204224\n",
      "NLL Loss: 1.7484678030014038\n",
      "NLL Loss: 2.2158658504486084\n",
      "NLL Loss: 1.4776960611343384\n",
      "NLL Loss: 2.050281286239624\n",
      "NLL Loss: 2.203371286392212\n",
      "NLL Loss: 2.0610008239746094\n",
      "NLL Loss: 2.161954402923584\n",
      "NLL Loss: 2.3024446964263916\n",
      "NLL Loss: 1.7361009120941162\n",
      "NLL Loss: 2.1097402572631836\n",
      "NLL Loss: 2.062561511993408\n",
      "NLL Loss: 2.353330135345459\n",
      "NLL Loss: 2.099410057067871\n",
      "NLL Loss: 2.580258369445801\n",
      "NLL Loss: 1.9219951629638672\n",
      "NLL Loss: 2.3179144859313965\n",
      "NLL Loss: 1.8331387042999268\n",
      "NLL Loss: 2.1682233810424805\n",
      "NLL Loss: 2.4154458045959473\n",
      "NLL Loss: 2.121776819229126\n",
      "NLL Loss: 2.0236308574676514\n",
      "NLL Loss: 1.993772268295288\n",
      "NLL Loss: 2.059504508972168\n",
      "NLL Loss: 1.9781078100204468\n",
      "NLL Loss: 1.934195637702942\n",
      "NLL Loss: 1.8435516357421875\n",
      "NLL Loss: 2.0844740867614746\n",
      "NLL Loss: 2.2521371841430664\n",
      "NLL Loss: 1.9295092821121216\n",
      "NLL Loss: 2.3007307052612305\n",
      "NLL Loss: 2.2871997356414795\n",
      "NLL Loss: 1.9688664674758911\n",
      "NLL Loss: 1.677938461303711\n",
      "NLL Loss: 2.1000308990478516\n",
      "NLL Loss: 1.5097813606262207\n",
      "NLL Loss: 2.2609381675720215\n",
      "NLL Loss: 1.8736072778701782\n",
      "NLL Loss: 2.131420135498047\n",
      "NLL Loss: 1.9565110206604004\n",
      "NLL Loss: 2.3332679271698\n",
      "NLL Loss: 1.8229491710662842\n",
      "NLL Loss: 2.141587257385254\n",
      "NLL Loss: 1.1910151243209839\n",
      "NLL Loss: 2.438013792037964\n",
      "NLL Loss: 2.0570740699768066\n",
      "NLL Loss: 2.20292067527771\n",
      "NLL Loss: 1.5906972885131836\n",
      "NLL Loss: 2.290471315383911\n",
      "NLL Loss: 1.885715365409851\n",
      "NLL Loss: 1.7930935621261597\n",
      "NLL Loss: 2.190927028656006\n",
      "NLL Loss: 2.2058727741241455\n",
      "NLL Loss: 1.9387452602386475\n",
      "NLL Loss: 1.8677743673324585\n",
      "NLL Loss: 2.2570769786834717\n",
      "NLL Loss: 2.2636866569519043\n",
      "NLL Loss: 2.091099977493286\n",
      "NLL Loss: 1.987000584602356\n",
      "NLL Loss: 2.786173105239868\n",
      "NLL Loss: 2.1060330867767334\n",
      "NLL Loss: 2.21589994430542\n",
      "NLL Loss: 1.5979344844818115\n",
      "NLL Loss: 1.9488500356674194\n",
      "NLL Loss: 1.7969975471496582\n",
      "NLL Loss: 2.212843179702759\n",
      "NLL Loss: 1.5326207876205444\n",
      "NLL Loss: 2.079983949661255\n",
      "NLL Loss: 2.136568069458008\n",
      "NLL Loss: 2.2650065422058105\n",
      "NLL Loss: 2.200861692428589\n",
      "NLL Loss: 2.2543532848358154\n",
      "NLL Loss: 2.212848424911499\n",
      "NLL Loss: 2.288994550704956\n",
      "NLL Loss: 1.8664743900299072\n",
      "NLL Loss: 2.0192947387695312\n",
      "NLL Loss: 1.9783663749694824\n",
      "NLL Loss: 2.151707172393799\n",
      "NLL Loss: 1.9126629829406738\n",
      "NLL Loss: 1.8021711111068726\n",
      "NLL Loss: 1.843459963798523\n",
      "NLL Loss: 2.1774957180023193\n",
      "NLL Loss: 2.5770864486694336\n",
      "NLL Loss: 2.3994791507720947\n",
      "NLL Loss: 2.0320308208465576\n",
      "NLL Loss: 2.2449965476989746\n",
      "NLL Loss: 2.0561811923980713\n",
      "NLL Loss: 2.1149652004241943\n",
      "NLL Loss: 2.269200325012207\n",
      "NLL Loss: 2.617191791534424\n",
      "NLL Loss: 2.172556161880493\n",
      "NLL Loss: 2.2460246086120605\n",
      "NLL Loss: 2.229052782058716\n",
      "NLL Loss: 2.101999044418335\n",
      "NLL Loss: 2.030184745788574\n",
      "NLL Loss: 1.714316725730896\n",
      "NLL Loss: 2.0217783451080322\n",
      "NLL Loss: 2.290452480316162\n",
      "NLL Loss: 1.8709619045257568\n",
      "NLL Loss: 1.624527931213379\n",
      "NLL Loss: 2.2407238483428955\n",
      "NLL Loss: 2.6561291217803955\n",
      "NLL Loss: 2.0890960693359375\n",
      "NLL Loss: 2.0196847915649414\n",
      "NLL Loss: 2.2596006393432617\n",
      "NLL Loss: 2.058993101119995\n",
      "NLL Loss: 1.8649643659591675\n",
      "NLL Loss: 2.543146848678589\n",
      "NLL Loss: 1.8496655225753784\n",
      "NLL Loss: 2.5414159297943115\n",
      "NLL Loss: 1.952672004699707\n",
      "NLL Loss: 2.1124393939971924\n",
      "NLL Loss: 1.8233239650726318\n",
      "NLL Loss: 1.8492212295532227\n",
      "NLL Loss: 1.845229983329773\n",
      "NLL Loss: 1.962257981300354\n",
      "NLL Loss: 2.291106939315796\n",
      "NLL Loss: 2.097745180130005\n",
      "NLL Loss: 1.8155126571655273\n",
      "NLL Loss: 1.708805799484253\n",
      "NLL Loss: 2.0335521697998047\n",
      "NLL Loss: 1.3029637336730957\n",
      "NLL Loss: 2.010378360748291\n",
      "NLL Loss: 2.162277936935425\n",
      "NLL Loss: 1.9545539617538452\n",
      "NLL Loss: 1.856303095817566\n",
      "NLL Loss: 1.8051146268844604\n",
      "NLL Loss: 2.4936487674713135\n",
      "NLL Loss: 2.363943338394165\n",
      "NLL Loss: 1.3332524299621582\n",
      "NLL Loss: 2.120388984680176\n",
      "NLL Loss: 2.1306276321411133\n",
      "NLL Loss: 2.1954267024993896\n",
      "NLL Loss: 2.3843703269958496\n",
      "NLL Loss: 2.225998640060425\n",
      "NLL Loss: 1.7756563425064087\n",
      "NLL Loss: 1.9910887479782104\n",
      "NLL Loss: 2.0809004306793213\n",
      "NLL Loss: 2.0606465339660645\n",
      "NLL Loss: 2.171701669692993\n",
      "NLL Loss: 2.308156728744507\n",
      "NLL Loss: 2.148533344268799\n",
      "NLL Loss: 2.0715014934539795\n",
      "NLL Loss: 2.3033268451690674\n",
      "NLL Loss: 2.3052878379821777\n",
      "NLL Loss: 2.320141315460205\n",
      "NLL Loss: 1.9904240369796753\n",
      "NLL Loss: 2.1697115898132324\n",
      "NLL Loss: 1.7913638353347778\n",
      "NLL Loss: 1.8858363628387451\n",
      "NLL Loss: 1.7933531999588013\n",
      "NLL Loss: 2.3573946952819824\n",
      "NLL Loss: 2.271912097930908\n",
      "NLL Loss: 1.6249408721923828\n",
      "NLL Loss: 2.1011242866516113\n",
      "NLL Loss: 2.5286343097686768\n",
      "NLL Loss: 2.251887798309326\n",
      "NLL Loss: 2.135802984237671\n",
      "NLL Loss: 2.1869442462921143\n",
      "NLL Loss: 2.0479815006256104\n",
      "NLL Loss: 2.1829493045806885\n",
      "NLL Loss: 2.204719305038452\n",
      "NLL Loss: 1.8738254308700562\n",
      "NLL Loss: 1.7584624290466309\n",
      "NLL Loss: 1.836098551750183\n",
      "NLL Loss: 2.0379016399383545\n",
      "NLL Loss: 2.1249449253082275\n",
      "NLL Loss: 2.262432098388672\n",
      "NLL Loss: 1.8175203800201416\n",
      "NLL Loss: 2.2596137523651123\n",
      "NLL Loss: 2.08699631690979\n",
      "NLL Loss: 1.9271528720855713\n",
      "NLL Loss: 2.435451030731201\n",
      "NLL Loss: 1.7887704372406006\n",
      "NLL Loss: 2.158381938934326\n",
      "NLL Loss: 2.38677978515625\n",
      "NLL Loss: 2.277592897415161\n",
      "NLL Loss: 2.180640697479248\n",
      "NLL Loss: 2.031266927719116\n",
      "NLL Loss: 2.0397841930389404\n",
      "NLL Loss: 2.263948917388916\n",
      "NLL Loss: 2.355290174484253\n",
      "NLL Loss: 1.9986406564712524\n",
      "NLL Loss: 2.349050760269165\n",
      "NLL Loss: 2.3820266723632812\n",
      "NLL Loss: 1.9016917943954468\n",
      "NLL Loss: 2.04723858833313\n",
      "NLL Loss: 2.3294994831085205\n",
      "NLL Loss: 2.106600761413574\n",
      "NLL Loss: 2.0010933876037598\n",
      "NLL Loss: 2.057396173477173\n",
      "NLL Loss: 2.3197150230407715\n",
      "NLL Loss: 2.9502415657043457\n",
      "NLL Loss: 2.2119812965393066\n",
      "NLL Loss: 2.0306193828582764\n",
      "NLL Loss: 2.2636380195617676\n",
      "NLL Loss: 1.9088926315307617\n",
      "NLL Loss: 2.3474013805389404\n",
      "NLL Loss: 2.099149703979492\n",
      "NLL Loss: 1.8522464036941528\n",
      "NLL Loss: 2.3719778060913086\n",
      "NLL Loss: 2.157384157180786\n",
      "NLL Loss: 1.8796474933624268\n",
      "NLL Loss: 1.948395013809204\n",
      "NLL Loss: 1.7700190544128418\n",
      "NLL Loss: 1.780709147453308\n",
      "NLL Loss: 2.2185192108154297\n",
      "NLL Loss: 1.6466270685195923\n",
      "NLL Loss: 2.110668897628784\n",
      "NLL Loss: 2.5498836040496826\n",
      "NLL Loss: 1.946752905845642\n",
      "NLL Loss: 2.2826623916625977\n",
      "NLL Loss: 1.8464810848236084\n",
      "WARN/ING: Token xRAG pada posisi 1 dan 2 memiliki nilai yang sama setelah disematkan.\n",
      "NLL Loss: 2.720869779586792\n",
      "NLL Loss: 1.9325121641159058\n",
      "NLL Loss: 2.078610897064209\n",
      "NLL Loss: 2.2325093746185303\n",
      "NLL Loss: 2.02346134185791\n",
      "NLL Loss: 1.746948480606079\n",
      "NLL Loss: 1.930216670036316\n",
      "NLL Loss: 1.7028255462646484\n",
      "NLL Loss: 1.8191672563552856\n",
      "NLL Loss: 2.0539748668670654\n",
      "NLL Loss: 1.9879138469696045\n",
      "NLL Loss: 1.9935606718063354\n",
      "NLL Loss: 1.891759991645813\n",
      "NLL Loss: 2.3623852729797363\n",
      "NLL Loss: 1.757135272026062\n",
      "NLL Loss: 2.3557240962982178\n",
      "NLL Loss: 1.9566015005111694\n",
      "NLL Loss: 2.0614840984344482\n",
      "NLL Loss: 1.941796898841858\n",
      "NLL Loss: 1.804762840270996\n",
      "NLL Loss: 2.1285526752471924\n",
      "NLL Loss: 1.8915600776672363\n",
      "NLL Loss: 2.1274659633636475\n",
      "NLL Loss: 2.101839780807495\n",
      "NLL Loss: 1.8054884672164917\n",
      "NLL Loss: 2.2498345375061035\n",
      "NLL Loss: 2.0648560523986816\n",
      "NLL Loss: 2.148712635040283\n",
      "NLL Loss: 2.2211453914642334\n",
      "NLL Loss: 2.296689987182617\n",
      "NLL Loss: 1.9820297956466675\n",
      "NLL Loss: 2.0061843395233154\n",
      "NLL Loss: 2.1458563804626465\n",
      "NLL Loss: 1.887673258781433\n",
      "NLL Loss: 1.9847747087478638\n",
      "NLL Loss: 2.1687934398651123\n",
      "NLL Loss: 2.1924469470977783\n",
      "NLL Loss: 2.0894017219543457\n",
      "NLL Loss: 2.272153854370117\n",
      "NLL Loss: 2.6951403617858887\n",
      "NLL Loss: 2.2117042541503906\n",
      "NLL Loss: 1.896270990371704\n",
      "NLL Loss: 1.9751399755477905\n",
      "NLL Loss: 2.079779863357544\n",
      "NLL Loss: 2.376664161682129\n",
      "NLL Loss: 1.8590996265411377\n",
      "NLL Loss: 2.1958088874816895\n",
      "NLL Loss: 1.93169105052948\n",
      "NLL Loss: 2.2325823307037354\n",
      "NLL Loss: 1.9103344678878784\n",
      "NLL Loss: 2.1477506160736084\n",
      "NLL Loss: 1.870841383934021\n",
      "NLL Loss: 1.9880874156951904\n",
      "NLL Loss: 2.14683198928833\n",
      "NLL Loss: 2.4653232097625732\n",
      "NLL Loss: 2.442326545715332\n",
      "NLL Loss: 1.9448461532592773\n",
      "NLL Loss: 2.591082811355591\n",
      "NLL Loss: 1.9782886505126953\n",
      "NLL Loss: 2.46219801902771\n",
      "NLL Loss: 2.1144344806671143\n",
      "NLL Loss: 2.3652842044830322\n",
      "NLL Loss: 2.2941999435424805\n",
      "NLL Loss: 1.9175431728363037\n",
      "NLL Loss: 1.92540442943573\n",
      "NLL Loss: 1.621793270111084\n",
      "NLL Loss: 2.495107412338257\n",
      "NLL Loss: 2.0317084789276123\n",
      "NLL Loss: 1.7856597900390625\n",
      "NLL Loss: 2.032438039779663\n",
      "NLL Loss: 2.3920443058013916\n",
      "NLL Loss: 2.10388445854187\n",
      "NLL Loss: 2.0218684673309326\n",
      "NLL Loss: 2.1523919105529785\n",
      "NLL Loss: 1.9960839748382568\n",
      "NLL Loss: 2.3519949913024902\n",
      "NLL Loss: 1.9771031141281128\n",
      "NLL Loss: 1.707537293434143\n",
      "NLL Loss: 1.916764497756958\n",
      "NLL Loss: 1.99562406539917\n",
      "NLL Loss: 2.0216171741485596\n",
      "NLL Loss: 2.040156126022339\n",
      "NLL Loss: 2.189714193344116\n",
      "NLL Loss: 2.507798194885254\n",
      "NLL Loss: 1.9066333770751953\n",
      "NLL Loss: 2.2877919673919678\n",
      "NLL Loss: 2.3825597763061523\n",
      "NLL Loss: 1.7084310054779053\n",
      "NLL Loss: 2.0178184509277344\n",
      "NLL Loss: 2.3391172885894775\n",
      "NLL Loss: 2.326780319213867\n",
      "NLL Loss: 1.779394507408142\n",
      "NLL Loss: 2.1743924617767334\n",
      "NLL Loss: 2.051893472671509\n",
      "NLL Loss: 1.89076828956604\n",
      "NLL Loss: 1.7866506576538086\n",
      "NLL Loss: 2.1277260780334473\n",
      "NLL Loss: 2.3498353958129883\n",
      "NLL Loss: 2.3728508949279785\n",
      "NLL Loss: 1.9787170886993408\n",
      "NLL Loss: 2.4727423191070557\n",
      "NLL Loss: 1.7329450845718384\n",
      "NLL Loss: 2.323868751525879\n",
      "NLL Loss: 1.7823021411895752\n",
      "NLL Loss: 1.9973294734954834\n",
      "NLL Loss: 2.371941328048706\n",
      "NLL Loss: 2.1368043422698975\n",
      "NLL Loss: 2.395878791809082\n",
      "NLL Loss: 1.8702901601791382\n",
      "NLL Loss: 1.8091193437576294\n",
      "NLL Loss: 1.9052464962005615\n",
      "NLL Loss: 2.0455245971679688\n",
      "NLL Loss: 1.8006412982940674\n",
      "NLL Loss: 2.4454071521759033\n",
      "NLL Loss: 2.3325932025909424\n",
      "NLL Loss: 1.9183918237686157\n",
      "NLL Loss: 2.1504883766174316\n",
      "NLL Loss: 1.9851384162902832\n",
      "NLL Loss: 1.9477683305740356\n",
      "NLL Loss: 1.8074830770492554\n",
      "NLL Loss: 1.619153618812561\n",
      "NLL Loss: 2.6296839714050293\n",
      "NLL Loss: 2.646414279937744\n",
      "NLL Loss: 2.2533621788024902\n",
      "NLL Loss: 2.274827480316162\n",
      "NLL Loss: 2.126650810241699\n",
      "NLL Loss: 1.8203884363174438\n",
      "NLL Loss: 2.0133297443389893\n",
      "NLL Loss: 1.9468657970428467\n",
      "NLL Loss: 2.407456398010254\n",
      "NLL Loss: 2.500934600830078\n",
      "NLL Loss: 2.300710678100586\n",
      "NLL Loss: 1.903328776359558\n",
      "NLL Loss: 1.8956799507141113\n",
      "NLL Loss: 1.7568825483322144\n",
      "NLL Loss: 2.1519935131073\n",
      "NLL Loss: 2.0497782230377197\n",
      "NLL Loss: 1.6154121160507202\n",
      "NLL Loss: 2.0140457153320312\n",
      "NLL Loss: 2.0984885692596436\n",
      "NLL Loss: 2.4945826530456543\n",
      "NLL Loss: 1.8564527034759521\n",
      "NLL Loss: 1.8867183923721313\n",
      "NLL Loss: 2.036127805709839\n",
      "NLL Loss: 1.8998609781265259\n",
      "NLL Loss: 2.046283483505249\n",
      "NLL Loss: 2.062864065170288\n",
      "NLL Loss: 2.095505714416504\n",
      "NLL Loss: 1.9076368808746338\n",
      "NLL Loss: 2.4857447147369385\n",
      "NLL Loss: 1.7364410161972046\n",
      "NLL Loss: 2.0462307929992676\n",
      "NLL Loss: 2.3249502182006836\n",
      "Perplexity on dev set: 8.04885482788086\n",
      "Average training loss for epoch 1: 2.1762218554814656\n"
     ]
    }
   ],
   "source": [
    "# Lakukan validasi setelah setiap epoch\n",
    "dev_losses = []\n",
    "\n",
    "\n",
    "if dev_dataloader is not None:\n",
    "    if args.task_type == 'pretrain':\n",
    "        print(f\"Validating after epoch {epoch + 1}...\")\n",
    "        ppl = validate_during_pretrain(model, dev_dataloader, vocab_size)\n",
    "        print(f\"Perplexity on dev set: {ppl}\")\n",
    "\n",
    "        dev_losses.append(ppl.item())\n",
    "\n",
    "# Simpan rata-rata loss setiap epoch ke dalam list atau file\n",
    "print(f\"Average training loss for epoch {epoch + 1}: {epoch_train_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e6c6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../output\\\\2025-05-05_22-50-00\\\\e5small-llama1Binstruct-batch4\\\\tokenizer_config.json',\n",
       " '../output\\\\2025-05-05_22-50-00\\\\e5small-llama1Binstruct-batch4\\\\special_tokens_map.json',\n",
       " '../output\\\\2025-05-05_22-50-00\\\\e5small-llama1Binstruct-batch4\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = os.path.join(args.output_dir, current_time)\n",
    "model_output_dir = os.path.join(output_dir, f\"e5small-llama1Binstruct-batch4\")\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "model.save_pretrained(model_output_dir)\n",
    "tokenizer.save_pretrained(model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb79c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan ke CSV\n",
    "import csv\n",
    "\n",
    "with open(os.path.join(output_dir, \"train_loss.csv\"), mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['step', 'train_loss'])\n",
    "    for step, loss in enumerate(train_losses):\n",
    "        writer.writerow([step, loss])\n",
    "\n",
    "with open(os.path.join(output_dir, \"dev_loss.csv\"), mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['epoch', 'dev_loss'])\n",
    "    for epoch, loss in enumerate(dev_losses):\n",
    "        writer.writerow([epoch, loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf8388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
