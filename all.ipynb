{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6fbe1c",
   "metadata": {},
   "source": [
    "# Load Model, Tokenizer, dan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c401e7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "dataset = load_from_disk(\"generated_data/research-raw-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c910ae6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels'],\n",
       "        num_rows: 5120\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Membuat salinan dataset\n",
    "ds = deepcopy(dataset)\n",
    "\n",
    "# Tentukan jumlah baris yang ingin dipindahkan\n",
    "num_rows_to_move = 578\n",
    "\n",
    "# Pilih 577 baris acak dari split 'dev'\n",
    "dev_dataset = ds['dev']\n",
    "\n",
    "selected_rows = dev_dataset.select(range(num_rows_to_move))  # Ambil 577 baris pertama setelah shuffle\n",
    "\n",
    "# Hapus 577 baris yang sudah dipilih dari 'dev'\n",
    "remaining_dev = dev_dataset.select(range(num_rows_to_move, len(dev_dataset)))\n",
    "\n",
    "# Konversi ke DataFrame pandas untuk dapat menggunakan concat\n",
    "train_df = ds['train'].to_pandas()\n",
    "selected_rows_df = selected_rows.to_pandas()\n",
    "\n",
    "# Gabungkan keduanya dengan pandas.concat\n",
    "new_train_df = pd.concat([train_df, selected_rows_df], ignore_index=True)\n",
    "\n",
    "# Kembali ke dataset HuggingFace dari DataFrame\n",
    "new_train = Dataset.from_pandas(new_train_df)\n",
    "\n",
    "# Perbarui split train dan dev\n",
    "ds['train'] = new_train\n",
    "ds['dev'] = remaining_dev\n",
    "\n",
    "# Sekarang, finetuning_dataset['train'] sudah berisi 577 baris tambahan, dan finetuning_dataset['dev'] sudah dikurangi.\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "179b29c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 32.70ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.30s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 31.94ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 43.60ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/khalidrizki/postretrieve-raw-dataset/commit/79184ccb3d078a7810a0ad1282ffd266a9a64719', commit_message='Upload dataset', commit_description='', oid='79184ccb3d078a7810a0ad1282ffd266a9a64719', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/khalidrizki/postretrieve-raw-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='khalidrizki/postretrieve-raw-dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub('khalidrizki/postretrieve-raw-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c63aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERHASIL MELOAD MODEL DAN DATASET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from general_utils import load_model_and_tokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\" \n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)\n",
    "\n",
    "print(\"BERHASIL MELOAD MODEL DAN DATASET\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a46abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels'],\n",
       "        num_rows: 4542\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels'],\n",
       "        num_rows: 1143\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for split in dataset.keys():\n",
    "#     dataset[split] = dataset[split].select(range(5))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dda177",
   "metadata": {},
   "source": [
    "# RECOMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95010dbb",
   "metadata": {},
   "source": [
    "## Membuat dataset finetuning untuk melatih kompresor RECOMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00767d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "recomp_draft_dataset = deepcopy(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3a850f",
   "metadata": {},
   "source": [
    "Mengekstrak text & is_positive dari ranked passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc6e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from general_utils import extract_topk_texts\n",
    "extract_fn = partial(\n",
    "    extract_topk_texts, \n",
    "    ranked_units='ranked_truncPassages_with_labels', \n",
    "    returned_units_col='joined_passages', \n",
    "    returned_labels_col='passages_label'\n",
    ")\n",
    "for split in recomp_draft_dataset.keys():\n",
    "    recomp_draft_dataset[split] = recomp_draft_dataset[split].map(extract_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6dfadf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing split: train:   0%|          | 0/4542 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Summarizing split: train:   0%|          | 3/4542 [00:16<6:58:15,  5.53s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(current_dataset, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarizing split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m         answer \u001b[38;5;241m=\u001b[39m generate_per_row(  \u001b[38;5;66;03m# decoded_output, \u001b[39;00m\n\u001b[0;32m     13\u001b[0m             row, \n\u001b[0;32m     14\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     15\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoined_passages\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     16\u001b[0m             tokenizer, \n\u001b[0;32m     17\u001b[0m             model, \n\u001b[0;32m     18\u001b[0m             config\u001b[38;5;241m.\u001b[39mdevice_type, \n\u001b[0;32m     19\u001b[0m             instruction\n\u001b[0;32m     20\u001b[0m         )\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m# decoded_outputs.append(decoded_output)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m         summaries\u001b[38;5;241m.\u001b[39mappend(answer)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\Documents\\Skripsi\\post-retrieval-eval\\general_utils.py:213\u001b[0m, in \u001b[0;36mgenerate_per_row\u001b[1;34m(row, query_col, ctx_col, tokenizer, model, device_type, instruction)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 213\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    214\u001b[0m             inputs,\n\u001b[0;32m    215\u001b[0m             max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m52\u001b[39m,\n\u001b[0;32m    216\u001b[0m             do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    217\u001b[0m             pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m    218\u001b[0m             return_dict_in_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    219\u001b[0m         )\n\u001b[0;32m    221\u001b[0m prompt_and_completion \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msequences\n\u001b[0;32m    222\u001b[0m completion \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msequences[:, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:]\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\utils.py:2460\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[0;32m   2452\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2453\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2454\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2455\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2456\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2457\u001b[0m     )\n\u001b[0;32m   2459\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2460\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2461\u001b[0m         input_ids,\n\u001b[0;32m   2462\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2463\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2464\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2465\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2466\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2467\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2468\u001b[0m     )\n\u001b[0;32m   2470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2471\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2472\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2473\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2474\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2475\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2476\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2477\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\utils.py:3429\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3427\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3428\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3429\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3431\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3432\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3433\u001b[0m     outputs,\n\u001b[0;32m   3434\u001b[0m     model_kwargs,\n\u001b[0;32m   3435\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3436\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:850\u001b[0m, in \u001b[0;36mQwen3ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    845\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    846\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    849\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 850\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    851\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    852\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    853\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    854\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    855\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    856\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    857\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    858\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    859\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    861\u001b[0m )\n\u001b[0;32m    863\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:576\u001b[0m, in \u001b[0;36mQwen3Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    565\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[0;32m    566\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m         position_embeddings,\n\u001b[0;32m    574\u001b[0m     )\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    577\u001b[0m         hidden_states,\n\u001b[0;32m    578\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    579\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    580\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    581\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    582\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    583\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    584\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    585\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    586\u001b[0m     )\n\u001b[0;32m    588\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:289\u001b[0m, in \u001b[0;36mQwen3DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    290\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    291\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    292\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    293\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    294\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    295\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    296\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    297\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    299\u001b[0m )\n\u001b[0;32m    300\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:217\u001b[0m, in \u001b[0;36mQwen3Attention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    215\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m--> 217\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_norm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    218\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_norm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    219\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:73\u001b[0m, in \u001b[0;36mQwen3RMSNorm.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m     72\u001b[0m     input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m---> 73\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     74\u001b[0m     variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     75\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from general_utils import generate_per_row\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "instruction = '{context}\\n\\nRingkaslah teks di atas menjadi tepat 2 kalimat (40 kata) agar menjawab pertanyaan secara mendetail. TANPA PENGANTAR. Pertanyaan: \"{query}' \n",
    "for split in recomp_draft_dataset.keys():\n",
    "    current_dataset = recomp_draft_dataset[split]\n",
    "    decoded_outputs = []\n",
    "    summaries = []\n",
    "\n",
    "    for row in tqdm(current_dataset, desc=f\"Summarizing split: {split}\"):\n",
    "        try:\n",
    "            answer = generate_per_row(  # decoded_output, \n",
    "                row, \n",
    "                'query', \n",
    "                'joined_passages', \n",
    "                tokenizer, \n",
    "                model, \n",
    "                config.device_type, \n",
    "                instruction\n",
    "            )\n",
    "            # decoded_outputs.append(decoded_output)\n",
    "            summaries.append(answer)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Error during summarization for row:\\n\"\n",
    "                f\"Query: {row['query']}\\n\"\n",
    "                f\"Context: {row['joined_passages'][:200]}...\\n\"\n",
    "                f\"Error message: {str(e)}\"\n",
    "            )\n",
    "\n",
    "    # dataset[split] = current_dataset.add_column(\"decoded_output\", decoded_outputs)\n",
    "    recomp_draft_dataset[split] = recomp_draft_dataset[split].add_column(\"summary\", summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f05a524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4542/4542 [00:00<00:00, 87498.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1143/1143 [00:00<00:00, 65990.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 565/565 [00:00<00:00, 38314.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "recomp_draft_dataset.save_to_disk('generated_data/RECOMP_draft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a62c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "recomp_draft_dataset = load_from_disk('generated_data/RECOMP_draft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848f3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses (w/ & wo/ summary) on split: train: 100%|██████████| 4542/4542 [5:54:29<00:00,  4.68s/it]  \n",
      "Generating responses (w/ & wo/ summary) on split: dev: 100%|██████████| 1143/1143 [1:30:02<00:00,  4.73s/it]\n",
      "Generating responses (w/ & wo/ summary) on split: test: 100%|██████████| 565/565 [45:39<00:00,  4.85s/it] \n"
     ]
    }
   ],
   "source": [
    "from metrics import evaluate_substringmatch_f1\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "\n",
    "finetuning_dict = {}\n",
    "\n",
    "instruction_w_summary = \"Konteks: {context}\\nBerdasarkan konteks sebelumnya, jawab pertanyaan berikut dalam dua kalimat. Pertanyaan: {query}\"\n",
    "instruction_wo_summary= \"Jawab pertanyaan berikut dalam dua kalimat. Pertanyaan: {query}\"\n",
    "for split in recomp_draft_dataset.keys():\n",
    "    current_dataset = recomp_draft_dataset[split]\n",
    "    results = []\n",
    "    for row in tqdm(current_dataset, desc=f\"Generating responses (w/ & wo/ summary) on split: {split}\"):\n",
    "        label = row['answer']\n",
    "        completion_w_summary = generate_per_row( # decoded_summ_output, \n",
    "            row=row, \n",
    "            query_col='query', \n",
    "            ctx_col='summary', \n",
    "            tokenizer=tokenizer, \n",
    "            model=model, \n",
    "            device_type=config.device_type, \n",
    "            instruction=instruction_w_summary\n",
    "        )\n",
    "        completion_wo_summary = generate_per_row(  #decoded_wo_summ_output, \n",
    "            row=row, \n",
    "            query_col='query', \n",
    "            ctx_col=None, \n",
    "            tokenizer=tokenizer, \n",
    "            model=model, \n",
    "            device_type=config.device_type, \n",
    "            instruction=instruction_wo_summary\n",
    "        )\n",
    "\n",
    "        sm_w, f1_w = evaluate_substringmatch_f1(completion_w_summary.strip(), label.strip())\n",
    "        sm_wo, f1_wo=evaluate_substringmatch_f1(completion_wo_summary.strip(), label.strip())\n",
    "\n",
    "        if (sm_wo == 1 and sm_wo > sm_w) or (f1_wo >= f1_w):\n",
    "            final_summary = \"\"\n",
    "        else:\n",
    "            final_summary = row['summary']\n",
    "        \n",
    "        results.append(\n",
    "            {\n",
    "                \"query\": row['query'], \n",
    "                \"passages\":row['ranked_truncPassages_with_labels'], \n",
    "                \"summary\": row['summary'], \n",
    "                \"final_summary\": final_summary, \n",
    "                \"label\": label, \n",
    "                \"model_outputs\": {\n",
    "                    \"w_summary\": {\n",
    "                        \"completion\": completion_w_summary,\n",
    "                        \"em\": sm_w,\n",
    "                        \"f1\": f1_w, \n",
    "                        # \"decoded_output\": decoded_summ_output\n",
    "                    },\n",
    "                    \"wo_summary\": {\n",
    "                        \"completion\": completion_wo_summary,\n",
    "                        \"em\": sm_wo,\n",
    "                        \"f1\": f1_wo, \n",
    "                        # \"decoded_output\": decoded_wo_summ_output\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    finetuning_dict[split] = Dataset.from_list(results)\n",
    "finetuning_dataset = DatasetDict(finetuning_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bc9a68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'label', 'generated_answer'],\n",
       "        num_rows: 5120\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'label', 'generated_answer'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'label', 'generated_answer'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "\n",
    "# Membuat salinan dataset\n",
    "ds = deepcopy(finetuning_dataset)\n",
    "\n",
    "# Tentukan jumlah baris yang ingin dipindahkan\n",
    "num_rows_to_move = 578\n",
    "\n",
    "# Pilih 577 baris acak dari split 'dev'\n",
    "dev_dataset = ds['dev']\n",
    "\n",
    "selected_rows = dev_dataset.select(range(num_rows_to_move))  # Ambil 577 baris pertama setelah shuffle\n",
    "\n",
    "# Hapus 577 baris yang sudah dipilih dari 'dev'\n",
    "remaining_dev = dev_dataset.select(range(num_rows_to_move, len(dev_dataset)))\n",
    "\n",
    "# Konversi ke DataFrame pandas untuk dapat menggunakan concat\n",
    "train_df = ds['train'].to_pandas()\n",
    "selected_rows_df = selected_rows.to_pandas()\n",
    "\n",
    "# Gabungkan keduanya dengan pandas.concat\n",
    "new_train_df = pd.concat([train_df, selected_rows_df], ignore_index=True)\n",
    "\n",
    "# Kembali ke dataset HuggingFace dari DataFrame\n",
    "new_train = Dataset.from_pandas(new_train_df)\n",
    "\n",
    "# Perbarui split train dan dev\n",
    "ds['train'] = new_train\n",
    "ds['dev'] = remaining_dev\n",
    "\n",
    "# Sekarang, finetuning_dataset['train'] sudah berisi 577 baris tambahan, dan finetuning_dataset['dev'] sudah dikurangi.\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eb6cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.rename_column(\"generated_answer\", \"model_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanpa kemampuan selektif\n",
    "!python train_summarizer.py \\\n",
    "  --dataset_name khalidrizki/RECOMP-finetuning-final \\\n",
    "  --text_column joined_passages \\\n",
    "  --query_column query \\\n",
    "  --summary_column summary \\\n",
    "  --model_name_or_path google/flan-t5-base \\\n",
    "  --seed 42 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --per_device_train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=8 \\\n",
    "  --per_device_eval_batch_size=16 \\\n",
    "  --learning_rate 1e-5 \\\n",
    "  --max_source_length 1620 \n",
    "  --resize_position_embeddings True\n",
    "  --max_target_length 52 \\\n",
    "  --output_dir ./models/ \\\n",
    "  --logging_first_step True \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --predict_with_generate \\\n",
    "  --save_total_limit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983a0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spiece.model: 100%|██████████| 792k/792k [00:01<00:00, 504kB/s] \n",
      "model.safetensors:  81%|████████  | 799M/990M [02:18<00:29, 6.38MB/s] "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_path = 'RECOMP/models/selective-epoch3-2025-06-24_23-46-23'\n",
    "RECOMP_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "RECOMP_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "repo = 'khalidrizki/RECOMP-selective-v2'\n",
    "RECOMP_tokenizer.push_to_hub(repo)\n",
    "RECOMP_model.push_to_hub(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb2394c",
   "metadata": {},
   "source": [
    "# CRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "849fe01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 838/838 [00:00<?, ?B/s] \n",
      "Downloading data: 100%|██████████| 15.1M/15.1M [00:02<00:00, 6.60MB/s]\n",
      "Downloading data: 100%|██████████| 1.56M/1.56M [00:00<00:00, 2.21MB/s]\n",
      "Downloading data: 100%|██████████| 1.52M/1.52M [00:00<00:00, 1.96MB/s]\n",
      "Generating train split: 100%|██████████| 5120/5120 [00:00<00:00, 75710.26 examples/s]\n",
      "Generating dev split: 100%|██████████| 565/565 [00:00<00:00, 28432.04 examples/s]\n",
      "Generating test split: 100%|██████████| 565/565 [00:00<00:00, 233338.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "crag_dataset = load_dataset('khalidrizki/postretrieve-raw-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d557967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5120/5120 [00:03<00:00, 1635.50 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerata jumlah elemen di 'context_chunks' split train: 4.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 1898.76 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerata jumlah elemen di 'context_chunks' split dev: 4.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 1795.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerata jumlah elemen di 'context_chunks' split test: 4.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def split_between_title_and_text(text: str) -> Tuple[str, str]:\n",
    "    \"\"\"Memisahkan bagian Judul dan Teks dari input lengkap.\"\"\"\n",
    "    title, content = text.split(\"|\", 1)\n",
    "    return title.strip(), content.strip()\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Hilangkan sitasi dan pecah teks menjadi kalimat-kalimat.\"\"\"\n",
    "    cleaned_text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    return [s.strip() for s in sent_tokenize(cleaned_text) if s.strip()]\n",
    "\n",
    "def create_rolling_segments(title: str, sentences: List[str], window_size: int = 3, stride: int = 2) -> List[str]:\n",
    "    \"\"\"Buat rolling window segment dari kalimat-kalimat dengan format 'Title | Kalimat…'.\"\"\"\n",
    "    segments = []\n",
    "    if len(sentences) < window_size:\n",
    "        segments.append(f\"{title} | {' '.join(sentences)}\")\n",
    "    else:\n",
    "        for i in range(0, len(sentences) - window_size + 1, stride):\n",
    "            group = sentences[i:i + window_size]\n",
    "            combined = f\"{title} | {' '.join(group)}\"\n",
    "            segments.append(combined)\n",
    "    return segments\n",
    "\n",
    "def prepare_context_chunks(text: str, is_positive: bool) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Proses satu passage menjadi rolling segments dan labelnya.\"\"\"\n",
    "    title, content = split_between_title_and_text(text)\n",
    "    sentences = split_sentences(content)\n",
    "    segments = create_rolling_segments(title, sentences, window_size=3)\n",
    "    return [{\"text\": seg, \"is_positive\": is_positive} for seg in segments]\n",
    "\n",
    "def split_each_passages(example):\n",
    "    ranked_passages = example[\"ranked_truncPassages_with_labels\"]\n",
    "    all_chunks = []\n",
    "    all_labels = []\n",
    "\n",
    "    for psg in ranked_passages:\n",
    "        chunks = prepare_context_chunks(psg[\"text\"], psg[\"is_positive\"])\n",
    "        for c in chunks:\n",
    "            all_chunks.append(c[\"text\"])\n",
    "            all_labels.append(c[\"is_positive\"])\n",
    "\n",
    "    return {\n",
    "        \"context_chunks\": all_chunks,\n",
    "        \"chunk_labels\": all_labels\n",
    "    }\n",
    "\n",
    "for split in crag_dataset.keys():\n",
    "    crag_dataset[split] = crag_dataset[split].map(split_each_passages)\n",
    "    total_chunks = sum(len(row[\"context_chunks\"]) for row in crag_dataset[split])\n",
    "    average_chunks = total_chunks / len(crag_dataset[split])\n",
    "    print(f\"Rerata jumlah elemen di 'context_chunks' split {split}: {average_chunks:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba402d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ranked_chunks_with_labels: 100%|██████████| 5120/5120 [13:13<00:00,  6.46it/s]\n",
      "Processing ranked_chunks_with_labels: 100%|██████████| 565/565 [01:24<00:00,  6.71it/s]\n",
      "Processing ranked_chunks_with_labels: 100%|██████████| 565/565 [01:23<00:00,  6.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import DatasetDict\n",
    "from general_utils import apply_similarity_ranking_to_dataset\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "embedding_model = AutoModel.from_pretrained('intfloat/multilingual-e5-small')\n",
    "embedding_model.eval()\n",
    "\n",
    "for split in crag_dataset.keys():\n",
    "    crag_dataset[split] = apply_similarity_ranking_to_dataset(\n",
    "        crag_dataset[split], \n",
    "        text_col=\"context_chunks\", \n",
    "        label_col=\"chunk_labels\", \n",
    "        output_col=\"ranked_chunks_with_labels\", \n",
    "        tokenizer=embedding_tokenizer,\n",
    "        model=embedding_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25f1df8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 18.58ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.85s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 24.83ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 28.33ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.83s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/khalidrizki/CRAG-3sentences-chunks/commit/d3970033c61ef586434db7037c81a274eb0cffcb', commit_message='Upload dataset', commit_description='', oid='d3970033c61ef586434db7037c81a274eb0cffcb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/khalidrizki/CRAG-3sentences-chunks', endpoint='https://huggingface.co', repo_type='dataset', repo_id='khalidrizki/CRAG-3sentences-chunks'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crag_dataset.push_to_hub('khalidrizki/CRAG-3sentences-chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbffc3b",
   "metadata": {},
   "source": [
    "## Membuat Jawaban\n",
    "dengan variasi chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1832937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 10.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from general_utils import load_model_and_tokenizer\n",
    "model_name='Qwen/Qwen3-1.7B'\n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fec27051",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('khalidrizki/CRAG-3sentences-chunks')\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442c43b",
   "metadata": {},
   "source": [
    "### Jumlah chunks = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a560ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 2571.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from general_utils import extract_topk_texts\n",
    "from functools import partial\n",
    "k = 1\n",
    "extract_fn = partial(\n",
    "    extract_topk_texts, \n",
    "    k=k, \n",
    "    ranked_units='ranked_chunks_with_labels', \n",
    "    returned_units_col=\"top1_chunks\",\n",
    "    returned_labels_col=\"top1_labels\"\n",
    ")\n",
    "test_dataset = test_dataset.map(extract_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79041fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answer using the 1 chunk with most similarity:   0%|          | 0/565 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating answer using the 1 chunk with most similarity: 100%|██████████| 565/565 [26:20<00:00,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL CRAG dengan Top 1\n",
      "rerata substring match: 0.4690265486725664\n",
      "rerata F1: 0.3026155217667255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from general_utils import generate_per_row\n",
    "from metrics import evaluate_substringmatch_f1\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "top1_generations = []\n",
    "instruction = \"Konteks: {context}\\nBerdasarkan konteks sebelumnya, jawab pertanyaan berikut dalam satu kalimat. Pertanyaan: {query}\"\n",
    "\n",
    "for row in tqdm(test_dataset, desc=\"Generating answer using the 1 chunk with most similarity\"):\n",
    "    label = row['answer']\n",
    "    generated_answer = generate_per_row(\n",
    "        row=row, \n",
    "        query_col='query', \n",
    "        ctx_col='top1_chunks', \n",
    "        tokenizer=tokenizer, \n",
    "        model=model, \n",
    "        device_type=config.device_type, \n",
    "        instruction=instruction\n",
    "    )\n",
    "    sm, f1 = evaluate_substringmatch_f1(generated_answer.strip(), label.strip())\n",
    "    top1_generations.append({\n",
    "        'query': row['query'], \n",
    "        'passages': row['ranked_chunks_with_labels'], \n",
    "        'filtered_chunks': row['top1_chunks'], \n",
    "        'label': label, \n",
    "        'generated_answer': generated_answer, \n",
    "        'em': sm, \n",
    "        'f1': f1\n",
    "        }\n",
    "    )\n",
    "\n",
    "CRAG_top1 = Dataset.from_list(top1_generations)\n",
    "print(\"HASIL CRAG dengan Top 1\")\n",
    "print(\"rerata substring match:\", sum(CRAG_top1['em'])/len(CRAG_top1))\n",
    "print(\"rerata F1:\", sum(CRAG_top1['f1'])/len(CRAG_top1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "546e6133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 565/565 [00:00<00:00, 38930.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "CRAG_top1.save_to_disk('./outputs/CRAG/top1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ac599",
   "metadata": {},
   "source": [
    "### Jumlah chunks = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fce7b498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 2567.49 examples/s]\n",
      "Generating answer using the 2 chunks with most similarity:   0%|          | 0/565 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating answer using the 2 chunks with most similarity: 100%|██████████| 565/565 [27:49<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL CRAG dengan Top 2\n",
      "rerata substring match: 0.5469026548672566\n",
      "rerata F1: 0.33058291391801087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from general_utils import extract_topk_texts\n",
    "from functools import partial\n",
    "k = 2\n",
    "extract2_fn = partial(\n",
    "    extract_topk_texts, \n",
    "    k=k, \n",
    "    ranked_units='ranked_chunks_with_labels', \n",
    "    returned_units_col=\"top2_chunks\",\n",
    "    returned_labels_col=\"top2_labels\"\n",
    ")\n",
    "test_dataset = test_dataset.map(extract2_fn)\n",
    "\n",
    "from general_utils import generate_per_row\n",
    "from metrics import evaluate_substringmatch_f1\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "top2_generations = []\n",
    "instruction = \"Konteks: {context}\\nBerdasarkan konteks sebelumnya, jawab pertanyaan berikut dalam satu kalimat. Pertanyaan: {query}\"\n",
    "\n",
    "for row in tqdm(test_dataset, desc=\"Generating answer using the 2 chunks with most similarity\"):\n",
    "    label = row['answer']\n",
    "    generated_answer = generate_per_row(\n",
    "        row=row, \n",
    "        query_col='query', \n",
    "        ctx_col='top2_chunks', \n",
    "        tokenizer=tokenizer, \n",
    "        model=model, \n",
    "        device_type=config.device_type, \n",
    "        instruction=instruction\n",
    "    )\n",
    "    sm, f1 = evaluate_substringmatch_f1(generated_answer.strip(), label.strip())\n",
    "    top2_generations.append({\n",
    "        'query': row['query'], \n",
    "        'passages': row['ranked_chunks_with_labels'], \n",
    "        'filtered_chunks': row['top2_chunks'], \n",
    "        'label': label, \n",
    "        'generated_answer': generated_answer, \n",
    "        'em': sm, \n",
    "        'f1': f1\n",
    "        }\n",
    "    )\n",
    "\n",
    "CRAG_top2 = Dataset.from_list(top2_generations)\n",
    "print(\"HASIL CRAG dengan Top 2\")\n",
    "print(\"rerata substring match:\", sum(CRAG_top2['em'])/len(CRAG_top2))\n",
    "print(\"rerata F1:\", sum(CRAG_top2['f1'])/len(CRAG_top2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a867fbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 565/565 [00:00<00:00, 35326.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "CRAG_top2.save_to_disk('./outputs/CRAG/top2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064d97c",
   "metadata": {},
   "source": [
    "### Jumlah chunks = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfba75ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 3272.12 examples/s]\n",
      "Generating answer using the 3 chunks with most similarity:   0%|          | 0/565 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating answer using the 3 chunks with most similarity: 100%|██████████| 565/565 [26:16<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL CRAG dengan Top 3\n",
      "rerata substring match: 0.584070796460177\n",
      "rerata F1: 0.35286040653377126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from general_utils import extract_topk_texts\n",
    "from functools import partial\n",
    "k = 3\n",
    "extract3_fn = partial(\n",
    "    extract_topk_texts, \n",
    "    k=k, \n",
    "    ranked_units='ranked_chunks_with_labels', \n",
    "    returned_units_col=\"top3_chunks\",\n",
    "    returned_labels_col=\"top3_labels\"\n",
    ")\n",
    "test_dataset = test_dataset.map(extract3_fn)\n",
    "\n",
    "from general_utils import generate_per_row\n",
    "from metrics import evaluate_substringmatch_f1\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "top3_generations = []\n",
    "instruction = \"Konteks: {context}\\nBerdasarkan konteks sebelumnya, jawab pertanyaan berikut dalam satu kalimat. Pertanyaan: {query}\"\n",
    "\n",
    "for row in tqdm(test_dataset, desc=\"Generating answer using the 3 chunks with most similarity\"):\n",
    "    label = row['answer']\n",
    "    generated_answer = generate_per_row(\n",
    "        row=row, \n",
    "        query_col='query', \n",
    "        ctx_col='top3_chunks', \n",
    "        tokenizer=tokenizer, \n",
    "        model=model, \n",
    "        device_type=config.device_type, \n",
    "        instruction=instruction\n",
    "    )\n",
    "    sm, f1 = evaluate_substringmatch_f1(generated_answer.strip(), label.strip())\n",
    "    top3_generations.append({\n",
    "        'query': row['query'], \n",
    "        'passages': row['ranked_chunks_with_labels'], \n",
    "        'filtered_chunks': row['top3_chunks'], \n",
    "        'label': label, \n",
    "        'generated_answer': generated_answer, \n",
    "        'em': sm, \n",
    "        'f1': f1\n",
    "        }\n",
    "    )\n",
    "\n",
    "CRAG_top3 = Dataset.from_list(top3_generations)\n",
    "print(\"HASIL CRAG dengan Top 3\")\n",
    "print(\"rerata substring match:\", sum(CRAG_top3['em'])/len(CRAG_top3))\n",
    "print(\"rerata F1:\", sum(CRAG_top3['f1'])/len(CRAG_top3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6584140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 565/565 [00:00<00:00, 86907.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "CRAG_top3.save_to_disk('./outputs/CRAG/top3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2270b442",
   "metadata": {},
   "source": [
    "### RAG Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33875e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 2399.06 examples/s]\n",
      "Generating answer using the original passages (not splitted into chunks): 100%|██████████| 565/565 [29:42<00:00,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL RAG normal\n",
      "rerata substring match: 0.6035398230088496\n",
      "rerata F1: 0.35978795140582226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from general_utils import extract_topk_texts\n",
    "from functools import partial\n",
    "k = 3\n",
    "extract_all_fn = partial(\n",
    "    extract_topk_texts, \n",
    "    k=k, \n",
    "    ranked_units='ranked_truncPassages_with_labels', \n",
    "    returned_units_col=\"sorted_passages\",\n",
    "    returned_labels_col=\"sortedPassages_labels\"\n",
    ")\n",
    "test_dataset = test_dataset.map(extract_all_fn)\n",
    "\n",
    "from general_utils import generate_per_row\n",
    "from metrics import evaluate_substringmatch_f1\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "ori_passages_generations = []\n",
    "instruction = \"Konteks: {context}\\nBerdasarkan konteks sebelumnya, jawab pertanyaan berikut dalam satu kalimat. Pertanyaan: {query}\"\n",
    "\n",
    "for row in tqdm(test_dataset, desc=\"Generating answer using the original passages (not splitted into chunks)\"):\n",
    "    label = row['answer']\n",
    "    generated_answer = generate_per_row(\n",
    "        row=row, \n",
    "        query_col='query', \n",
    "        ctx_col='sorted_passages', \n",
    "        tokenizer=tokenizer, \n",
    "        model=model, \n",
    "        device_type=config.device_type, \n",
    "        instruction=instruction\n",
    "    )\n",
    "    sm, f1 = evaluate_substringmatch_f1(generated_answer.strip(), label.strip())\n",
    "    ori_passages_generations.append({\n",
    "        'query': row['query'], \n",
    "        'passages': row['ranked_truncPassages_with_labels'], \n",
    "        'filtered_chunks': row['sorted_passages'], \n",
    "        'label': label, \n",
    "        'generated_answer': generated_answer, \n",
    "        'em': sm, \n",
    "        'f1': f1\n",
    "        }\n",
    "    )\n",
    "\n",
    "normal_RAG = Dataset.from_list(ori_passages_generations)\n",
    "print(\"HASIL RAG normal\")\n",
    "print(\"rerata substring match:\", sum(normal_RAG['em'])/len(normal_RAG))\n",
    "print(\"rerata F1:\", sum(normal_RAG['f1'])/len(normal_RAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee8d01ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 565/565 [00:00<00:00, 75441.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "normal_RAG.save_to_disk('./outputs/CRAG/normal-rag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c8f16",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cf16361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'label', 'model_outputs'],\n",
       "        num_rows: 5120\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'label', 'model_outputs'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'label', 'model_outputs'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('khalidrizki/RECOMP-finetuning-final')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ae1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
