{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7dde62",
   "metadata": {},
   "source": [
    "# Load Model, Tokenizer, dan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da683428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'label', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels', 'sorted_truncPassages'],\n",
       "        num_rows: 5120\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'label', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels', 'sorted_truncPassages'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'label', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels', 'sorted_truncPassages'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "dataset = load_dataset(\"khalidrizki/postretrieve-raw-dataset-v2\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fdab23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels'],\n",
       "        num_rows: 5120\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Membuat salinan dataset\n",
    "ds = deepcopy(dataset)\n",
    "\n",
    "# Tentukan jumlah baris yang ingin dipindahkan\n",
    "num_rows_to_move = 578\n",
    "\n",
    "# Pilih 577 baris acak dari split 'dev'\n",
    "dev_dataset = ds['dev']\n",
    "\n",
    "selected_rows = dev_dataset.select(range(num_rows_to_move))  # Ambil 577 baris pertama setelah shuffle\n",
    "\n",
    "# Hapus 577 baris yang sudah dipilih dari 'dev'\n",
    "remaining_dev = dev_dataset.select(range(num_rows_to_move, len(dev_dataset)))\n",
    "\n",
    "# Konversi ke DataFrame pandas untuk dapat menggunakan concat\n",
    "train_df = ds['train'].to_pandas()\n",
    "selected_rows_df = selected_rows.to_pandas()\n",
    "\n",
    "# Gabungkan keduanya dengan pandas.concat\n",
    "new_train_df = pd.concat([train_df, selected_rows_df], ignore_index=True)\n",
    "\n",
    "# Kembali ke dataset HuggingFace dari DataFrame\n",
    "new_train = Dataset.from_pandas(new_train_df)\n",
    "\n",
    "# Perbarui split train dan dev\n",
    "ds['train'] = new_train\n",
    "ds['dev'] = remaining_dev\n",
    "\n",
    "# Sekarang, finetuning_dataset['train'] sudah berisi 577 baris tambahan, dan finetuning_dataset['dev'] sudah dikurangi.\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88eaa5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 32.70ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.30s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 31.94ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 43.60ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/khalidrizki/postretrieve-raw-dataset/commit/79184ccb3d078a7810a0ad1282ffd266a9a64719', commit_message='Upload dataset', commit_description='', oid='79184ccb3d078a7810a0ad1282ffd266a9a64719', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/khalidrizki/postretrieve-raw-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='khalidrizki/postretrieve-raw-dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub('khalidrizki/postretrieve-raw-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5167a9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERHASIL MELOAD MODEL DAN DATASET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from general_utils import load_model_and_tokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\" \n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)\n",
    "\n",
    "print(\"BERHASIL MELOAD MODEL DAN DATASET\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1918d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels'],\n",
       "        num_rows: 4542\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels'],\n",
       "        num_rows: 1143\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query_id', 'query', 'tydiqa_id', 'answer', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for split in dataset.keys():\n",
    "#     dataset[split] = dataset[split].select(range(5))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0d32e",
   "metadata": {},
   "source": [
    "# RECOMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee009c",
   "metadata": {},
   "source": [
    "## Membuat dataset finetuning untuk melatih kompresor RECOMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77bc8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "recomp_draft_dataset = deepcopy(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9679a",
   "metadata": {},
   "source": [
    "Mengekstrak text & is_positive dari ranked passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185ca2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from general_utils import extract_topk_texts\n",
    "extract_fn = partial(\n",
    "    extract_topk_texts, \n",
    "    ranked_units='ranked_truncPassages_with_labels', \n",
    "    returned_units_col='joined_passages', \n",
    "    returned_labels_col='passages_label'\n",
    ")\n",
    "for split in recomp_draft_dataset.keys():\n",
    "    recomp_draft_dataset[split] = recomp_draft_dataset[split].map(extract_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_utils import generate_per_row\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "instruction = '{context}\\n\\nRingkaslah teks di atas menjadi tepat 2 kalimat (40 kata) agar menjawab pertanyaan secara mendetail. TANPA PENGANTAR. Pertanyaan: \"{query}' \n",
    "for split in recomp_draft_dataset.keys():\n",
    "    current_dataset = recomp_draft_dataset[split]\n",
    "    decoded_outputs = []\n",
    "    summaries = []\n",
    "\n",
    "    for row in tqdm(current_dataset, desc=f\"Summarizing split: {split}\"):\n",
    "        try:\n",
    "            answer = generate_per_row(  # decoded_output, \n",
    "                row, \n",
    "                'query', \n",
    "                'joined_passages', \n",
    "                tokenizer, \n",
    "                model, \n",
    "                config.device_type, \n",
    "                instruction\n",
    "            )\n",
    "            # decoded_outputs.append(decoded_output)\n",
    "            summaries.append(answer)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Error during summarization for row:\\n\"\n",
    "                f\"Query: {row['query']}\\n\"\n",
    "                f\"Context: {row['joined_passages'][:200]}...\\n\"\n",
    "                f\"Error message: {str(e)}\"\n",
    "            )\n",
    "\n",
    "    # dataset[split] = current_dataset.add_column(\"decoded_output\", decoded_outputs)\n",
    "    recomp_draft_dataset[split] = recomp_draft_dataset[split].add_column(\"summary\", summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b91c1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4542/4542 [00:00<00:00, 87498.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1143/1143 [00:00<00:00, 65990.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 565/565 [00:00<00:00, 38314.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "recomp_draft_dataset.save_to_disk('generated_data/RECOMP_draft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "750e4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "recomp_draft_dataset = load_from_disk('generated_data/RECOMP_draft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f12de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses (w/ & wo/ summary) on split: train: 100%|██████████| 4542/4542 [5:54:29<00:00,  4.68s/it]  \n",
      "Generating responses (w/ & wo/ summary) on split: dev: 100%|██████████| 1143/1143 [1:30:02<00:00,  4.73s/it]\n",
      "Generating responses (w/ & wo/ summary) on split: test: 100%|██████████| 565/565 [45:39<00:00,  4.85s/it] \n"
     ]
    }
   ],
   "source": [
    "from metrics import evaluate_substringmatch_f1\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "\n",
    "finetuning_dict = {}\n",
    "\n",
    "instruction_w_summary = \"Konteks: {context}\\nBerdasarkan konteks sebelumnya, jawab pertanyaan berikut dalam dua kalimat. Pertanyaan: {query}\"\n",
    "instruction_wo_summary= \"Jawab pertanyaan berikut dalam dua kalimat. Pertanyaan: {query}\"\n",
    "for split in recomp_draft_dataset.keys():\n",
    "    current_dataset = recomp_draft_dataset[split]\n",
    "    results = []\n",
    "    for row in tqdm(current_dataset, desc=f\"Generating responses (w/ & wo/ summary) on split: {split}\"):\n",
    "        label = row['answer']\n",
    "        completion_w_summary = generate_per_row( # decoded_summ_output, \n",
    "            row=row, \n",
    "            query_col='query', \n",
    "            ctx_col='summary', \n",
    "            tokenizer=tokenizer, \n",
    "            model=model, \n",
    "            device_type=config.device_type, \n",
    "            instruction=instruction_w_summary\n",
    "        )\n",
    "        completion_wo_summary = generate_per_row(  #decoded_wo_summ_output, \n",
    "            row=row, \n",
    "            query_col='query', \n",
    "            ctx_col=None, \n",
    "            tokenizer=tokenizer, \n",
    "            model=model, \n",
    "            device_type=config.device_type, \n",
    "            instruction=instruction_wo_summary\n",
    "        )\n",
    "\n",
    "        sm_w, f1_w = evaluate_substringmatch_f1(completion_w_summary.strip(), label.strip())\n",
    "        sm_wo, f1_wo=evaluate_substringmatch_f1(completion_wo_summary.strip(), label.strip())\n",
    "\n",
    "        if (sm_wo == 1 and sm_wo > sm_w) or (f1_wo >= f1_w):\n",
    "            final_summary = \"\"\n",
    "        else:\n",
    "            final_summary = row['summary']\n",
    "        \n",
    "        results.append(\n",
    "            {\n",
    "                \"query\": row['query'], \n",
    "                \"passages\":row['ranked_truncPassages_with_labels'], \n",
    "                \"summary\": row['summary'], \n",
    "                \"final_summary\": final_summary, \n",
    "                \"label\": label, \n",
    "                \"model_outputs\": {\n",
    "                    \"w_summary\": {\n",
    "                        \"completion\": completion_w_summary,\n",
    "                        \"em\": sm_w,\n",
    "                        \"f1\": f1_w, \n",
    "                        # \"decoded_output\": decoded_summ_output\n",
    "                    },\n",
    "                    \"wo_summary\": {\n",
    "                        \"completion\": completion_wo_summary,\n",
    "                        \"em\": sm_wo,\n",
    "                        \"f1\": f1_wo, \n",
    "                        # \"decoded_output\": decoded_wo_summ_output\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    finetuning_dict[split] = Dataset.from_list(results)\n",
    "finetuning_dataset = DatasetDict(finetuning_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f244234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'label', 'generated_answer'],\n",
       "        num_rows: 5120\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'label', 'generated_answer'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'label', 'generated_answer'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "\n",
    "# Membuat salinan dataset\n",
    "ds = deepcopy(finetuning_dataset)\n",
    "\n",
    "# Tentukan jumlah baris yang ingin dipindahkan\n",
    "num_rows_to_move = 578\n",
    "\n",
    "# Pilih 577 baris acak dari split 'dev'\n",
    "dev_dataset = ds['dev']\n",
    "\n",
    "selected_rows = dev_dataset.select(range(num_rows_to_move))  # Ambil 577 baris pertama setelah shuffle\n",
    "\n",
    "# Hapus 577 baris yang sudah dipilih dari 'dev'\n",
    "remaining_dev = dev_dataset.select(range(num_rows_to_move, len(dev_dataset)))\n",
    "\n",
    "# Konversi ke DataFrame pandas untuk dapat menggunakan concat\n",
    "train_df = ds['train'].to_pandas()\n",
    "selected_rows_df = selected_rows.to_pandas()\n",
    "\n",
    "# Gabungkan keduanya dengan pandas.concat\n",
    "new_train_df = pd.concat([train_df, selected_rows_df], ignore_index=True)\n",
    "\n",
    "# Kembali ke dataset HuggingFace dari DataFrame\n",
    "new_train = Dataset.from_pandas(new_train_df)\n",
    "\n",
    "# Perbarui split train dan dev\n",
    "ds['train'] = new_train\n",
    "ds['dev'] = remaining_dev\n",
    "\n",
    "# Sekarang, finetuning_dataset['train'] sudah berisi 577 baris tambahan, dan finetuning_dataset['dev'] sudah dikurangi.\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32fe4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.rename_column(\"generated_answer\", \"model_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec3c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from general_utils import extract_topk_texts\n",
    "from datasets import load_dataset\n",
    "\n",
    "recomp_draft_dataset = load_dataset(\"khalidrizki/postretrieve-raw-dataset-v2\")\n",
    "\n",
    "extract_fn = partial(\n",
    "    extract_topk_texts, \n",
    "    ranked_units='ranked_truncPassages_with_labels', \n",
    "    returned_units_col='joined_passages', \n",
    "    returned_labels_col='passages_label'\n",
    ")\n",
    "for split in recomp_draft_dataset.keys():\n",
    "    recomp_draft_dataset[split] = recomp_draft_dataset[split].map(extract_fn)\n",
    "    \n",
    "dataset = load_dataset('khalidrizki/RECOMP-finetuning-final')\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Buat dictionary baru dengan split yang sudah dimodifikasi\n",
    "new_dataset = DatasetDict({\n",
    "    \"train\": dataset[\"train\"].add_column(\"joined_passages\", recomp_draft_dataset[\"train\"][\"joined_passages\"]),\n",
    "    \"dev\": dataset[\"dev\"].add_column(\"joined_passages\", recomp_draft_dataset[\"dev\"][\"joined_passages\"]),\n",
    "    \"test\": dataset[\"test\"].add_column(\"joined_passages\", recomp_draft_dataset[\"test\"][\"joined_passages\"]),\n",
    "})\n",
    "\n",
    "repo_ds = \"khalidrizki/RECOMP-finetuning-final-fix\"\n",
    "new_dataset.push_to_hub(repo_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27de35c",
   "metadata": {},
   "source": [
    "## Latihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b588ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanpa kemampuan selektif\n",
    "!python train_summarizer.py \\\n",
    "  --dataset_name khalidrizki/RECOMP-finetuning-final \\\n",
    "  --text_column joined_passages \\\n",
    "  --query_column query \\\n",
    "  --summary_column summary \\\n",
    "  --model_name_or_path google/flan-t5-base \\\n",
    "  --seed 42 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --per_device_train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=8 \\\n",
    "  --per_device_eval_batch_size=16 \\\n",
    "  --learning_rate 1e-5 \\\n",
    "  --max_source_length 1620 \n",
    "  --resize_position_embeddings True\n",
    "  --max_target_length 52 \\\n",
    "  --output_dir ./models/ \\\n",
    "  --logging_first_step True \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --predict_with_generate \\\n",
    "  --save_total_limit 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0e842",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a50610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 14.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "dataset = load_dataset(\"khalidrizki/RECOMP-finetuning-final-fix\")\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "from general_utils import load_model_and_tokenizer\n",
    "model_name='Qwen/Qwen3-1.7B'\n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da97e3",
   "metadata": {},
   "source": [
    "### Selective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35a35923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "test_data_selective = deepcopy(test_data)\n",
    "test_data_selective = test_data_selective.rename_column('summary', 'summary_generated_by_qwen_during_training')\n",
    "test_data_selective = test_data_selective.rename_column('final_summary', 'selected_summary_generated_by_qwen_during_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f90f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muat baris-baris teks dari file\n",
    "with open(\"./RECOMP/outputs/RECOMP-selective-fix-2025-06-27_15-12-35/generated_predictions.txt\", \"r\", encoding=\"utf-8\", errors='replace') as f:\n",
    "    predictions = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Menambahkan try-except di bagian assert\n",
    "try:\n",
    "    # Pastikan jumlah prediksi sama dengan jumlah data\n",
    "    assert len(predictions) == len(test_data_selective), f\"Jumlah prediksi ({len(predictions)}) tidak sama dengan jumlah data ({len(test_data_selective)})\"\n",
    "\n",
    "except AssertionError:\n",
    "    # Jika terjadi error, tambahkan string kosong pada predictions\n",
    "    while len(predictions) < len(test_data_selective):\n",
    "        predictions.append(\"\")  # Menambahkan string kosong ke predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2b0f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_summary_col = \"selective_summary_for_testing\"\n",
    "test_data_selective = test_data_selective.add_column(gen_summary_col, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f08fbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TESTING: Generating responses using summary from selective model...:   0%|          | 0/565 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "TESTING: Generating responses using summary from selective model...: 100%|██████████| 565/565 [24:49<00:00,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from metrics import evaluate_substringmatch_f1\n",
    "from general_utils import generate_per_row\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "\n",
    "results = []\n",
    "for row in tqdm(test_data_selective, desc=f\"TESTING: Generating responses using summary from selective model...\"):\n",
    "    label = row['label']\n",
    "    instruction = ''\n",
    "    if test_data_selective[gen_summary_col] == '':\n",
    "        instruction = \"Jawab pertanyaan berikut dalam satu kalimat. Pertanyaan: {query}\"\n",
    "    else:\n",
    "        instruction = \"Konteks: {context}\\nBerdasarkan konteks sebelumnya, jawab pertanyaan berikut dalam satu kalimat. Pertanyaan: {query}\"\n",
    "\n",
    "    completion = generate_per_row( # decoded_summ_output, \n",
    "        row=row, \n",
    "        query_col='query', \n",
    "        ctx_col=gen_summary_col, \n",
    "        tokenizer=tokenizer, \n",
    "        model=model, \n",
    "        device_type=config.device_type, \n",
    "        instruction=instruction\n",
    "    )\n",
    "\n",
    "    sm, f1 = evaluate_substringmatch_f1(completion.strip(), label.strip())\n",
    "    \n",
    "    results.append(\n",
    "        {\n",
    "            \"query\": row['query'], \n",
    "            \"passages\":row['joined_passages'], \n",
    "            \"summary_used_for_ctx\": row[gen_summary_col], \n",
    "            \"label\": label, \n",
    "            \"completion\": completion,\n",
    "            \"em\": sm,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ee346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07f8e645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results telah disimpan di ./outputs/RECOMP\\SELECTIVE_test_results.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Tentukan path file output JSON\n",
    "output_dir = \"./outputs/RECOMP\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Membuat direktori jika belum ada\n",
    "output_file = os.path.join(output_dir, \"SELECTIVE_test_results.json\")\n",
    "\n",
    "# Simpan results ke dalam file JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Results telah disimpan di {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "339fd7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL RECOMP dengan kemampuan selektif\n",
      "rerata substring match: 0.11327433628318584\n",
      "rerata F1: 0.11271414083351855\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "selective_recomp = Dataset.from_list(results)\n",
    "print(\"HASIL RECOMP dengan kemampuan selektif\")\n",
    "print(\"rerata substring match:\", sum(selective_recomp['em'])/len(selective_recomp))\n",
    "print(\"rerata F1:\", sum(selective_recomp['f1'])/len(selective_recomp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93433209",
   "metadata": {},
   "source": [
    "### Unselective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73cf187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "test_data_unselective = deepcopy(test_data)\n",
    "test_data_unselective = test_data_unselective.rename_column('summary', 'summary_generated_by_qwen_during_training')\n",
    "test_data_unselective.remove_columns('final_summary')\n",
    "# test_data_unselective = test_data_unselective.rename_column('final_summary', 'selected_summary_generated_by_qwen_during_training')\n",
    "\n",
    "# Muat baris-baris teks dari file\n",
    "with open(\"./RECOMP/outputs/RECOMP-unselective-fix-2025-06-27_15-17-40/generated_predictions.txt\", \"r\", encoding=\"utf-8\", errors='replace') as f:\n",
    "    predictions = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Menambahkan try-except di bagian assert\n",
    "try:\n",
    "    # Pastikan jumlah prediksi sama dengan jumlah data\n",
    "    assert len(predictions) == len(test_data_unselective), f\"Jumlah prediksi ({len(predictions)}) tidak sama dengan jumlah data ({len(test_data_unselective)})\"\n",
    "\n",
    "except AssertionError:\n",
    "    # Jika terjadi error, tambahkan string kosong pada predictions\n",
    "    while len(predictions) < len(test_data_unselective):\n",
    "        predictions.append(\"\")  # Menambahkan string kosong ke predictions\n",
    "\n",
    "\n",
    "gen_summary_col = \"unselective_summary_for_testing\"\n",
    "test_data_unselective = test_data_unselective.add_column(gen_summary_col, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66670079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TESTING: Generating responses using summary from unselective model...:   0%|          | 0/565 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "TESTING: Generating responses using summary from unselective model...: 100%|██████████| 565/565 [22:58<00:00,  2.44s/it]\n"
     ]
    }
   ],
   "source": [
    "from metrics import evaluate_substringmatch_f1\n",
    "from general_utils import generate_per_row\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "\n",
    "results_un = []\n",
    "for row in tqdm(test_data_unselective, desc=f\"TESTING: Generating responses using summary from unselective model...\"):\n",
    "    label = row['label']\n",
    "    instruction = \"Konteks: {context}\\nBerdasarkan konteks sebelumnya, jawab pertanyaan berikut dalam satu kalimat. Pertanyaan: {query}\"\n",
    "\n",
    "    completion = generate_per_row( # decoded_summ_output, \n",
    "        row=row, \n",
    "        query_col='query', \n",
    "        ctx_col=gen_summary_col, \n",
    "        tokenizer=tokenizer, \n",
    "        model=model, \n",
    "        device_type=config.device_type, \n",
    "        instruction=instruction\n",
    "    )\n",
    "\n",
    "    sm, f1 = evaluate_substringmatch_f1(completion.strip(), label.strip())\n",
    "    \n",
    "    results_un.append(\n",
    "        {\n",
    "            \"query\": row['query'], \n",
    "            \"passages\":row['joined_passages'], \n",
    "            \"summary_used_for_ctx\": row[gen_summary_col], \n",
    "            \"label\": label, \n",
    "            \"completion\": completion,\n",
    "            \"em\": sm,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "415aba78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL RECOMP tanpa kemampuan selektif\n",
      "rerata substring match: 0.2743362831858407\n",
      "rerata F1: 0.220610480602904\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "unselective_recomp = Dataset.from_list(results_un)\n",
    "print(\"HASIL RECOMP tanpa kemampuan selektif\")\n",
    "print(\"rerata substring match:\", sum(unselective_recomp['em'])/len(unselective_recomp))\n",
    "print(\"rerata F1:\", sum(unselective_recomp['f1'])/len(unselective_recomp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caafc445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results telah disimpan di ./outputs/RECOMP\\UNSelective_test_results.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Tentukan path file output JSON\n",
    "output_dir = \"./outputs/RECOMP\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Membuat direktori jika belum ada\n",
    "output_file = os.path.join(output_dir, \"UNSelective_test_results.json\")\n",
    "\n",
    "# Simpan results_un ke dalam file JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_un, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Results telah disimpan di {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc9c42",
   "metadata": {},
   "source": [
    "# CRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa1d7cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 838/838 [00:00<?, ?B/s] \n",
      "Downloading data: 100%|██████████| 15.1M/15.1M [00:02<00:00, 6.60MB/s]\n",
      "Downloading data: 100%|██████████| 1.56M/1.56M [00:00<00:00, 2.21MB/s]\n",
      "Downloading data: 100%|██████████| 1.52M/1.52M [00:00<00:00, 1.96MB/s]\n",
      "Generating train split: 100%|██████████| 5120/5120 [00:00<00:00, 75710.26 examples/s]\n",
      "Generating dev split: 100%|██████████| 565/565 [00:00<00:00, 28432.04 examples/s]\n",
      "Generating test split: 100%|██████████| 565/565 [00:00<00:00, 233338.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "crag_dataset = load_dataset('khalidrizki/postretrieve-raw-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc77598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5120/5120 [00:03<00:00, 1635.50 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerata jumlah elemen di 'context_chunks' split train: 4.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 1898.76 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerata jumlah elemen di 'context_chunks' split dev: 4.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 1795.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerata jumlah elemen di 'context_chunks' split test: 4.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def split_between_title_and_text(text: str) -> Tuple[str, str]:\n",
    "    \"\"\"Memisahkan bagian Judul dan Teks dari input lengkap.\"\"\"\n",
    "    title, content = text.split(\"|\", 1)\n",
    "    return title.strip(), content.strip()\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Hilangkan sitasi dan pecah teks menjadi kalimat-kalimat.\"\"\"\n",
    "    cleaned_text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    return [s.strip() for s in sent_tokenize(cleaned_text) if s.strip()]\n",
    "\n",
    "def create_rolling_segments(title: str, sentences: List[str], window_size: int = 3, stride: int = 2) -> List[str]:\n",
    "    \"\"\"Buat rolling window segment dari kalimat-kalimat dengan format 'Title | Kalimat…'.\"\"\"\n",
    "    segments = []\n",
    "    if len(sentences) < window_size:\n",
    "        segments.append(f\"{title} | {' '.join(sentences)}\")\n",
    "    else:\n",
    "        for i in range(0, len(sentences) - window_size + 1, stride):\n",
    "            group = sentences[i:i + window_size]\n",
    "            combined = f\"{title} | {' '.join(group)}\"\n",
    "            segments.append(combined)\n",
    "    return segments\n",
    "\n",
    "def prepare_context_chunks(text: str, is_positive: bool) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Proses satu passage menjadi rolling segments dan labelnya.\"\"\"\n",
    "    title, content = split_between_title_and_text(text)\n",
    "    sentences = split_sentences(content)\n",
    "    segments = create_rolling_segments(title, sentences, window_size=3)\n",
    "    return [{\"text\": seg, \"is_positive\": is_positive} for seg in segments]\n",
    "\n",
    "def split_each_passages(example):\n",
    "    ranked_passages = example[\"ranked_truncPassages_with_labels\"]\n",
    "    all_chunks = []\n",
    "    all_labels = []\n",
    "\n",
    "    for psg in ranked_passages:\n",
    "        chunks = prepare_context_chunks(psg[\"text\"], psg[\"is_positive\"])\n",
    "        for c in chunks:\n",
    "            all_chunks.append(c[\"text\"])\n",
    "            all_labels.append(c[\"is_positive\"])\n",
    "\n",
    "    return {\n",
    "        \"context_chunks\": all_chunks,\n",
    "        \"chunk_labels\": all_labels\n",
    "    }\n",
    "\n",
    "for split in crag_dataset.keys():\n",
    "    crag_dataset[split] = crag_dataset[split].map(split_each_passages)\n",
    "    total_chunks = sum(len(row[\"context_chunks\"]) for row in crag_dataset[split])\n",
    "    average_chunks = total_chunks / len(crag_dataset[split])\n",
    "    print(f\"Rerata jumlah elemen di 'context_chunks' split {split}: {average_chunks:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c504ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ranked_chunks_with_labels: 100%|██████████| 5120/5120 [13:13<00:00,  6.46it/s]\n",
      "Processing ranked_chunks_with_labels: 100%|██████████| 565/565 [01:24<00:00,  6.71it/s]\n",
      "Processing ranked_chunks_with_labels: 100%|██████████| 565/565 [01:23<00:00,  6.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import DatasetDict\n",
    "from general_utils import apply_similarity_ranking_to_dataset\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "embedding_model = AutoModel.from_pretrained('intfloat/multilingual-e5-small')\n",
    "embedding_model.eval()\n",
    "\n",
    "for split in crag_dataset.keys():\n",
    "    crag_dataset[split] = apply_similarity_ranking_to_dataset(\n",
    "        crag_dataset[split], \n",
    "        text_col=\"context_chunks\", \n",
    "        label_col=\"chunk_labels\", \n",
    "        output_col=\"ranked_chunks_with_labels\", \n",
    "        tokenizer=embedding_tokenizer,\n",
    "        model=embedding_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a29815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 18.58ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.85s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 24.83ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 28.33ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.83s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/khalidrizki/CRAG-3sentences-chunks/commit/d3970033c61ef586434db7037c81a274eb0cffcb', commit_message='Upload dataset', commit_description='', oid='d3970033c61ef586434db7037c81a274eb0cffcb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/khalidrizki/CRAG-3sentences-chunks', endpoint='https://huggingface.co', repo_type='dataset', repo_id='khalidrizki/CRAG-3sentences-chunks'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crag_dataset.push_to_hub('khalidrizki/CRAG-3sentences-chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213f498",
   "metadata": {},
   "source": [
    "## Membuat Jawaban\n",
    "dengan variasi chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0353a52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 10.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from general_utils import load_model_and_tokenizer\n",
    "model_name='Qwen/Qwen3-1.7B'\n",
    "model, tokenizer, config = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48c9a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('khalidrizki/CRAG-3sentences-chunks')\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01fcf32",
   "metadata": {},
   "source": [
    "### Jumlah chunks = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9a87cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 2571.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from general_utils import extract_topk_texts\n",
    "from functools import partial\n",
    "k = 1\n",
    "extract_fn = partial(\n",
    "    extract_topk_texts, \n",
    "    k=k, \n",
    "    ranked_units='ranked_chunks_with_labels', \n",
    "    returned_units_col=\"top1_chunks\",\n",
    "    returned_labels_col=\"top1_labels\"\n",
    ")\n",
    "test_dataset = test_dataset.map(extract_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a57e254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answer using the 1 chunk with most similarity:   0%|          | 0/565 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating answer using the 1 chunk with most similarity: 100%|██████████| 565/565 [26:20<00:00,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL CRAG dengan Top 1\n",
      "rerata substring match: 0.4690265486725664\n",
      "rerata F1: 0.3026155217667255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from general_utils import generate_per_row\n",
    "from metrics import evaluate_substringmatch_f1\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "top1_generations = []\n",
    "instruction = \"Konteks: {context}\\nBerdasarkan konteks sebelumnya, jawab pertanyaan berikut dalam satu kalimat. Pertanyaan: {query}\"\n",
    "\n",
    "for row in tqdm(test_dataset, desc=\"Generating answer using the 1 chunk with most similarity\"):\n",
    "    label = row['answer']\n",
    "    generated_answer = generate_per_row(\n",
    "        row=row, \n",
    "        query_col='query', \n",
    "        ctx_col='top1_chunks', \n",
    "        tokenizer=tokenizer, \n",
    "        model=model, \n",
    "        device_type=config.device_type, \n",
    "        instruction=instruction\n",
    "    )\n",
    "    sm, f1 = evaluate_substringmatch_f1(generated_answer.strip(), label.strip())\n",
    "    top1_generations.append({\n",
    "        'query': row['query'], \n",
    "        'passages': row['ranked_chunks_with_labels'], \n",
    "        'filtered_chunks': row['top1_chunks'], \n",
    "        'label': label, \n",
    "        'generated_answer': generated_answer, \n",
    "        'em': sm, \n",
    "        'f1': f1\n",
    "        }\n",
    "    )\n",
    "\n",
    "CRAG_top1 = Dataset.from_list(top1_generations)\n",
    "print(\"HASIL CRAG dengan Top 1\")\n",
    "print(\"rerata substring match:\", sum(CRAG_top1['em'])/len(CRAG_top1))\n",
    "print(\"rerata F1:\", sum(CRAG_top1['f1'])/len(CRAG_top1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67ee2031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 565/565 [00:00<00:00, 38930.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "CRAG_top1.save_to_disk('./outputs/CRAG/top1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1506303",
   "metadata": {},
   "source": [
    "### Jumlah chunks = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1489db68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 2567.49 examples/s]\n",
      "Generating answer using the 2 chunks with most similarity:   0%|          | 0/565 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating answer using the 2 chunks with most similarity: 100%|██████████| 565/565 [27:49<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL CRAG dengan Top 2\n",
      "rerata substring match: 0.5469026548672566\n",
      "rerata F1: 0.33058291391801087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from general_utils import extract_topk_texts\n",
    "from functools import partial\n",
    "k = 2\n",
    "extract2_fn = partial(\n",
    "    extract_topk_texts, \n",
    "    k=k, \n",
    "    ranked_units='ranked_chunks_with_labels', \n",
    "    returned_units_col=\"top2_chunks\",\n",
    "    returned_labels_col=\"top2_labels\"\n",
    ")\n",
    "test_dataset = test_dataset.map(extract2_fn)\n",
    "\n",
    "from general_utils import generate_per_row\n",
    "from metrics import evaluate_substringmatch_f1\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "top2_generations = []\n",
    "instruction = \"Konteks: {context}\\nBerdasarkan konteks sebelumnya, jawab pertanyaan berikut dalam satu kalimat. Pertanyaan: {query}\"\n",
    "\n",
    "for row in tqdm(test_dataset, desc=\"Generating answer using the 2 chunks with most similarity\"):\n",
    "    label = row['answer']\n",
    "    generated_answer = generate_per_row(\n",
    "        row=row, \n",
    "        query_col='query', \n",
    "        ctx_col='top2_chunks', \n",
    "        tokenizer=tokenizer, \n",
    "        model=model, \n",
    "        device_type=config.device_type, \n",
    "        instruction=instruction\n",
    "    )\n",
    "    sm, f1 = evaluate_substringmatch_f1(generated_answer.strip(), label.strip())\n",
    "    top2_generations.append({\n",
    "        'query': row['query'], \n",
    "        'passages': row['ranked_chunks_with_labels'], \n",
    "        'filtered_chunks': row['top2_chunks'], \n",
    "        'label': label, \n",
    "        'generated_answer': generated_answer, \n",
    "        'em': sm, \n",
    "        'f1': f1\n",
    "        }\n",
    "    )\n",
    "\n",
    "CRAG_top2 = Dataset.from_list(top2_generations)\n",
    "print(\"HASIL CRAG dengan Top 2\")\n",
    "print(\"rerata substring match:\", sum(CRAG_top2['em'])/len(CRAG_top2))\n",
    "print(\"rerata F1:\", sum(CRAG_top2['f1'])/len(CRAG_top2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9fd4733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 565/565 [00:00<00:00, 35326.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "CRAG_top2.save_to_disk('./outputs/CRAG/top2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f1629",
   "metadata": {},
   "source": [
    "### Jumlah chunks = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "098f200f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 3272.12 examples/s]\n",
      "Generating answer using the 3 chunks with most similarity:   0%|          | 0/565 [00:00<?, ?it/s]c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating answer using the 3 chunks with most similarity: 100%|██████████| 565/565 [26:16<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL CRAG dengan Top 3\n",
      "rerata substring match: 0.584070796460177\n",
      "rerata F1: 0.35286040653377126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from general_utils import extract_topk_texts\n",
    "from functools import partial\n",
    "k = 3\n",
    "extract3_fn = partial(\n",
    "    extract_topk_texts, \n",
    "    k=k, \n",
    "    ranked_units='ranked_chunks_with_labels', \n",
    "    returned_units_col=\"top3_chunks\",\n",
    "    returned_labels_col=\"top3_labels\"\n",
    ")\n",
    "test_dataset = test_dataset.map(extract3_fn)\n",
    "\n",
    "from general_utils import generate_per_row\n",
    "from metrics import evaluate_substringmatch_f1\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "top3_generations = []\n",
    "instruction = \"Konteks: {context}\\nBerdasarkan konteks sebelumnya, jawab pertanyaan berikut dalam satu kalimat. Pertanyaan: {query}\"\n",
    "\n",
    "for row in tqdm(test_dataset, desc=\"Generating answer using the 3 chunks with most similarity\"):\n",
    "    label = row['answer']\n",
    "    generated_answer = generate_per_row(\n",
    "        row=row, \n",
    "        query_col='query', \n",
    "        ctx_col='top3_chunks', \n",
    "        tokenizer=tokenizer, \n",
    "        model=model, \n",
    "        device_type=config.device_type, \n",
    "        instruction=instruction\n",
    "    )\n",
    "    sm, f1 = evaluate_substringmatch_f1(generated_answer.strip(), label.strip())\n",
    "    top3_generations.append({\n",
    "        'query': row['query'], \n",
    "        'passages': row['ranked_chunks_with_labels'], \n",
    "        'filtered_chunks': row['top3_chunks'], \n",
    "        'label': label, \n",
    "        'generated_answer': generated_answer, \n",
    "        'em': sm, \n",
    "        'f1': f1\n",
    "        }\n",
    "    )\n",
    "\n",
    "CRAG_top3 = Dataset.from_list(top3_generations)\n",
    "print(\"HASIL CRAG dengan Top 3\")\n",
    "print(\"rerata substring match:\", sum(CRAG_top3['em'])/len(CRAG_top3))\n",
    "print(\"rerata F1:\", sum(CRAG_top3['f1'])/len(CRAG_top3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d42b9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 565/565 [00:00<00:00, 86907.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "CRAG_top3.save_to_disk('./outputs/CRAG/top3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fff2a1",
   "metadata": {},
   "source": [
    "### RAG Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80669208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 565/565 [00:00<00:00, 2399.06 examples/s]\n",
      "Generating answer using the original passages (not splitted into chunks): 100%|██████████| 565/565 [29:42<00:00,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL RAG normal\n",
      "rerata substring match: 0.6035398230088496\n",
      "rerata F1: 0.35978795140582226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from general_utils import extract_topk_texts\n",
    "from functools import partial\n",
    "k = 3\n",
    "extract_all_fn = partial(\n",
    "    extract_topk_texts, \n",
    "    k=k, \n",
    "    ranked_units='ranked_truncPassages_with_labels', \n",
    "    returned_units_col=\"sorted_passages\",\n",
    "    returned_labels_col=\"sortedPassages_labels\"\n",
    ")\n",
    "test_dataset = test_dataset.map(extract_all_fn)\n",
    "\n",
    "from general_utils import generate_per_row\n",
    "from metrics import evaluate_substringmatch_f1\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "ori_passages_generations = []\n",
    "instruction = \"Konteks: {context}\\nBerdasarkan konteks sebelumnya, jawab pertanyaan berikut dalam satu kalimat. Pertanyaan: {query}\"\n",
    "\n",
    "for row in tqdm(test_dataset, desc=\"Generating answer using the original passages (not splitted into chunks)\"):\n",
    "    label = row['answer']\n",
    "    generated_answer = generate_per_row(\n",
    "        row=row, \n",
    "        query_col='query', \n",
    "        ctx_col='sorted_passages', \n",
    "        tokenizer=tokenizer, \n",
    "        model=model, \n",
    "        device_type=config.device_type, \n",
    "        instruction=instruction\n",
    "    )\n",
    "    sm, f1 = evaluate_substringmatch_f1(generated_answer.strip(), label.strip())\n",
    "    ori_passages_generations.append({\n",
    "        'query': row['query'], \n",
    "        'passages': row['ranked_truncPassages_with_labels'], \n",
    "        'filtered_chunks': row['sorted_passages'], \n",
    "        'label': label, \n",
    "        'generated_answer': generated_answer, \n",
    "        'em': sm, \n",
    "        'f1': f1\n",
    "        }\n",
    "    )\n",
    "\n",
    "normal_RAG = Dataset.from_list(ori_passages_generations)\n",
    "print(\"HASIL RAG normal\")\n",
    "print(\"rerata substring match:\", sum(normal_RAG['em'])/len(normal_RAG))\n",
    "print(\"rerata F1:\", sum(normal_RAG['f1'])/len(normal_RAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b7421d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 565/565 [00:00<00:00, 75441.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "normal_RAG.save_to_disk('./outputs/CRAG/normal-rag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a1fdd",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aead79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'label', 'model_outputs'],\n",
       "        num_rows: 5120\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'label', 'model_outputs'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query', 'passages', 'summary', 'final_summary', 'label', 'model_outputs'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('khalidrizki/RECOMP-finetuning-final')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a758bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
